# helper_rag.py
# RAGãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†ã®å…±é€šæ©Ÿèƒ½ï¼ˆãƒ­ã‚¸ãƒƒã‚¯ã®ã¿ï¼‰
# -----------------------------------------

import pandas as pd
import re
import io
import logging
import json
from typing import List, Dict, Any, Optional, Tuple
from pathlib import Path
from datetime import datetime
from functools import wraps

# ===================================================================
# åŸºæœ¬ãƒ­ã‚°è¨­å®š
# ===================================================================
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# ===================================================================
# å…±é€šãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‹ã‚‰ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
# ===================================================================
from helper_llm import create_llm_client, LLMClient, DEFAULT_LLM_PROVIDER
from helper_embedding import create_embedding_client, EmbeddingClient, DEFAULT_EMBEDDING_PROVIDER, get_embedding_dimensions, get_embedding_model_pricing
from helper_llm import get_llm_model_pricing
from helper_text import clean_text


# ==================================================
# RAGè¨­å®šã‚¯ãƒ©ã‚¹ï¼ˆå…¨ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå¯¾å¿œï¼‰
# ==================================================
class RAGConfig:
    """RAGãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†ã®è¨­å®šï¼ˆå…¨ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆçµ±åˆï¼‰"""

    DATASET_CONFIGS = {
        # ã‚«ã‚¹ã‚¿ãƒãƒ¼ã‚µãƒãƒ¼ãƒˆFAQ
        "customer_support_faq": {
            "name"            : "ã‚«ã‚¹ã‚¿ãƒãƒ¼ã‚µãƒãƒ¼ãƒˆãƒ»FAQ",
            "icon"            : "ğŸ’¬",
            "required_columns": ["question", "answer"],
            "description"     : "ã‚«ã‚¹ã‚¿ãƒãƒ¼ã‚µãƒãƒ¼ãƒˆFAQãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ",
            "combine_template": "{question} {answer}",
            "port"            : 8501
        },

        # åŒ»ç™‚QA
        "medical_qa"          : {
            "name"            : "åŒ»ç™‚QAãƒ‡ãƒ¼ã‚¿",
            "icon"            : "ğŸ¥",
            "required_columns": ["Question", "Complex_CoT", "Response"],
            "description"     : "åŒ»ç™‚è³ªå•å›ç­”ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ",
            "combine_template": "{question} {complex_cot} {response}",
            "port"            : 8503
        },

        # ç§‘å­¦ãƒ»æŠ€è¡“QA
        "sciq_qa"             : {
            "name"            : "ç§‘å­¦ãƒ»æŠ€è¡“QAï¼ˆSciQï¼‰",
            "icon"            : "ğŸ”¬",
            "required_columns": ["question", "correct_answer"],
            "description"     : "ç§‘å­¦ãƒ»æŠ€è¡“è³ªå•å›ç­”ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ",
            "combine_template": "{question} {correct_answer}",
            "port"            : 8504
        },

        # æ³•å¾‹ãƒ»åˆ¤ä¾‹QA
        "legal_qa"            : {
            "name"            : "æ³•å¾‹ãƒ»åˆ¤ä¾‹QA",
            "icon"            : "âš–ï¸",
            "required_columns": ["question", "answer"],
            "description"     : "æ³•å¾‹ãƒ»åˆ¤ä¾‹è³ªå•å›ç­”ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ",
            "combine_template": "{question} {answer}",
            "port"            : 8505
        },
        
        # TriviaQA
        "trivia_qa"           : {
            "name"            : "é›‘å­¦QAï¼ˆTriviaQAï¼‰",
            "icon"            : "ğŸ¯",
            "required_columns": ["question", "answer"],
            "description"     : "é›‘å­¦è³ªå•å›ç­”ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ",
            "combine_template": "{question} {answer} {entity_pages} {search_results}",
            "port"            : 8506
        }
    }

    @classmethod
    def get_config(cls, dataset_type: str) -> Dict[str, Any]:
        """ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆè¨­å®šã®å–å¾—"""
        return cls.DATASET_CONFIGS.get(dataset_type, {
            "name"            : "æœªçŸ¥ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ",
            "icon"            : "â“",
            "required_columns": [],
            "description"     : "æœªçŸ¥ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ",
            "combine_template": "{}",
            "port"            : 8500
        })

    @classmethod
    def get_all_datasets(cls) -> List[str]:
        """å…¨ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚¿ã‚¤ãƒ—ã®ãƒªã‚¹ãƒˆã‚’å–å¾—"""
        return list(cls.DATASET_CONFIGS.keys())

    @classmethod
    def get_dataset_by_port(cls, port: int) -> Optional[str]:
        """ãƒãƒ¼ãƒˆç•ªå·ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚¿ã‚¤ãƒ—ã‚’å–å¾—"""
        for dataset_type, config in cls.DATASET_CONFIGS.items():
            if config.get("port") == port:
                return dataset_type
        return None


# ==================================================
# ãƒˆãƒ¼ã‚¯ãƒ³ç®¡ç†ã‚¯ãƒ©ã‚¹ï¼ˆservices/token_serviceã‹ã‚‰çµ±åˆï¼‰
# ==================================================
# å¾Œæ–¹äº’æ›æ€§ã®ãŸã‚ã€services.token_serviceã‹ã‚‰import
from services.token_service import TokenManager  # noqa: E402


# ==================================================
# ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ãƒ‡ã‚³ãƒ¬ãƒ¼ã‚¿ï¼ˆå…±é€šï¼‰
# ==================================================
def safe_execute(func):
    """å®‰å…¨å®Ÿè¡Œãƒ‡ã‚³ãƒ¬ãƒ¼ã‚¿ï¼ˆUIéä¾å­˜ï¼‰"""

    @wraps(func)
    def wrapper(*args, **kwargs):
        try:
            return func(*args, **kwargs)
        except Exception as e:
            logger.error(f"Error in {func.__name__}: {str(e)}")
            # UIãŒãªã„ã®ã§st.errorã¯å‰Šé™¤
            return None

    return wrapper


# ==================================================
# ãƒ‡ãƒ¼ã‚¿å‡¦ç†é–¢æ•°ç¾¤ï¼ˆå…±é€šï¼‰
# ==================================================
def combine_columns(row: pd.Series, dataset_type: str) -> str:
    """è¤‡æ•°åˆ—ã‚’çµåˆã—ã¦1ã¤ã®ãƒ†ã‚­ã‚¹ãƒˆã«ã™ã‚‹ï¼ˆãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå¯¾å¿œï¼‰"""
    config_data = RAGConfig.get_config(dataset_type)
    required_columns = config_data["required_columns"]

    # å„åˆ—ã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡ºãƒ»ã‚¯ãƒ¬ãƒ³ã‚¸ãƒ³ã‚°
    cleaned_values = []
    for col in required_columns:
        if col in row.index:
            value = row.get(col, '')
            cleaned_text = clean_text(str(value))
            if cleaned_text:  # ç©ºã§ãªã„å ´åˆã®ã¿è¿½åŠ 
                cleaned_values.append(cleaned_text)

    # åŒ»ç™‚QAã®ç‰¹åˆ¥å‡¦ç†ï¼ˆQuestion, Complex_CoT, Responseï¼‰
    if dataset_type == "medical_qa":
        # å¤§æ–‡å­—å°æ–‡å­—ã‚’è€ƒæ…®ã—ãŸåˆ—åãƒãƒƒãƒ”ãƒ³ã‚°
        medical_cols = {}
        for col in row.index:
            col_lower = col.lower()
            if 'question' in col_lower:
                medical_cols['question'] = clean_text(str(row.get(col, '')))
            elif 'complex_cot' in col_lower or 'cot' in col_lower:
                medical_cols['complex_cot'] = clean_text(str(row.get(col, '')))
            elif 'response' in col_lower:
                medical_cols['response'] = clean_text(str(row.get(col, '')))

        # åŒ»ç™‚QAç”¨ã®çµåˆ
        medical_values = [v for v in medical_cols.values() if v]
        if medical_values:
            return " ".join(medical_values).strip()

    # çµåˆ
    combined = " ".join(cleaned_values)
    return combined.strip()


def validate_data(df: pd.DataFrame, dataset_type: str = None) -> List[str]:
    """ãƒ‡ãƒ¼ã‚¿ã®æ¤œè¨¼"""
    issues = []

    # åŸºæœ¬çµ±è¨ˆ
    issues.append(f"ç·è¡Œæ•°: {len(df):,}")
    issues.append(f"ç·åˆ—æ•°: {len(df.columns)}")

    # å¿…é ˆåˆ—ã®ç¢ºèª
    if dataset_type:
        config_data = RAGConfig.get_config(dataset_type)
        required_columns = config_data["required_columns"]

        # å¤§æ–‡å­—å°æ–‡å­—ã‚’è€ƒæ…®ã—ãŸåˆ—åãƒã‚§ãƒƒã‚¯
        [col.lower() for col in df.columns]
        missing_columns = []
        found_columns = []

        for req_col in required_columns:
            req_col_lower = req_col.lower()
            # éƒ¨åˆ†ä¸€è‡´ã‚‚è¨±å¯ï¼ˆä¾‹ï¼šQuestion -> question, Complex_CoT -> complex_cotï¼‰
            found = False
            for available_col in df.columns:
                if req_col_lower in available_col.lower() or available_col.lower() in req_col_lower:
                    found_columns.append(available_col)
                    found = True
                    break
            if not found:
                missing_columns.append(req_col)

        if missing_columns:
            issues.append(f"âš ï¸ å¿…é ˆåˆ—ãŒä¸è¶³: {missing_columns}")
        else:
            issues.append(f"âœ… å¿…é ˆåˆ—ç¢ºèªæ¸ˆã¿: {found_columns}")

    # å„åˆ—ã®ç©ºå€¤ç¢ºèª
    for col in df.columns:
        empty_count = df[col].isna().sum() + (df[col] == '').sum()
        if empty_count > 0:
            percentage = (empty_count / len(df)) * 100
            issues.append(f"{col}åˆ—: ç©ºå€¤ {empty_count:,}å€‹ ({percentage:.1f}%)")

    # é‡è¤‡è¡Œã®ç¢ºèª
    duplicate_count = df.duplicated().sum()
    if duplicate_count > 0:
        issues.append(f"âš ï¸ é‡è¤‡è¡Œ: {duplicate_count:,}å€‹")
    else:
        issues.append("âœ… é‡è¤‡è¡Œãªã—")

    return issues


@safe_execute
def load_dataset(uploaded_file, dataset_type: str = None) -> Tuple[pd.DataFrame, List[str]]:
    """ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®èª­ã¿è¾¼ã¿ã¨åŸºæœ¬æ¤œè¨¼"""
    # CSVãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿
    df = pd.read_csv(uploaded_file)

    # åŸºæœ¬æ¤œè¨¼
    validation_results = validate_data(df, dataset_type)

    logger.info(f"ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆèª­ã¿è¾¼ã¿å®Œäº†: {len(df):,}è¡Œ, {len(df.columns)}åˆ—")
    return df, validation_results


@safe_execute
def process_rag_data(df: pd.DataFrame, dataset_type: str, combine_columns_option: bool = True) -> pd.DataFrame:
    """RAGãƒ‡ãƒ¼ã‚¿ã®å‰å‡¦ç†ã‚’å®Ÿè¡Œ"""
    # åŸºæœ¬çš„ãªå‰å‡¦ç†
    df_processed = df.copy()

    # é‡è¤‡è¡Œã®é™¤å»
    initial_rows = len(df_processed)
    df_processed = df_processed.drop_duplicates()
    duplicates_removed = initial_rows - len(df_processed)

    # ç©ºè¡Œã®é™¤å»ï¼ˆå…¨åˆ—ãŒNAã®è¡Œï¼‰
    df_processed = df_processed.dropna(how='all')
    empty_rows_removed = initial_rows - duplicates_removed - len(df_processed)

    # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®ãƒªã‚»ãƒƒãƒˆ
    df_processed = df_processed.reset_index(drop=True)

    logger.info(f"å‰å‡¦ç†å®Œäº†: é‡è¤‡é™¤å»={duplicates_removed:,}è¡Œ, ç©ºè¡Œé™¤å»={empty_rows_removed:,}è¡Œ")

    # å„åˆ—ã®ã‚¯ãƒ¬ãƒ³ã‚¸ãƒ³ã‚°ï¼ˆãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå¯¾å¿œï¼‰
    config_data = RAGConfig.get_config(dataset_type)
    required_columns = config_data["required_columns"]

    # å¤§æ–‡å­—å°æ–‡å­—ã‚’è€ƒæ…®ã—ãŸåˆ—åå‡¦ç†
    for col in df_processed.columns:
        for req_col in required_columns:
            if req_col.lower() in col.lower() or col.lower() in req_col.lower():
                df_processed[col] = df_processed[col].apply(clean_text)

    # åˆ—ã®çµåˆï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
    if combine_columns_option:
        df_processed['Combined_Text'] = df_processed.apply(
            lambda row: combine_columns(row, dataset_type),
            axis=1
        )

        # ç©ºã®çµåˆãƒ†ã‚­ã‚¹ãƒˆã‚’é™¤å»
        before_filter = len(df_processed)
        df_processed = df_processed[df_processed['Combined_Text'].str.strip() != '']
        after_filter = len(df_processed)
        empty_combined_removed = before_filter - after_filter

        if empty_combined_removed > 0:
            logger.info(f"ç©ºã®çµåˆãƒ†ã‚­ã‚¹ãƒˆã‚’é™¤å»: {empty_combined_removed:,}è¡Œ")

    return df_processed


@safe_execute
def create_download_data(df: pd.DataFrame, include_combined: bool = True, dataset_type: str = None) -> Tuple[
    str, Optional[str]]:
    """ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ç”¨ãƒ‡ãƒ¼ã‚¿ã®ä½œæˆ"""
    # CSVãƒ‡ãƒ¼ã‚¿ã®ä½œæˆ
    csv_buffer = io.StringIO()
    df.to_csv(csv_buffer, index=False, encoding='utf-8')
    csv_data = csv_buffer.getvalue()

    # çµåˆãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®ä½œæˆ
    text_data = None
    if include_combined and 'Combined_Text' in df.columns:
        # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ãªã—ã§çµåˆãƒ†ã‚­ã‚¹ãƒˆã®ã¿ã‚’å‡ºåŠ›
        text_lines = []
        for text in df['Combined_Text']:
            if text and str(text).strip():
                text_lines.append(str(text).strip())
        text_data = '\n'.join(text_lines)

    return csv_data, text_data


# ==================================================
# ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜é–¢æ•°ç¾¤ï¼ˆå…±é€šï¼‰
# ==================================================
def create_output_directory() -> Path:
    """OUTPUTãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ä½œæˆ"""
    try:
        output_dir = Path("OUTPUT")
        output_dir.mkdir(exist_ok=True)

        # æ›¸ãè¾¼ã¿æ¨©é™ã®ãƒ†ã‚¹ãƒˆ
        test_file = output_dir / ".test_write"
        try:
            test_file.write_text("test", encoding='utf-8')
            if test_file.exists():
                test_file.unlink()
                logger.info("æ›¸ãè¾¼ã¿æ¨©é™ãƒ†ã‚¹ãƒˆ: æˆåŠŸ")
        except Exception as e:
            raise PermissionError(f"æ›¸ãè¾¼ã¿æ¨©é™ãƒ†ã‚¹ãƒˆã«å¤±æ•—: {e}")

        logger.info(f"OUTPUTãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªæº–å‚™å®Œäº†: {output_dir.absolute()}")
        return output_dir

    except Exception as e:
        logger.error(f"ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆã‚¨ãƒ©ãƒ¼: {e}")
        raise


@safe_execute
def save_files_to_output(df_processed, dataset_type: str, csv_data: str, text_data: str = None) -> Dict[str, str]:
    """å‡¦ç†æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ã‚’OUTPUTãƒ•ã‚©ãƒ«ãƒ€ã«ä¿å­˜"""
    output_dir = create_output_directory()
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    saved_files = {}

    # CSVãƒ•ã‚¡ã‚¤ãƒ«ã®ä¿å­˜
    csv_filename = f"preprocessed_{dataset_type}.csv"
    csv_path = output_dir / csv_filename

    with open(csv_path, 'w', encoding='utf-8') as f:
        f.write(csv_data)

    if csv_path.exists():
        saved_files['csv'] = str(csv_path)
        logger.info(f"CSVãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜å®Œäº†: {csv_path}")

    # ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã®ä¿å­˜
    if text_data and len(text_data.strip()) > 0:
        txt_filename = f"{dataset_type}.txt"
        txt_path = output_dir / txt_filename

        with open(txt_path, 'w', encoding='utf-8') as f:
            f.write(text_data)

        if txt_path.exists():
            saved_files['txt'] = str(txt_path)
            logger.info(f"ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜å®Œäº†: {txt_path}")

    # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ã®ä¿å­˜
    metadata = {
        "dataset_type"        : dataset_type,
        "processed_rows"      : len(df_processed),
        "processing_timestamp": timestamp,
        "created_at"          : datetime.now().isoformat(),
        "files_created"       : list(saved_files.keys()),
        "processing_info"     : {
            "original_rows": 0, # ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ä¾å­˜ã‚’æ’é™¤ã®ãŸã‚ä¸€æ—¦0
            "removed_rows" : 0
        }
    }

    metadata_filename = f"metadata_{dataset_type}.json"
    metadata_path = output_dir / metadata_filename

    with open(metadata_path, 'w', encoding='utf-8') as f:
        json.dump(metadata, f, indent=2, ensure_ascii=False)

    if metadata_path.exists():
        saved_files['metadata'] = str(metadata_path)
        logger.info(f"ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜å®Œäº†: {metadata_path}")

    return saved_files


# ==================================================
# ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆï¼ˆå…±é€šé–¢æ•°ä¸€è¦§ï¼‰
# ==================================================
__all__ = [
    # è¨­å®šã‚¯ãƒ©ã‚¹
    'RAGConfig',
    'TokenManager',

    # ãƒ‡ã‚³ãƒ¬ãƒ¼ã‚¿
    'safe_execute',

    # ãƒ‡ãƒ¼ã‚¿å‡¦ç†é–¢æ•°
    'clean_text',
    'combine_columns',
    'validate_data',
    'load_dataset',
    'process_rag_data',
    'create_download_data',

    # ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜é–¢æ•°
    'create_output_directory',
    'save_files_to_output',
]