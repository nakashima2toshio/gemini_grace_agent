#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
ui/components/rag_components.py - RAGãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†ç”¨UIã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆ
==================================================================
Streamlitä¾å­˜ã®UIé–¢æ•°ç¾¤ã€‚helper_rag_ui.pyã‹ã‚‰ç§»å‹•ã€‚

å…ƒãƒ•ã‚¡ã‚¤ãƒ«: helper_rag_ui.py
"""

import streamlit as st
import pandas as pd
import logging
from typing import List, Dict, Any, Optional

from helper_llm import (
    DEFAULT_LLM_PROVIDER,
    get_available_llm_models,
    get_llm_model_limits,
    get_llm_model_pricing,
    get_available_embedding_models,
    get_embedding_model_pricing,
)
from helper_rag import RAGConfig, TokenManager

logger = logging.getLogger(__name__)

# ==================================================
# UIé–¢æ•°ç¾¤
# ==================================================
def select_model(key: str = "model_selection") -> str:
    """ãƒ¢ãƒ‡ãƒ«é¸æŠUI"""
    models = get_available_llm_models()
    default_model = DEFAULT_LLM_PROVIDER

    try:
        default_index = models.index(default_model)
    except ValueError:
        default_index = 0

    selected = st.sidebar.selectbox(
        "ğŸ¤– ãƒ¢ãƒ‡ãƒ«ã‚’é¸æŠ",
        models,
        index=default_index,
        key=key,
        help="åˆ©ç”¨ã™ã‚‹LLMãƒ¢ãƒ‡ãƒ«ã‚’é¸æŠã—ã¦ãã ã•ã„"
    )

    return selected


def show_model_info(selected_model: str) -> None:
    """é¸æŠã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã®æƒ…å ±ã‚’è¡¨ç¤º"""
    try:
        limits = get_llm_model_limits(selected_model)
        pricing = get_llm_model_pricing(selected_model)

        with st.sidebar.expander("ğŸ“Š é¸æŠãƒ¢ãƒ‡ãƒ«æƒ…å ±", expanded=False):
            # åŸºæœ¬æƒ…å ±
            col1, col2 = st.columns(2)
            with col1:
                st.write("**æœ€å¤§å…¥åŠ›**")
                st.write(f"{limits.get('max_tokens', 0):,}")
            with col2:
                st.write("**æœ€å¤§å‡ºåŠ›**")
                st.write(f"{limits.get('max_output', 0):,}")

            # æ–™é‡‘æƒ…å ±
            st.write("**æ–™é‡‘ï¼ˆ1000ãƒˆãƒ¼ã‚¯ãƒ³ï¼‰**")
            st.write(f"- å…¥åŠ›: ${pricing.get('input', 0.0):.5f}")
            st.write(f"- å‡ºåŠ›: ${pricing.get('output', 0.0):.5f}")

            # ãƒ¢ãƒ‡ãƒ«ç‰¹æ€§ï¼ˆGeminiã«ç‰¹åŒ–ï¼‰
            if "gemini-2.0" in selected_model:
                st.info("âœ¨ Gemini 2.0 ã‚·ãƒªãƒ¼ã‚º")
                st.caption("é«˜é€Ÿãƒ»é«˜æ€§èƒ½ãªæ¬¡ä¸–ä»£ãƒ¢ãƒ‡ãƒ«")
            elif "gemini-1.5" in selected_model:
                st.info("ğŸ’¡ Gemini 1.5 ã‚·ãƒªãƒ¼ã‚º")
                st.caption("é•·æ–‡ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒ»ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«å¯¾å¿œ")
            elif "gpt" in selected_model:
                st.info("âš™ï¸ OpenAIäº’æ›ãƒ¢ãƒ‡ãƒ«")
                st.caption("OpenAI APIã‚’ä»‹ã—ã¦åˆ©ç”¨å¯èƒ½")
            else:
                st.info("ğŸ’¬ ãã®ä»–ã®LLMãƒ¢ãƒ‡ãƒ«")

            # RAGç”¨é€”ã§ã®æ¨å¥¨åº¦
            st.write("**RAGç”¨é€”æ¨å¥¨åº¦**")
            if "flash" in selected_model:
                st.success("âœ… æœ€é©ï¼ˆé«˜é€Ÿãƒ»ã‚³ã‚¹ãƒˆåŠ¹ç‡è‰¯å¥½ï¼‰")
            elif "pro" in selected_model:
                st.info("ğŸ’¡ é«˜å“è³ªï¼ˆè©³ç´°ãªæ¨è«–ã«æœ€é©ï¼‰")
            elif "gpt" in selected_model:
                st.info("ğŸ’¬ OpenAIäº’æ›ï¼ˆç”¨é€”ã«å¿œã˜ã¦é¸æŠï¼‰")
            else:
                st.info("ğŸ’¬ æ¨™æº–çš„ãªæ€§èƒ½")

    except Exception as e:
        logger.error(f"ãƒ¢ãƒ‡ãƒ«æƒ…å ±è¡¨ç¤ºã‚¨ãƒ©ãƒ¼: {e}")
        st.sidebar.error("ãƒ¢ãƒ‡ãƒ«æƒ…å ±ã®å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸ")


def estimate_token_usage(df_processed: pd.DataFrame, selected_model: str) -> None:
    """å‡¦ç†æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ã®ãƒˆãƒ¼ã‚¯ãƒ³ä½¿ç”¨é‡æ¨å®š"""
    try:
        if 'Combined_Text' in df_processed.columns:
            # ã‚µãƒ³ãƒ—ãƒ«ãƒ†ã‚­ã‚¹ãƒˆã§ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã‚’æ¨å®š
            sample_size = min(10, len(df_processed))
            sample_texts = df_processed['Combined_Text'].head(sample_size).tolist()
            total_chars = df_processed['Combined_Text'].str.len().sum()

            if sample_texts:
                sample_text = " ".join(sample_texts)
                # TokenManagerã®count_tokensã‚’ä½¿ç”¨
                sample_tokens = TokenManager.count_tokens(sample_text, selected_model)
                sample_chars = len(sample_text)

                if sample_chars > 0:
                    # å…¨ä½“ã®ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã‚’æ¨å®š
                    estimated_total_tokens = int((total_chars / sample_chars) * sample_tokens)

                    # Embeddingãƒ¢ãƒ‡ãƒ«ã®æ–™é‡‘ã‚’å–å¾— (Gemini Embeddingã‚’æƒ³å®š)
                    embedding_model_name = get_available_embedding_models()[0] # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®Gemini Embeddingãƒ¢ãƒ‡ãƒ«ã‚’å–å¾—
                    embedding_pricing_per_1k_tokens = get_embedding_model_pricing(embedding_model_name)

                    with st.expander("ğŸ”¢ ãƒˆãƒ¼ã‚¯ãƒ³ä½¿ç”¨é‡æ¨å®š", expanded=False):
                        col1, col2, col3 = st.columns(3)
                        with col1:
                            st.metric("æ¨å®šç·ãƒˆãƒ¼ã‚¯ãƒ³æ•°", f"{estimated_total_tokens:,}")
                        with col2:
                            avg_tokens_per_record = estimated_total_tokens / len(df_processed)
                            st.metric("å¹³å‡ãƒˆãƒ¼ã‚¯ãƒ³/ãƒ¬ã‚³ãƒ¼ãƒ‰", f"{avg_tokens_per_record:.0f}")
                        with col3:
                            # embeddingç”¨ã®ã‚³ã‚¹ãƒˆæ¨å®š
                            embedding_cost = (estimated_total_tokens / 1000) * embedding_pricing_per_1k_tokens
                            st.metric("æ¨å®šembeddingè²»ç”¨", f"${embedding_cost:.4f}")

                        st.info(f"ğŸ’¡ é¸æŠLLMãƒ¢ãƒ‡ãƒ«ã€Œ{selected_model}ã€ãŠã‚ˆã³Embeddingãƒ¢ãƒ‡ãƒ«ã€Œ{embedding_model_name}ã€ã§ã®æ¨å®šå€¤")
                        st.caption("â€» å®Ÿéš›ã®ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã¨ã¯ç•°ãªã‚‹å ´åˆãŒã‚ã‚Šã¾ã™")

    except Exception as e:
        logger.error(f"ãƒˆãƒ¼ã‚¯ãƒ³ä½¿ç”¨é‡æ¨å®šã‚¨ãƒ©ãƒ¼: {e}")
        st.error("ãƒˆãƒ¼ã‚¯ãƒ³ä½¿ç”¨é‡ã®æ¨å®šã«å¤±æ•—ã—ã¾ã—ãŸ")


def display_statistics(df_original: pd.DataFrame, df_processed: pd.DataFrame, dataset_type: str = None) -> None:
    """å‡¦ç†å‰å¾Œã®çµ±è¨ˆæƒ…å ±ã‚’è¡¨ç¤º"""
    st.subheader("ğŸ“Š çµ±è¨ˆæƒ…å ±")

    col1, col2, col3 = st.columns(3)

    with col1:
        st.metric("å…ƒã®è¡Œæ•°", f"{len(df_original):,}")
    with col2:
        st.metric("å‡¦ç†å¾Œã®è¡Œæ•°", f"{len(df_processed):,}")
    with col3:
        removed_rows = len(df_original) - len(df_processed)
        st.metric("é™¤å»ã•ã‚ŒãŸè¡Œæ•°", f"{removed_rows:,}")

    # çµåˆãƒ†ã‚­ã‚¹ãƒˆã®åˆ†æ
    if 'Combined_Text' in df_processed.columns:
        st.subheader("ğŸ“ çµåˆå¾Œãƒ†ã‚­ã‚¹ãƒˆåˆ†æ")
        text_lengths = df_processed['Combined_Text'].str.len()

        col1, col2, col3 = st.columns(3)
        with col1:
            st.metric("å¹³å‡æ–‡å­—æ•°", f"{text_lengths.mean():.0f}")
        with col2:
            st.metric("æœ€å¤§æ–‡å­—æ•°", f"{text_lengths.max():,}")
        with col3:
            st.metric("æœ€å°æ–‡å­—æ•°", f"{text_lengths.min():,}")

        # ãƒ‘ãƒ¼ã‚»ãƒ³ã‚¿ã‚¤ãƒ«è¡¨ç¤º
        percentiles = text_lengths.quantile([0.25, 0.5, 0.75])
        st.write("**æ–‡å­—æ•°åˆ†å¸ƒ:**")
        col1, col2, col3 = st.columns(3)
        with col1:
            st.write(f"25%ç‚¹: {percentiles[0.25]:.0f}æ–‡å­—")
        with col2:
            st.write(f"ä¸­å¤®å€¤: {percentiles[0.5]:.0f}æ–‡å­—")
        with col3:
            st.write(f"75%ç‚¹: {percentiles[0.75]:.0f}æ–‡å­—")


# ==================================================
# ä½¿ç”¨æ–¹æ³•èª¬æ˜é–¢æ•°ï¼ˆãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå¯¾å¿œï¼‰
# ==================================================
def show_usage_instructions(dataset_type: str) -> None:
    """ä½¿ç”¨æ–¹æ³•ã®èª¬æ˜ã‚’è¡¨ç¤ºï¼ˆãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆåˆ¥å¯¾å¿œï¼‰"""
    config_data = RAGConfig.get_config(dataset_type)
    required_columns_str = ", ".join(config_data["required_columns"])

    st.markdown("---")
    st.subheader("ğŸ“– ä½¿ç”¨æ–¹æ³•")

    # åŸºæœ¬çš„ãªä½¿ç”¨æ–¹æ³•
    basic_usage = f"""
    ### ğŸ“‹ å‰å‡¦ç†æ‰‹é †
    1. **ãƒ¢ãƒ‡ãƒ«é¸æŠ**: ã‚µã‚¤ãƒ‰ãƒãƒ¼ã§RAGç”¨é€”ã«é©ã—ãŸãƒ¢ãƒ‡ãƒ«ã‚’é¸æŠ
    2. **CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰**: {required_columns_str} åˆ—ã‚’å«ã‚€CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’é¸æŠ
    3. **å‰å‡¦ç†å®Ÿè¡Œ**: ä»¥ä¸‹ã®å‡¦ç†ãŒè‡ªå‹•ã§å®Ÿè¡Œã•ã‚Œã¾ã™ï¼š
       - æ”¹è¡Œãƒ»ç©ºç™½ã®æ­£è¦åŒ–
       - é‡è¤‡è¡Œã®é™¤å»
       - ç©ºè¡Œã®é™¤å»
       - å¼•ç”¨ç¬¦ã®æ­£è¦åŒ–
    4. **åˆ—çµåˆ**: Vector Store/RAGç”¨ã«æœ€é©åŒ–ã•ã‚ŒãŸè‡ªç„¶ãªæ–‡ç« ã¨ã—ã¦çµåˆ
    5. **ãƒˆãƒ¼ã‚¯ãƒ³ä½¿ç”¨é‡ç¢ºèª**: é¸æŠãƒ¢ãƒ‡ãƒ«ã§ã®ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã¨ã‚³ã‚¹ãƒˆã‚’æ¨å®š
    6. **ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰**: å‰å‡¦ç†æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ã‚’å„ç¨®å½¢å¼ã§ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰

    ### ğŸ¯ RAGæœ€é©åŒ–ã®ç‰¹å¾´
    - **è‡ªç„¶ãªæ–‡ç« çµåˆ**: ãƒ©ãƒ™ãƒ«ãªã—ã§èª­ã¿ã‚„ã™ã„æ–‡ç« ã¨ã—ã¦çµåˆ
    - **Gemini Embeddingå¯¾å¿œ**: `gemini-embedding-001`ç­‰ã«æœ€é©åŒ–
    - **æ¤œç´¢æ€§èƒ½å‘ä¸Š**: æ„å‘³çš„æ¤œç´¢ã®ç²¾åº¦å‘ä¸Š

    ### ğŸ’¡ æ¨å¥¨ãƒ¢ãƒ‡ãƒ«
    - **ã‚³ã‚¹ãƒˆé‡è¦–**: `gemini-2.0-flash`
    - **å“è³ªé‡è¦–**: `gemini-2.0-pro`
    - **OpenAIäº’æ›**: `gpt-4o-mini`, `gpt-4o` ï¼ˆOpenAI APIã‚­ãƒ¼ãŒå¿…è¦ï¼‰
    """

    # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆç‰¹æœ‰ã®èª¬æ˜
    dataset_specific = ""
    if dataset_type == "customer_support_faq":
        dataset_specific = """
    ### ğŸ’¬ ã‚«ã‚¹ã‚¿ãƒãƒ¼ã‚µãƒãƒ¼ãƒˆFAQã®ç‰¹å¾´
    - **FAQå½¢å¼**: è³ªå•ã¨å›ç­”ã®ãƒšã‚¢ã«ã‚ˆã‚‹æ§‹é€ 
    - **å®Ÿç”¨çš„ãªå†…å®¹**: å®Ÿéš›ã®é¡§å®¢ã‹ã‚‰ã®è³ªå•ã«åŸºã¥ã
    - **ç°¡æ½”ãªå›ç­”**: åˆ†ã‹ã‚Šã‚„ã™ãå®Ÿç”¨çš„ãªå›ç­”
        """
    elif dataset_type == "medical_qa":
        dataset_specific = """
    ### ğŸ¥ åŒ»ç™‚QAãƒ‡ãƒ¼ã‚¿ã®ç‰¹å¾´
    - **è¤‡é›‘ãªæ¨è«–**: Complex_CoTåˆ—ã«ã‚ˆã‚‹æ®µéšçš„æ¨è«–éç¨‹
    - **å°‚é–€ç”¨èª**: åŒ»ç™‚å°‚é–€ç”¨èªã®é©åˆ‡ãªå‡¦ç†
    - **è©³ç´°ãªå›ç­”**: åŒ»ç™‚æƒ…å ±ã«ç‰¹åŒ–ã—ãŸåŒ…æ‹¬çš„ãªå›ç­”
        """
    elif dataset_type == "sciq_qa":
        dataset_specific = """
    ### ğŸ”¬ SciQãƒ‡ãƒ¼ã‚¿ã®ç‰¹å¾´
    - **ç§‘å­¦ãƒ»æŠ€è¡“å•é¡Œ**: åŒ–å­¦ã€ç‰©ç†ã€ç”Ÿç‰©å­¦ã€æ•°å­¦ãªã©ã®åˆ†é‡ã‚’ã‚«ãƒãƒ¼
    - **å¤šè‚¢é¸æŠå½¢å¼**: distractoråˆ—ã«ã‚ˆã‚‹é¸æŠè‚¢å•é¡Œ
    - **è£œè¶³èª¬æ˜**: supportåˆ—ã«ã‚ˆã‚‹è©³ç´°ãªèƒŒæ™¯æƒ…å ±
    - **å¹…åºƒã„é›£æ˜“åº¦**: åŸºç¤ã‹ã‚‰å¿œç”¨ã¾ã§æ§˜ã€…ãªãƒ¬ãƒ™ãƒ«ã®ç§‘å­¦å•é¡Œ
        """
    elif dataset_type == "legal_qa":
        dataset_specific = """
    ### âš–ï¸ æ³•å¾‹ãƒ»åˆ¤ä¾‹QAãƒ‡ãƒ¼ã‚¿ã®ç‰¹å¾´
    - **æ³•å¾‹å°‚é–€ç”¨èª**: æ¡æ–‡ã€åˆ¤ä¾‹ã€æ³•çš„æ¦‚å¿µã®é©åˆ‡ãªå‡¦ç†
    - **è©³ç´°ãªå›ç­”**: æ³•çš„æ ¹æ‹ ã‚’å«ã‚€åŒ…æ‹¬çš„ãªèª¬æ˜
    - **æ­£ç¢ºæ€§é‡è¦–**: æ³•çš„æƒ…å ±ã®æ­£ç¢ºæ€§ã‚’ä¿æŒã—ãŸå‰å‡¦ç†
    - **å¼•ç”¨ãƒ»å‚ç…§**: æ¡æ–‡ç•ªå·ã‚„åˆ¤ä¾‹ç•ªå·ãªã©ã®æ³•çš„æ ¹æ‹ ã®ä¿è­·
        """

    st.markdown(basic_usage + dataset_specific)


# ==================================================
# ãƒšãƒ¼ã‚¸è¨­å®šé–¢æ•°ï¼ˆå…±é€šï¼‰
# ==================================================
def setup_page_config(dataset_type: str) -> None:
    """ãƒšãƒ¼ã‚¸è¨­å®šã®åˆæœŸåŒ–"""
    config_data = RAGConfig.get_config(dataset_type)

    try:
        st.set_page_config(
            page_title=f"{config_data['name']}å‰å‡¦ç†ï¼ˆå®Œå…¨ç‹¬ç«‹ç‰ˆï¼‰",
            page_icon=config_data['icon'],
            layout="wide",
            initial_sidebar_state="expanded"
        )
    except st.errors.StreamlitAPIException:
        pass


def setup_page_header(dataset_type: str) -> None:
    """ãƒšãƒ¼ã‚¸ãƒ˜ãƒƒãƒ€ãƒ¼ã®è¨­å®š"""
    config_data = RAGConfig.get_config(dataset_type)

    st.title(f"{config_data['icon']} {config_data['name']}å‰å‡¦ç†ã‚¢ãƒ—ãƒª")
    st.caption("RAGï¼ˆRetrieval-Augmented Generationï¼‰ç”¨ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç† - å®Œå…¨ç‹¬ç«‹ç‰ˆ")
    st.markdown("---")


def setup_sidebar_header(dataset_type: str) -> None:
    """ã‚µã‚¤ãƒ‰ãƒãƒ¼ãƒ˜ãƒƒãƒ€ãƒ¼ã®è¨­å®š"""
    config_data = RAGConfig.get_config(dataset_type)

    st.sidebar.title(f"{config_data['icon']} {config_data['name']}")
    st.sidebar.markdown("---")


# ==================================================
# ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ
# ==================================================
__all__ = [
    "select_model",
    "show_model_info",
    "estimate_token_usage",
    "display_statistics",
    "show_usage_instructions",
    "setup_page_config",
    "setup_page_header",
    "setup_sidebar_header",
]