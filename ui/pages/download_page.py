#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
download_page.py - RAGãƒ‡ãƒ¼ã‚¿ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãƒšãƒ¼ã‚¸
==============================================
HuggingFaceã‹ã‚‰ã®ãƒ‡ãƒ¼ã‚¿ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã¨å‰å‡¦ç†

æ©Ÿèƒ½:
- HuggingFaceãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
- ãƒ­ãƒ¼ã‚«ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã®ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
- ãƒ‡ãƒ¼ã‚¿ã®å‰å‡¦ç†ï¼ˆãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡ºã€ã‚¯ãƒ¬ãƒ³ã‚¸ãƒ³ã‚°ï¼‰
- OUTPUT/ãƒ•ã‚©ãƒ«ãƒ€ã¸ã®ä¿å­˜
"""

import streamlit as st
from datetime import datetime
from pathlib import Path

# ã‚µãƒ¼ãƒ“ã‚¹ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‹ã‚‰ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from services.dataset_service import (
    download_livedoor_corpus,
    load_livedoor_corpus,
    download_hf_dataset,
    extract_text_content,
    load_uploaded_file,
)
from services.file_service import (
    load_preprocessed_history,
    save_to_output,
)
from config import DATASET_CONFIGS


def show_rag_download_page():
    """ç”»é¢1: RAGãƒ‡ãƒ¼ã‚¿ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãƒ»å‰å‡¦ç†"""
    st.title("ğŸ“¥ RAGãƒ‡ãƒ¼ã‚¿ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãƒ»å‰å‡¦ç†ãƒ„ãƒ¼ãƒ«")
    st.caption(
        "HuggingFaceãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¾ãŸã¯ãƒ­ãƒ¼ã‚«ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãƒ»å‰å‡¦ç†ã—ã¦OUTPUT/ãƒ•ã‚©ãƒ«ãƒ€ã«ä¿å­˜"
    )

    # ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãƒ»å‰å‡¦ç†æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ
    st.subheader("ğŸ“¦ ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãƒ»å‰å‡¦ç†æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ")
    df_preprocessed = load_preprocessed_history()

    if not df_preprocessed.empty:
        st.dataframe(
            df_preprocessed, width='stretch', hide_index=True, height=200
        )
    else:
        st.info(
            "ã¾ã å‰å‡¦ç†æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚ä¸‹è¨˜ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãƒ»å‰å‡¦ç†ã—ã¦ãã ã•ã„ã€‚"
        )

    st.divider()
    st.caption("ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®è‡ªå‹•ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ â†’ å‰å‡¦ç† â†’ OUTPUT/ãƒ•ã‚©ãƒ«ãƒ€ã«ä¿å­˜")

    # ã‚µã‚¤ãƒ‰ãƒãƒ¼ï¼šãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹é¸æŠ
    with st.sidebar:
        st.header("ğŸ“‚ ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹é¸æŠ")

        # ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹é¸æŠï¼ˆãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ or ãƒ­ãƒ¼ã‚«ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ï¼‰
        data_source = st.radio(
            "ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ã‚’é¸æŠ",
            options=["dataset", "local_file"],
            format_func=lambda x: "ğŸŒ ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ"
            if x == "dataset"
            else "ğŸ“ ãƒ­ãƒ¼ã‚«ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«",
            key="data_source_selector",
        )

        st.divider()

        if data_source == "dataset":
            # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆé¸æŠ
            st.subheader("ğŸ“¥ ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ")

            dataset_options = list(DATASET_CONFIGS.keys())
            dataset_labels = {
                key: f"{DATASET_CONFIGS[key]['icon']} {DATASET_CONFIGS[key]['name']}"
                for key in dataset_options
            }

            selected_dataset = st.radio(
                "ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ",
                options=dataset_options,
                format_func=lambda x: dataset_labels[x],
                label_visibility="collapsed",
            )

            uploaded_file = None
            config = DATASET_CONFIGS[selected_dataset]

        else:
            # ãƒ­ãƒ¼ã‚«ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
            st.subheader("ğŸ“ ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰")

            uploaded_file = st.file_uploader(
                "ãƒ•ã‚¡ã‚¤ãƒ«ã‚’é¸æŠ",
                type=["csv", "txt", "json", "jsonl"],
                help="CSV, TXT, JSON, JSONLå½¢å¼ã«å¯¾å¿œ",
            )

            selected_dataset = "custom_upload"
            config = {
                "name": "ã‚«ã‚¹ã‚¿ãƒ ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰",
                "icon": "ğŸ“",
                "description": "ãƒ­ãƒ¼ã‚«ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰Q/Aãƒšã‚¢ã‚’ç”Ÿæˆ",
                "text_field": "Combined_Text",
                "title_field": None,
                "sample_size": 0,
                "min_text_length": 50,
            }

    # ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ã®è¡¨ç¤ºå
    data_source_name = (
        config["name"]
        if data_source == "dataset"
        else (uploaded_file.name if uploaded_file else "æœªé¸æŠ")
    )

    # ãƒ¡ã‚¤ãƒ³ã‚¨ãƒªã‚¢ï¼šå‡¦ç†ã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼ˆä¸Šéƒ¨ï¼‰
    st.subheader("âš™ï¸ å‡¦ç†ã‚ªãƒ—ã‚·ãƒ§ãƒ³")

    # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæƒ…å ±ã¨å‡¦ç†ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã‚’æ¨ªä¸¦ã³
    col_info, col_opts = st.columns([1, 1])

    with col_info:
        if data_source == "dataset":
            st.info(f"""
**{config["name"]}**

{config["description"]}

- ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹: {config.get("hf_dataset", "ç›´æ¥ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰")}
- ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã‚µãƒ³ãƒ—ãƒ«æ•°: {config["sample_size"]:,} ä»¶
            """)
        else:
            if uploaded_file:
                st.info(f"""
**ğŸ“ ãƒ­ãƒ¼ã‚«ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«**

ãƒ•ã‚¡ã‚¤ãƒ«å: {uploaded_file.name}

ãƒ•ã‚¡ã‚¤ãƒ«å½¢å¼: {uploaded_file.name.split(".")[-1].upper()}
                """)
            else:
                st.warning("ãƒ•ã‚¡ã‚¤ãƒ«ã‚’é¸æŠã—ã¦ãã ã•ã„")

    with col_opts:
        if data_source == "dataset":
            sample_size = st.number_input(
                "ã‚µãƒ³ãƒ—ãƒ«æ•°",
                min_value=10,
                max_value=10000,
                value=config["sample_size"],
                step=50,
                help="ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ä»¶æ•°",
            )
        else:
            sample_size = st.number_input(
                "æœ€å¤§ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ•°ï¼ˆä¸Šé™: 1,000ä»¶ï¼‰",
                min_value=1,
                max_value=1000,
                value=100,
                step=10,
                help="å‡¦ç†ã™ã‚‹æœ€å¤§ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ•°ã€‚å…¨ä»¶å‡¦ç†ã™ã‚‹å ´åˆã¯1,000ã«è¨­å®š",
            )

        min_length = st.number_input(
            "æœ€å°ãƒ†ã‚­ã‚¹ãƒˆé•·",
            min_value=10,
            max_value=1000,
            value=config["min_text_length"],
            step=10,
            help="ã“ã®é•·ã•æœªæº€ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’é™¤å¤–",
        )

        remove_duplicates = st.checkbox(
            "é‡è¤‡ã‚’é™¤å»", value=True, help="å®Œå…¨ã«åŒã˜ãƒ†ã‚­ã‚¹ãƒˆã‚’é™¤å¤–"
        )

    st.divider()
    st.caption("ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®è‡ªå‹•ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ â†’ å‰å‡¦ç† â†’ OUTPUT/ãƒ•ã‚©ãƒ«ãƒ€ã«ä¿å­˜")

    # APIè²»ç”¨ç¯€ç´„ã®ãŸã‚ã®èª¬æ˜ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ï¼ˆãƒ”ãƒ³ã‚¯èƒŒæ™¯ï¼‰
    pink_message_html = """
    <div style="background-color:#FFC0CB; padding:10px; border-radius:5px; border:1px solid #FF69B4;">
        <p style="color:#8B0000; font-weight:bold; margin-bottom:0px;">
            ã™ã§ã«ã€HuggingFaceã‹ã‚‰ä¸‹è¨˜ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¦é…ç½®ã€<br>
            Q/Aãƒšã‚¢ã‚’ä½œæˆæ¸ˆã¿ã€Qdrantã«embeddingãƒ™ã‚¯ãƒˆãƒ«ãƒ‡ãƒ¼ã‚¿ã‚’ç™»éŒ²æ¸ˆã¿ã§ã™ã€‚<br>
            ãƒ»Wikipediaæ—¥æœ¬èªç‰ˆ<br>
            ãƒ»æ—¥æœ¬èªWebãƒ†ã‚­ã‚¹ãƒˆï¼ˆCC100ï¼‰<br>
            ãƒ»CC-Newsï¼ˆè‹±èªãƒ‹ãƒ¥ãƒ¼ã‚¹ï¼‰<br>
            ãƒ»Livedoorãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚³ãƒ¼ãƒ‘ã‚¹<br>
            ã‚ˆã£ã¦ã€ã“ã“ã®é€ä¿¡ãƒœã‚¿ãƒ³ã¯disableã«ã—ã¦ã‚ã‚Šã¾ã™ã€‚ï¼ˆAPIè²»ç”¨ãŒã‹ã‹ã‚Šéãã‚‹ã®ã§ğŸ˜¹ï¼‰
        </p>
    </div>
    """
    st.markdown(pink_message_html, unsafe_allow_html=True)
    st.write("") # 1è¡Œç©ºã‘ã‚‹

    # å®Ÿè¡Œãƒœã‚¿ãƒ³
    run_download = st.button(
        "ğŸš€ ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ï¼†å‰å‡¦ç†é–‹å§‹", type="primary", width='stretch', disabled=True
    )

    st.divider()

    # ãƒ¡ã‚¤ãƒ³ã‚¨ãƒªã‚¢ï¼šå‡¦ç†æƒ…å ±ã¨å±¥æ­´ï¼ˆä¸‹éƒ¨ï¼‰
    col1, col2 = st.columns([1, 2])

    with col1:
        st.subheader("ğŸ“Š å‡¦ç†æƒ…å ±")
        info_container = st.container()

    with col2:
        st.subheader("ğŸ“œ å‡¦ç†å±¥æ­´ãƒ»é€²æ—")
        log_container = st.container()

    # åˆæœŸæƒ…å ±è¡¨ç¤º
    with info_container:
        st.metric("é¸æŠãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ", config["name"])
        st.metric("å‡¦ç†äºˆå®šä»¶æ•°", f"{sample_size:,} ä»¶")
        if "result_count" in st.session_state:
            st.metric("å‡¦ç†å®Œäº†ä»¶æ•°", f"{st.session_state['result_count']:,} ä»¶")

    # ãƒ­ã‚°è¡¨ç¤ºç”¨
    if "logs" not in st.session_state:
        st.session_state["logs"] = []

    def add_log(message: str):
        """ãƒ­ã‚°ã‚’è¿½åŠ """
        timestamp = datetime.now().strftime("%H:%M:%S")
        st.session_state["logs"].append(f"[{timestamp}] {message}")

    # å‡¦ç†å®Ÿè¡Œ
    if run_download:
        st.session_state["logs"] = []  # ãƒ­ã‚°ã‚¯ãƒªã‚¢
        add_log(f"ğŸš€ å‡¦ç†é–‹å§‹: {data_source_name}")

        # ãƒ­ãƒ¼ã‚«ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã®å ´åˆã¯ãƒ•ã‚¡ã‚¤ãƒ«ãƒã‚§ãƒƒã‚¯
        if data_source == "local_file" and not uploaded_file:
            st.error("ãƒ•ã‚¡ã‚¤ãƒ«ã‚’é¸æŠã—ã¦ãã ã•ã„")
            st.stop()

        try:
            # ===================================================================
            # ãƒ­ãƒ¼ã‚«ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã®å ´åˆ
            # ===================================================================
            if data_source == "local_file":
                # ã‚¹ãƒ†ãƒƒãƒ—1: ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿
                with st.spinner("ğŸ“ ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ä¸­..."):
                    add_log(f"ğŸ“ ãƒ­ãƒ¼ã‚«ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿: {uploaded_file.name}")
                    df = load_uploaded_file(uploaded_file)
                    add_log(f"âœ… {len(df)} ä»¶ã®ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ")

                    # ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
                    if len(df) > sample_size:
                        df = df.head(sample_size)
                        add_log(f"ğŸ“Š {len(df)} ä»¶ã«åˆ¶é™ã—ã¾ã—ãŸ")

                # ã‚¹ãƒ†ãƒƒãƒ—2: question, answerã‚«ãƒ©ãƒ ã®ç¢ºèªã¨æŠ½å‡º
                with st.spinner("âš™ï¸ ãƒ‡ãƒ¼ã‚¿å‡¦ç†ä¸­..."):
                    add_log("âš™ï¸ question, answerã‚«ãƒ©ãƒ ã‚’ç¢ºèªä¸­...")

                    # question, answerã‚«ãƒ©ãƒ ã‚’æ¢ã™
                    question_col = None
                    answer_col = None

                    for col in df.columns:
                        col_lower = col.lower()
                        if "question" in col_lower and not question_col:
                            question_col = col
                        if "answer" in col_lower and not answer_col:
                            answer_col = col

                    # question, answerã‚«ãƒ©ãƒ ãŒãªã„å ´åˆã¯é€šå¸¸å‡¦ç†
                    if question_col and answer_col:
                        add_log(f"  âœ… questionã‚«ãƒ©ãƒ : {question_col}")
                        add_log(f"  âœ… answerã‚«ãƒ©ãƒ : {answer_col}")

                        # question, answerã®ã¿æŠ½å‡º
                        df_qa = df[[question_col, answer_col]].copy()
                        df_qa.columns = ["question", "answer"]  # ã‚«ãƒ©ãƒ åã‚’çµ±ä¸€

                        # ç©ºã®ãƒ‡ãƒ¼ã‚¿ã‚’é™¤å¤–
                        before_len = len(df_qa)
                        df_qa = df_qa.dropna(subset=["question", "answer"])
                        df_qa = df_qa[
                            (df_qa["question"].str.strip() != "")
                            & (df_qa["answer"].str.strip() != "")
                        ]
                        removed = before_len - len(df_qa)
                        if removed > 0:
                            add_log(
                                f"ğŸ“Š ç©ºãƒ‡ãƒ¼ã‚¿é™¤å¤–: {removed} ä»¶ã‚’é™¤å¤–ï¼ˆæ®‹ã‚Š {len(df_qa)} ä»¶ï¼‰"
                            )

                        # é‡è¤‡é™¤å»ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
                        if remove_duplicates:
                            before_len = len(df_qa)
                            df_qa = df_qa.drop_duplicates()
                            removed = before_len - len(df_qa)
                            if removed > 0:
                                add_log(
                                    f"ğŸ“Š é‡è¤‡é™¤å»: {removed} ä»¶ã‚’é™¤å¤–ï¼ˆæ®‹ã‚Š {len(df_qa)} ä»¶ï¼‰"
                                )

                        df_qa = df_qa.reset_index(drop=True)
                        add_log(f"âœ… ãƒ‡ãƒ¼ã‚¿å‡¦ç†å®Œäº†: {len(df_qa)} ä»¶")

                        # ã‚¹ãƒ†ãƒƒãƒ—3: qa_output/ã«ä¿å­˜
                        with st.spinner("ğŸ’¾ ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜ä¸­..."):
                            add_log("ğŸ’¾ qa_output/ãƒ•ã‚©ãƒ«ãƒ€ã«ä¿å­˜ä¸­...")

                            qa_output_dir = Path("qa_output")
                            qa_output_dir.mkdir(exist_ok=True)

                            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                            csv_filename = f"qa_pairs_upload_{timestamp}.csv"
                            csv_path = qa_output_dir / csv_filename

                            df_qa.to_csv(csv_path, index=False, encoding="utf-8-sig")
                            add_log(f"  ğŸ“„ CSVä¿å­˜: {csv_filename}")
                            add_log("âœ… ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜å®Œäº†")

                        # çµæœã‚’ä¿å­˜
                        st.session_state["result_count"] = len(df_qa)
                        st.session_state["qa_saved_files"] = {"csv": str(csv_path)}
                        st.session_state["qa_count"] = len(df_qa)
                        st.session_state["processed_df"] = df_qa

                        add_log("ğŸ‰ å…¨å‡¦ç†å®Œäº†ï¼")
                    else:
                        add_log(
                            "âš ï¸ Q/Aã‚«ãƒ©ãƒ ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚ãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã¨ã—ã¦å‡¦ç†ã—ã¾ã™"
                        )

                        # ãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã¨ã—ã¦ä¿å­˜
                        with st.spinner("ğŸ’¾ ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜ä¸­..."):
                            output_dir = Path("OUTPUT")
                            output_dir.mkdir(exist_ok=True)

                            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                            csv_filename = f"preprocessed_upload_{timestamp}.csv"
                            csv_path = output_dir / csv_filename

                            df.to_csv(csv_path, index=False, encoding="utf-8-sig")
                            add_log(f"  ğŸ“„ CSVä¿å­˜: {csv_filename}")

                            st.session_state["result_count"] = len(df)
                            st.session_state["saved_files"] = {"csv": str(csv_path)}
                            st.session_state["processed_df"] = df

                            add_log("âœ… ãƒ‡ãƒ¼ã‚¿ä¿å­˜å®Œäº†")
                            add_log("ğŸ‰ å…¨å‡¦ç†å®Œäº†ï¼")

            # ===================================================================
            # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®å ´åˆï¼šæ—¢å­˜ã®å‡¦ç†ãƒ•ãƒ­ãƒ¼
            # ===================================================================
            else:
                # ã‚¹ãƒ†ãƒƒãƒ—1: ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
                with st.spinner("ğŸ“¥ ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ä¸­..."):
                    if selected_dataset == "livedoor":
                        # Livedoorç‰¹åˆ¥å‡¦ç†
                        add_log("Livedoorã‚³ãƒ¼ãƒ‘ã‚¹ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ä¸­...")
                        data_dir = download_livedoor_corpus("datasets")
                        add_log("âœ… ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å®Œäº†")

                        add_log("ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿ä¸­...")
                        df = load_livedoor_corpus(data_dir)
                        add_log(f"âœ… {len(df)} ä»¶ã®ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ")

                        # ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
                        if sample_size < len(df):
                            df = df.sample(n=sample_size, random_state=42)
                            add_log(f"ğŸ“Š {len(df)} ä»¶ã«ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã—ã¾ã—ãŸ")

                    else:
                        # HuggingFaceã‹ã‚‰ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
                        df = download_hf_dataset(
                            config["hf_dataset"],
                            config.get("hf_config"),
                            config["split"],
                            sample_size,
                            add_log,
                        )

                    add_log(f"âœ… ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿å®Œäº†: {len(df)} ä»¶")

                # ã‚¹ãƒ†ãƒƒãƒ—2: å‰å‡¦ç†
                with st.spinner("âš™ï¸ å‰å‡¦ç†å®Ÿè¡Œä¸­..."):
                    add_log("âš™ï¸ å‰å‡¦ç†é–‹å§‹")

                    add_log("ãƒ†ã‚­ã‚¹ãƒˆã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’æŠ½å‡ºä¸­...")
                    df_processed = extract_text_content(df, config)
                    add_log(f"âœ… ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡ºå®Œäº†: {len(df_processed)} ä»¶")

                    # çŸ­æ–‡é™¤å¤–
                    before_len = len(df_processed)
                    df_processed = df_processed[
                        df_processed["Combined_Text"].str.len() >= min_length
                    ]
                    removed = before_len - len(df_processed)
                    if removed > 0:
                        add_log(
                            f"ğŸ“Š çŸ­æ–‡é™¤å¤–: {removed} ä»¶ã‚’é™¤å¤–ï¼ˆæ®‹ã‚Š {len(df_processed)} ä»¶ï¼‰"
                        )

                    # é‡è¤‡é™¤å»
                    if remove_duplicates:
                        before_len = len(df_processed)
                        df_processed = df_processed.drop_duplicates(
                            subset=["Combined_Text"]
                        )
                        removed = before_len - len(df_processed)
                        if removed > 0:
                            add_log(
                                f"ğŸ“Š é‡è¤‡é™¤å»: {removed} ä»¶ã‚’é™¤å¤–ï¼ˆæ®‹ã‚Š {len(df_processed)} ä»¶ï¼‰"
                            )

                    df_processed = df_processed.reset_index(drop=True)
                    add_log(f"âœ… å‰å‡¦ç†å®Œäº†: {len(df_processed)} ä»¶")

                # ã‚¹ãƒ†ãƒƒãƒ—3: OUTPUTä¿å­˜
                with st.spinner("ğŸ’¾ ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜ä¸­..."):
                    add_log("ğŸ’¾ OUTPUTãƒ•ã‚©ãƒ«ãƒ€ã«ä¿å­˜ä¸­...")
                    saved_files = save_to_output(df_processed, selected_dataset)
                    add_log("âœ… ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜å®Œäº†")

                # çµæœã‚’ä¿å­˜
                st.session_state["result_count"] = len(df_processed)
                st.session_state["saved_files"] = saved_files
                st.session_state["processed_df"] = df_processed

                add_log("ğŸ‰ å…¨å‡¦ç†å®Œäº†ï¼")

        except Exception as e:
            add_log(f"âŒ ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿ: {str(e)}")
            st.error(f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}")

    # ãƒ­ã‚°è¡¨ç¤º
    with log_container:
        if st.session_state["logs"]:
            log_text = "\n".join(st.session_state["logs"])
            st.text_area("å‡¦ç†ãƒ­ã‚°", value=log_text, height=400, disabled=True)
        else:
            st.info("å‡¦ç†ã‚’é–‹å§‹ã™ã‚‹ã¨ã“ã“ã«ãƒ­ã‚°ãŒè¡¨ç¤ºã•ã‚Œã¾ã™")

    # çµæœè¡¨ç¤º
    if "saved_files" in st.session_state:
        st.divider()
        st.subheader("ğŸ“ ä¿å­˜ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«")

        saved_files = st.session_state["saved_files"]
        for file_type, file_path in saved_files.items():
            st.success(f"âœ… {file_type.upper()}: {file_path}")

    if "qa_saved_files" in st.session_state:
        st.divider()
        st.subheader("ğŸ“ ä¿å­˜ã•ã‚ŒãŸQ/Aãƒ•ã‚¡ã‚¤ãƒ«")

        qa_files = st.session_state["qa_saved_files"]
        for file_type, file_path in qa_files.items():
            st.success(f"âœ… {file_type.upper()}: {file_path}")

    # ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼è¡¨ç¤º
    if "processed_df" in st.session_state:
        st.divider()
        st.subheader("ğŸ“‹ ãƒ‡ãƒ¼ã‚¿ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ï¼ˆæœ€åˆã®10ä»¶ï¼‰")
        df_preview = st.session_state["processed_df"].head(10)
        st.dataframe(df_preview, width='stretch', hide_index=True)