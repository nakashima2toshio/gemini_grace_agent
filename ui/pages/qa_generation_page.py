#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
qa_generation_page.py - Q/Aç”Ÿæˆãƒšãƒ¼ã‚¸
=====================================
Q/Aãƒšã‚¢ã®è‡ªå‹•ç”Ÿæˆæ©Ÿèƒ½

æ©Ÿèƒ½:
- ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¾ãŸã¯ãƒ­ãƒ¼ã‚«ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰Q/Aç”Ÿæˆ
- Celeryä¸¦åˆ—å‡¦ç†å¯¾å¿œ
- ã‚«ãƒãƒ¬ãƒ¼ã‚¸åˆ†æ
"""

from datetime import datetime
from pathlib import Path

import streamlit as st

# ã‚µãƒ¼ãƒ“ã‚¹ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‹ã‚‰ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from services.file_service import load_qa_output_history
from services.qa_service import run_advanced_qa_generation
from config import DATASET_CONFIGS, ModelConfig


def show_qa_generation_page():
    """ç”»é¢2: Q/Aç”Ÿæˆ"""

    st.title("ğŸ¤– Q/Aç”Ÿæˆãƒ„ãƒ¼ãƒ«")
    st.caption(
        "æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã¾ãŸã¯ãƒ­ãƒ¼ã‚«ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰Q/Aãƒšã‚¢ã‚’ç”Ÿæˆï¼ˆmake_qa.pyæ©Ÿèƒ½ï¼‰"
    )

    # æœ€æ–°ã®Q/Aå±¥æ­´è¡¨ç¤º
    st.subheader("ğŸ“‹ æœ€æ–°ã®Q&Aãƒšã‚¢")
    df_history = load_qa_output_history()

    if not df_history.empty:
        st.dataframe(df_history, width='stretch', hide_index=True, height=200)
    else:
        st.info("ã¾ã Q&Aãƒšã‚¢ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")

    st.divider()

    # ã‚µã‚¤ãƒ‰ãƒãƒ¼ï¼šå…¥åŠ›ã‚½ãƒ¼ã‚¹é¸æŠ
    with st.sidebar:
        st.header("ğŸ“‚ å…¥åŠ›ã‚½ãƒ¼ã‚¹é¸æŠ")

        # å…¥åŠ›ã‚½ãƒ¼ã‚¹é¸æŠ
        input_source = st.radio(
            "å…¥åŠ›ã‚½ãƒ¼ã‚¹ã‚’é¸æŠ",
            options=["dataset", "local_file"],
            format_func=lambda x: "ğŸŒ ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ"
            if x == "dataset"
            else "ğŸ“ ãƒ­ãƒ¼ã‚«ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«",
            key="input_source_selector",
        )

        st.divider()

        if input_source == "dataset":
            # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆé¸æŠ
            st.subheader("ğŸ“¥ ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ")

            dataset_options = list(DATASET_CONFIGS.keys())
            dataset_labels = {
                key: f"{DATASET_CONFIGS[key]['icon']} {DATASET_CONFIGS[key]['name']}"
                for key in dataset_options
            }

            selected_dataset = st.radio(
                "Q/Aç”Ÿæˆã™ã‚‹ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ",
                options=dataset_options,
                format_func=lambda x: dataset_labels[x],
                label_visibility="collapsed",
            )

            uploaded_file = None
            input_file_path = None

        else:
            # ãƒ­ãƒ¼ã‚«ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
            st.subheader("ğŸ“ ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰")

            uploaded_file = st.file_uploader(
                "ãƒ•ã‚¡ã‚¤ãƒ«ã‚’é¸æŠ",
                type=["csv", "txt", "json", "jsonl"],
                help="CSV, TXT, JSON, JSONLå½¢å¼ã«å¯¾å¿œ",
            )

            selected_dataset = None
            input_file_path = None

        # =========================================================
        # Q/Aç”Ÿæˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼ˆa02_make_qa_para.pyç›¸å½“ï¼‰
        # =========================================================
        st.divider()
        st.subheader("ğŸš€ Q/Aç”Ÿæˆè¨­å®š")

        # Celeryè¨­å®š
        use_celery = st.checkbox(
            "Celeryä¸¦åˆ—å‡¦ç†", value=True, help="è¤‡æ•°ãƒ¯ãƒ¼ã‚«ãƒ¼ã§ä¸¦åˆ—å‡¦ç†"
        )

        if use_celery:
            celery_workers = st.number_input(
                "Celeryãƒ¯ãƒ¼ã‚«ãƒ¼æ•°",
                min_value=1,
                max_value=48,
                value=24,  # Gemini APIãƒ¬ãƒ¼ãƒˆåˆ¶é™å¯¾ç­–ã®ãŸã‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã‚’24ã«è¨­å®š
                step=1,
                help="ä¸¦åˆ—å‡¦ç†ã™ã‚‹ãƒ¯ãƒ¼ã‚«ãƒ¼æ•°ï¼ˆGeminiæ¨å¥¨: 24ï¼‰",
            )
        else:
            celery_workers = 1

        col_a1, col_a2 = st.columns(2)
        with col_a1:
            batch_chunks = st.number_input(
                "ãƒãƒƒãƒãƒãƒ£ãƒ³ã‚¯æ•°",
                min_value=1,
                max_value=5,
                value=3,
                step=1,
                help="1å›ã®APIã§å‡¦ç†ã™ã‚‹ãƒãƒ£ãƒ³ã‚¯æ•°",
            )

            max_docs = st.number_input(
                "æœ€å¤§ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ•°",
                min_value=1,
                max_value=10000,
                value=100,
                step=10,
                help="å‡¦ç†ã™ã‚‹æœ€å¤§ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ•°",
            )

        with col_a2:
            min_tokens = st.number_input(
                "æœ€å°ãƒˆãƒ¼ã‚¯ãƒ³æ•°",
                min_value=50,
                max_value=500,
                value=150,
                step=10,
                help="çµ±åˆå¯¾è±¡ã®æœ€å°ãƒˆãƒ¼ã‚¯ãƒ³æ•°",
            )

            max_tokens = st.number_input(
                "æœ€å¤§ãƒˆãƒ¼ã‚¯ãƒ³æ•°",
                min_value=100,
                max_value=1000,
                value=400,
                step=50,
                help="çµ±åˆå¾Œã®æœ€å¤§ãƒˆãƒ¼ã‚¯ãƒ³æ•°",
            )

        merge_chunks = st.checkbox(
            "ãƒãƒ£ãƒ³ã‚¯çµ±åˆ", value=True, help="å°ã•ã„ãƒãƒ£ãƒ³ã‚¯ã‚’çµ±åˆ"
        )

        coverage_threshold = st.slider(
            "ã‚«ãƒãƒ¬ãƒ¼ã‚¸é–¾å€¤",
            min_value=0.0,
            max_value=1.0,
            value=0.58,
            step=0.01,
            help="ã‚«ãƒãƒ¬ãƒ¼ã‚¸åˆ¤å®šã®é¡ä¼¼åº¦é–¾å€¤",
        )

        qa_model = st.selectbox(
            "ãƒ¢ãƒ‡ãƒ«",
            options=ModelConfig.AVAILABLE_MODELS,
            index=ModelConfig.AVAILABLE_MODELS.index("gemini-2.0-flash") if "gemini-2.0-flash" in ModelConfig.AVAILABLE_MODELS else 0,
            help="Q/Aç”Ÿæˆã«ä½¿ç”¨ã™ã‚‹ãƒ¢ãƒ‡ãƒ«ï¼ˆGemini APIå­¦ç¿’ç”¨ï¼‰",
        )

        analyze_coverage = st.checkbox(
            "ã‚«ãƒãƒ¬ãƒ¼ã‚¸åˆ†æ", value=True, help="Q/Aãƒšã‚¢ã®ã‚«ãƒãƒ¬ãƒ¼ã‚¸ã‚’åˆ†æ"
        )

    # ãƒ¡ã‚¤ãƒ³ã‚¨ãƒªã‚¢ï¼šå‡¦ç†ã‚ªãƒ—ã‚·ãƒ§ãƒ³
    st.subheader("âš™ï¸ å…¥åŠ›æƒ…å ±")

    # å…¥åŠ›æƒ…å ±è¡¨ç¤º
    col_info, col_opts = st.columns([1, 1])

    with col_info:
        if input_source == "dataset":
            config = DATASET_CONFIGS[selected_dataset]
            st.info(f"""
**{config["name"]}**

{config["description"]}

- ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹: {config.get("hf_dataset", "ç›´æ¥ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰")}
            """)
        else:
            if uploaded_file:
                st.info(f"""
**ğŸ“ ãƒ­ãƒ¼ã‚«ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«**

ãƒ•ã‚¡ã‚¤ãƒ«å: {uploaded_file.name}

ãƒ•ã‚¡ã‚¤ãƒ«å½¢å¼: {uploaded_file.name.split(".")[-1].upper()}
                """)
            else:
                st.warning("ãƒ•ã‚¡ã‚¤ãƒ«ã‚’é¸æŠã—ã¦ãã ã•ã„")

    with col_opts:
        st.markdown("**å‡¦ç†è¨­å®š**")
        st.write(f"- Celeryä¸¦åˆ—å‡¦ç†: {'æœ‰åŠ¹' if use_celery else 'ç„¡åŠ¹'}")
        if use_celery:
            st.write(f"- ãƒ¯ãƒ¼ã‚«ãƒ¼æ•°: {celery_workers}")
        st.write(f"- ãƒãƒƒãƒãƒãƒ£ãƒ³ã‚¯æ•°: {batch_chunks}")
        st.write(f"- æœ€å¤§ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ•°: {max_docs}")
        st.write(f"- ãƒ¢ãƒ‡ãƒ«: {qa_model}")
        st.write(f"- ã‚«ãƒãƒ¬ãƒ¼ã‚¸åˆ†æ: {'å®Ÿè¡Œ' if analyze_coverage else 'ã‚¹ã‚­ãƒƒãƒ—'}")

    # å®Ÿè¡Œä¸­ãƒ•ãƒ©ã‚°ã®åˆæœŸåŒ–
    if "qa_generation_running" not in st.session_state:
        st.session_state["qa_generation_running"] = False

    # APIè²»ç”¨ç¯€ç´„ã®ãŸã‚ã®èª¬æ˜ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ï¼ˆãƒ”ãƒ³ã‚¯èƒŒæ™¯ï¼‰
    pink_message_html = """
    <div style="background-color:#FFC0CB; padding:10px; border-radius:5px; border:1px solid #FF69B4;">
        <p style="color:#8B0000; font-weight:bold; margin-bottom:0px;">
            ã™ã§ã«ã€HuggingFaceã‹ã‚‰ä¸‹è¨˜ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¦é…ç½®ã€<br>
            Q/Aãƒšã‚¢ã‚’ä½œæˆæ¸ˆã¿ã€Qdrantã«embeddingãƒ™ã‚¯ãƒˆãƒ«ãƒ‡ãƒ¼ã‚¿ã‚’ç™»éŒ²æ¸ˆã¿ã§ã™ã€‚<br>
            ãƒ»Wikipediaæ—¥æœ¬èªç‰ˆ<br>
            ãƒ»æ—¥æœ¬èªWebãƒ†ã‚­ã‚¹ãƒˆï¼ˆCC100ï¼‰<br>
            ãƒ»CC-Newsï¼ˆè‹±èªãƒ‹ãƒ¥ãƒ¼ã‚¹ï¼‰<br>
            ãƒ»Livedoorãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚³ãƒ¼ãƒ‘ã‚¹<br>
            ã‚ˆã£ã¦ã€ã“ã“ã®é€ä¿¡ãƒœã‚¿ãƒ³ã¯disableã«ã—ã¦ã‚ã‚Šã¾ã™ã€‚ï¼ˆAPIè²»ç”¨ãŒã‹ã‹ã‚Šéãã‚‹ã®ã§ğŸ˜¹ï¼‰
        </p>
    </div>
    """
    st.markdown(pink_message_html, unsafe_allow_html=True)
    st.write("") # 1è¡Œç©ºã‘ã‚‹

    # å®Ÿè¡Œãƒœã‚¿ãƒ³ï¼ˆå®Ÿè¡Œä¸­ã¯ç„¡åŠ¹åŒ–ï¼‰
    run_qa_generation = st.button(
        "ğŸš€ Q/Aç”Ÿæˆé–‹å§‹" if not st.session_state["qa_generation_running"] else "â³ å‡¦ç†ä¸­...",
        type="primary",
        width='stretch',
        disabled=True # st.session_state["qa_generation_running"]
    )

    st.divider()

    # ãƒ¡ã‚¤ãƒ³ã‚¨ãƒªã‚¢ï¼šé€²æ—è¡¨ç¤º
    st.subheader("ğŸ“œ å‡¦ç†å±¥æ­´ãƒ»é€²æ—")
    log_container = st.container()

    # ãƒ­ã‚°è¡¨ç¤ºç”¨
    if "qa_logs" not in st.session_state:
        st.session_state["qa_logs"] = []

    def add_log(message: str):
        """ãƒ­ã‚°ã‚’è¿½åŠ ï¼ˆæœ€æ–°1000è¡Œã®ã¿ä¿æŒï¼‰"""
        timestamp = datetime.now().strftime("%H:%M:%S")
        st.session_state["qa_logs"].append(f"[{timestamp}] {message}")

        # æœ€æ–°1000è¡Œã®ã¿ä¿æŒï¼ˆãƒ¡ãƒ¢ãƒªç¯€ç´„ï¼‹ãƒ¬ãƒ³ãƒ€ãƒªãƒ³ã‚°é«˜é€ŸåŒ–ï¼‰
        if len(st.session_state["qa_logs"]) > 1000:
            st.session_state["qa_logs"] = st.session_state["qa_logs"][-1000:]

    # å‡¦ç†å®Ÿè¡Œ
    if run_qa_generation and not st.session_state["qa_generation_running"]:
        st.session_state["qa_generation_running"] = True  # å®Ÿè¡Œé–‹å§‹
        st.session_state["qa_logs"] = []  # ãƒ­ã‚°ã‚¯ãƒªã‚¢

        # å…¥åŠ›ãƒã‚§ãƒƒã‚¯
        if input_source == "local_file" and not uploaded_file:
            st.error("ãƒ•ã‚¡ã‚¤ãƒ«ã‚’é¸æŠã—ã¦ãã ã•ã„")
            st.stop()

        try:
            add_log("ğŸš€ Q/Aç”Ÿæˆå‡¦ç†é–‹å§‹")

            # ãƒ­ãƒ¼ã‚«ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã®å ´åˆã€ä¸€æ™‚ä¿å­˜
            if input_source == "local_file":
                with st.spinner("ğŸ“ ãƒ•ã‚¡ã‚¤ãƒ«æº–å‚™ä¸­..."):
                    add_log(f"ğŸ“ ãƒ­ãƒ¼ã‚«ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿: {uploaded_file.name}")

                    # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
                    temp_dir = Path("temp_uploads")
                    temp_dir.mkdir(exist_ok=True)

                    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                    temp_filename = f"temp_qa_{timestamp}_{uploaded_file.name}"
                    temp_path = temp_dir / temp_filename

                    # ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¿å­˜
                    with open(temp_path, "wb") as f:
                        f.write(uploaded_file.getbuffer())

                    input_file_path = str(temp_path)
                    add_log(f"  âœ… ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆ: {temp_filename}")

            # make_qa.pyã‚’å®Ÿè¡Œ
            add_log("ğŸš€ make_qa.py (pipeline)å®Ÿè¡Œé–‹å§‹")

            # ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ãƒãƒ¼ç”¨ã®ã‚³ãƒ³ãƒ†ãƒŠã¨ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹
            progress_container = st.empty()
            progress_bar = progress_container.progress(0)

            # é€²æ—ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯é–¢æ•°
            def update_progress(current: int, total: int):
                """é€²æ—ãƒãƒ¼ã‚’æ›´æ–°"""
                if total > 0:
                    progress = current / total
                    progress_bar.progress(progress, text=f"é€²æ—: {current}/{total} ã‚¿ã‚¹ã‚¯å®Œäº†")

            with st.spinner("ğŸš€ Q/Aãƒšã‚¢ç”Ÿæˆä¸­ï¼ˆmake_qa.pyå®Ÿè¡Œï¼‰..."):
                result = run_advanced_qa_generation(
                    dataset=selected_dataset if input_source == "dataset" else None,
                    input_file=input_file_path,
                    use_celery=use_celery,
                    celery_workers=celery_workers,
                    batch_chunks=batch_chunks,
                    max_docs=max_docs,
                    merge_chunks=merge_chunks,
                    min_tokens=min_tokens,
                    max_tokens=max_tokens,
                    coverage_threshold=coverage_threshold,
                    model=qa_model,
                    analyze_coverage=analyze_coverage,
                    log_callback=add_log,
                    progress_callback=update_progress,
                )

                # å‡¦ç†å®Œäº†å¾Œã¯ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ãƒãƒ¼ã‚’ã‚¯ãƒªã‚¢
                progress_container.empty()

                # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‰Šé™¤
                if input_source == "local_file" and input_file_path:
                    try:
                        Path(input_file_path).unlink()
                        add_log("  ğŸ—‘ï¸ ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‰Šé™¤ã—ã¾ã—ãŸ")
                    except Exception:
                        pass

                if result["success"]:
                    qa_saved_files = result.get("saved_files")
                    qa_count = result.get("qa_count", 0)

                    # çµæœã‚’ä¿å­˜
                    st.session_state["qa_result_files"] = qa_saved_files
                    st.session_state["qa_result_count"] = qa_count

                    if result.get("coverage_results"):
                        add_log(
                            f"ğŸ“Š ã‚«ãƒãƒ¬ãƒ¼ã‚¸ç‡: {result['coverage_results']['coverage_rate']:.1%}"
                        )

                    add_log("ğŸ‰ Q/Aç”Ÿæˆå®Œäº†ï¼")
                else:
                    add_log("âš ï¸ Q/Aç”Ÿæˆã«å¤±æ•—ã—ã¾ã—ãŸ")

        except Exception as e:
            add_log(f"âŒ ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿ: {str(e)}")
            st.error(f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}")
        finally:
            # å®Ÿè¡Œå®Œäº† - ãƒ•ãƒ©ã‚°ã‚’ãƒªã‚»ãƒƒãƒˆ
            st.session_state["qa_generation_running"] = False

    # ãƒ­ã‚°è¡¨ç¤º
    with log_container:
        if st.session_state["qa_logs"]:
            with st.expander("ğŸ“œ å‡¦ç†ãƒ­ã‚°ã‚’è¡¨ç¤º", expanded=False):
                log_text = "\n".join(st.session_state["qa_logs"])
                st.text_area("å‡¦ç†ãƒ­ã‚°", value=log_text, height=400, disabled=True)
                st.caption(f"ç·ãƒ­ã‚°æ•°: {len(st.session_state['qa_logs'])} è¡Œ")
        else:
            st.info("Q/Aç”Ÿæˆã‚’é–‹å§‹ã™ã‚‹ã¨ã“ã“ã«ãƒ­ã‚°ãŒè¡¨ç¤ºã•ã‚Œã¾ã™")

    # çµæœè¡¨ç¤º
    if "qa_result_files" in st.session_state and st.session_state["qa_result_files"]:
        st.divider()
        st.subheader("ğŸ“ ç”Ÿæˆçµæœ")

        qa_files = st.session_state["qa_result_files"]
        qa_count = st.session_state.get("qa_result_count", 0)

        st.info(f"âœ… ç”Ÿæˆã•ã‚ŒãŸQ/Aãƒšã‚¢: **{qa_count}** å€‹")

        col_qa1, col_qa2 = st.columns(2)

        with col_qa1:
            if "csv" in qa_files:
                st.success(f"ğŸ“„ CSV: {qa_files['csv']}")

        with col_qa2:
            if "json" in qa_files:
                st.success(f"ğŸ“„ JSON: {qa_files['json']}")