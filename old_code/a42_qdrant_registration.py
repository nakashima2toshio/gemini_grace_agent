#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
a42_qdrant_registration.py â€” 3ã¤ã®ç•°ãªã‚‹ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ã¸ã®åˆ†é›¢ç™»éŒ²ç‰ˆ (Geminiå¯¾å¿œ)
--------------------------------------------------------------------------------
cc_newsãƒ‰ãƒ¡ã‚¤ãƒ³ã®Q&Aãƒ‡ãƒ¼ã‚¿ã‚’ã€ç”Ÿæˆæ–¹æ³•ã”ã¨ã«åˆ¥ã€…ã®ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ã«ç™»éŒ²ã—ã¾ã™ã€‚

ã€ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³æ§‹æˆã€‘
- qa_cc_news_a02_llm    : a02_qa_pairs_cc_news.csv (LLMç”Ÿæˆæ–¹å¼)
- qa_cc_news_a03_rule   : a03_qa_pairs_cc_news.csv (ãƒ«ãƒ¼ãƒ«ãƒ™ãƒ¼ã‚¹ç”Ÿæˆæ–¹å¼)
- qa_cc_news_a10_hybrid : a10_qa_pairs_cc_news.csv (ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ç”Ÿæˆæ–¹å¼)

ã€ä¸»ãªå¤‰æ›´ç‚¹ã€‘
- OpenAI APIã®ä½¿ç”¨ã‚’å»ƒæ­¢ã—ã€Gemini API (helper_embedding) ã«ç§»è¡Œ
- å˜ä¸€ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ â†’ 3ã¤ã®ç‹¬ç«‹ã—ãŸã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³
- å„CSVãƒ•ã‚¡ã‚¤ãƒ«ãŒå°‚ç”¨ã®ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ã‚’æŒã¤

ä½¿ã„æ–¹ï¼š
  # 1. CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç”Ÿæˆï¼ˆa20_output_qa_csv.pyã‚’å®Ÿè¡Œï¼‰
  python a20_output_qa_csv.py

  # 2. Qdrantã‚µãƒ¼ãƒãƒ¼ã‚’èµ·å‹•
  export GOOGLE_API_KEY=...
docker run -p 6333:6333 -p 6334:6334 qdrant/qdrant

  # 3. 3ã¤ã®ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ã«ãƒ‡ãƒ¼ã‚¿ã‚’ç™»éŒ²
  python a42_qdrant_registration.py --recreate --include-answer

ä¸»è¦å¼•æ•°ï¼š
  --recreate          : ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³å‰Šé™¤â†’æ–°è¦ä½œæˆ
  --collection        : ç‰¹å®šã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ã®ã¿å‡¦ç†ï¼ˆæŒ‡å®šãªã—ã§å…¨ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ï¼‰
  --input-file        : ãƒ­ãƒ¼ã‚«ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç™»éŒ²ï¼ˆCSV/TXT/JSON/JSONLï¼‰
  --qdrant-url        : æ—¢å®šã¯ http://localhost:6333
  --batch-size        : Embeddings/Upsert ãƒãƒƒãƒã‚µã‚¤ã‚ºï¼ˆæ—¢å®š 32ï¼‰
  --limit             : ãƒ‡ãƒ¼ã‚¿ä»¶æ•°ä¸Šé™ï¼ˆé–‹ç™ºç”¨ã€0=ç„¡åˆ¶é™ï¼‰
  --include-answer    : åŸ‹ã‚è¾¼ã¿å…¥åŠ›ã« answer ã‚‚çµåˆï¼ˆquestion + "\n" + answerï¼‰
  --search            : ã‚¯ã‚¨ãƒªæŒ‡å®šã§æ¤œç´¢ã®ã¿å®Ÿè¡Œ
  --topk              : ä¸Šä½ä»¶æ•°ï¼ˆæ—¢å®š5ï¼‰
"""
import argparse
import os
from datetime import datetime, timezone
from typing import Dict, Iterable, List, Optional, Any
from pathlib import Path

import pandas as pd

try:
    import yaml  # PyYAML
except Exception:
    yaml = None

try:
    import helper_rag as hrag
except Exception:
    hrag = None

# Geminiå¯¾å¿œã®ãŸã‚ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from helper_embedding import create_embedding_client

from qdrant_client import QdrantClient
from qdrant_client.http import models

# ------------------ ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®š ------------------
DEFAULTS = {
    "rag": {
        "include_answer_in_embedding": False,
    },
    "embeddings": {
        "primary": {"provider": "gemini", "model": "gemini-embedding-001", "dims": 3072},
    },
    "qdrant": {"url": "http://localhost:6333"},
}

# ------------------ ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³å®šç¾© ------------------
# CSVãƒ•ã‚¡ã‚¤ãƒ«/TXTãƒ•ã‚¡ã‚¤ãƒ« â†’ ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³å â†’ ç”Ÿæˆæ–¹æ³•ã®ãƒãƒƒãƒ”ãƒ³ã‚°
COLLECTION_MAPPINGS = [
    {
        "csv_file": "qa_output/a02_qa_pairs_cc_news.csv",
        "collection": "qa_cc_news_a02_llm",
        "generation_method": "a02_make_qa",
        "domain": "cc_news",
        "description": "LLMç”Ÿæˆæ–¹å¼ï¼ˆa02_make_qa_para.pyï¼‰",
        "type": "qa"
    },
    {
        "csv_file": "qa_output/a03_qa_pairs_cc_news.csv",
        "collection": "qa_cc_news_a03_rule",
        "generation_method": "a03_coverage",
        "domain": "cc_news",
        "description": "ãƒ«ãƒ¼ãƒ«ãƒ™ãƒ¼ã‚¹ç”Ÿæˆæ–¹å¼ï¼ˆa03_rag_qa_coverage_improved.pyï¼‰",
        "type": "qa"
    },
    {
        "csv_file": "qa_output/a10_qa_pairs_cc_news.csv",
        "collection": "qa_cc_news_a10_hybrid",
        "generation_method": "a10_hybrid",
        "domain": "cc_news",
        "description": "ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ç”Ÿæˆæ–¹å¼ï¼ˆa10_qa_optimized_hybrid_batch.pyï¼‰",
        "type": "qa"
    },
    {
        "csv_file": "qa_output/a02_qa_pairs_livedoor.csv",
        "collection": "qa_livedoor_a02_20_llm",
        "generation_method": "a02_make_qa",
        "domain": "livedoor",
        "description": "LLMç”Ÿæˆæ–¹å¼ï¼ˆa02_make_qa_para.pyï¼‰- livedoorãƒ‡ãƒ¼ã‚¿",
        "type": "qa"
    },
    {
        "csv_file": "qa_output/a03_qa_pairs_livedoor.csv",
        "collection": "qa_livedoor_a03_rule",
        "generation_method": "a03_coverage",
        "domain": "livedoor",
        "description": "ãƒ«ãƒ¼ãƒ«ãƒ™ãƒ¼ã‚¹ç”Ÿæˆæ–¹å¼ï¼ˆa03_rag_qa_coverage_improved.pyï¼‰- livedoorãƒ‡ãƒ¼ã‚¿",
        "type": "qa"
    },
    {
        "csv_file": "qa_output/a10_qa_pairs_livedoor.csv",
        "collection": "qa_livedoor_a10_hybrid",
        "generation_method": "a10_hybrid",
        "domain": "livedoor",
        "description": "ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ç”Ÿæˆæ–¹å¼ï¼ˆa10_qa_optimized_hybrid_batch.pyï¼‰- livedoorãƒ‡ãƒ¼ã‚¿",
        "type": "qa"
    },
    {
        "csv_file": "OUTPUT/cc_news.txt",
        "collection": "raw_cc_news",
        "generation_method": "raw_text",
        "domain": "cc_news",
        "description": "ç”Ÿãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ï¼ˆcc_newsï¼‰",
        "type": "raw"
    },
    {
        "csv_file": "OUTPUT/livedoor.txt",
        "collection": "raw_livedoor",
        "generation_method": "raw_text",
        "domain": "livedoor",
        "description": "ç”Ÿãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ï¼ˆlivedoorï¼‰",
        "type": "raw"
    }
]


# ------------------ è¨­å®šãƒ­ãƒ¼ãƒ‰ ------------------
def load_config(path: str = "config.yml") -> Dict[str, Any]:
    cfg = {}
    if yaml and os.path.exists(path):
        with open(path, "r", encoding="utf-8") as f:
            cfg = yaml.safe_load(f) or {}
    # ãƒãƒ¼ã‚¸ï¼ˆæµ…ã„ãƒãƒ¼ã‚¸ã§ååˆ†ã€‚å¿…è¦ãªã‚‰æ·±ã„ãƒãƒ¼ã‚¸ã«å·®ã—æ›¿ãˆï¼‰
    def merge(dst, src):
        for k, v in src.items():
            if isinstance(v, dict) and isinstance(dst.get(k), dict):
                merge(dst[k], v)
            else:
                dst.setdefault(k, v)
    full = {}
    merge(full, DEFAULTS)
    merge(full, cfg)
    return full

# ------------------ å°é“å…· ------------------
def batched(seq: Iterable, size: int):
    buf = []
    for x in seq:
        buf.append(x)
        if len(buf) >= size:
            yield buf
            buf = []
    if buf:
        yield buf

# ------------------ åŸ‹ã‚è¾¼ã¿å®Ÿè£…ï¼ˆhelperå„ªå…ˆï¼‰ ------------------
def embed_texts(texts: List[str], model: str, batch_size: int = 128, provider: str = "gemini") -> List[List[float]]:
    """
    ãƒ†ã‚­ã‚¹ãƒˆã‚’ãƒãƒƒãƒå‡¦ç†ã§Embeddingã«å¤‰æ›
    helper_embeddingã‚’ä½¿ç”¨ã—ã¦ã€Gemini/OpenAI/FastEmbedã«å¯¾å¿œ
    """
    # ç©ºæ–‡å­—åˆ—ã®é™¤å»ã¨ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä¿æŒ
    valid_texts = []
    valid_indices = []
    for i, text in enumerate(texts):
        if text and text.strip():
            valid_texts.append(text)
            valid_indices.append(i)

    if not valid_texts:
        print("\r   [WARN] å…¨ã¦ã®ãƒ†ã‚­ã‚¹ãƒˆãŒç©ºæ–‡å­—åˆ—ã§ã™ã€‚ãƒ€ãƒŸãƒ¼ãƒ™ã‚¯ãƒˆãƒ«ã‚’è¿”ã—ã¾ã™ã€‚", flush=True)
        # æ¬¡å…ƒæ•°ã¯ä»®ã«3072ã¨ã™ã‚‹ï¼ˆGeminiï¼‰
        return [[0.0] * 3072] * len(texts)

    # ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆç”Ÿæˆ
    client = create_embedding_client(provider=provider, model=model)
    
    # åŸ‹ã‚è¾¼ã¿ç”Ÿæˆ
    print(f"\r   åŸ‹ã‚è¾¼ã¿ç”Ÿæˆä¸­: {len(valid_texts)}ä»¶ (Provider: {provider})... ", end="", flush=True)
    valid_vecs = client.embed_texts(valid_texts, batch_size=batch_size)
    
    # ãƒ™ã‚¯ãƒˆãƒ«ã®å†æ§‹ç¯‰
    vecs = []
    dims = client.dimensions
    valid_vec_idx = 0
    
    for i in range(len(texts)):
        if i in valid_indices:
            if valid_vec_idx < len(valid_vecs):
                vecs.append(valid_vecs[valid_vec_idx])
                valid_vec_idx += 1
            else:
                # ã‚¨ãƒ©ãƒ¼ãªã©ã§æ•°ãŒåˆã‚ãªã„å ´åˆã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
                vecs.append([0.0] * dims)
        else:
            vecs.append([0.0] * dims)
            
    return vecs

# ------------------ å…¥åŠ›ãƒ†ã‚­ã‚¹ãƒˆæ§‹ç¯‰ ------------------
def build_inputs(df: pd.DataFrame, include_answer: bool) -> List[str]:
    if include_answer:
        return (df["question"].astype(str) + "\n" + df["answer"].astype(str)).tolist()
    return df["question"].astype(str).tolist()

# ------------------ CSVãƒ­ãƒ¼ãƒ‰ ------------------
def load_csv(path: str, required=("question", "answer"), limit: int = 0) -> pd.DataFrame:
    if not os.path.exists(path):
        raise FileNotFoundError(f"CSV not found: {path}")
    df = pd.read_csv(path)
    # åˆ—åãƒãƒƒãƒ”ãƒ³ã‚°ãŒå¿…è¦ãªã‚‰ã“ã“ã§èª¿æ•´ï¼ˆä¾‹: 'Question'->'question'ï¼‰
    column_mappings = {
        'Question': 'question',
        'Response': 'answer',
        'Answer': 'answer',
        'correct_answer': 'answer'
    }
    df = df.rename(columns=column_mappings)
    for col in required:
        if col not in df.columns:
            raise ValueError(f"{path} ã«ã¯ '{col}' åˆ—ãŒå¿…è¦ã§ã™ï¼ˆåˆ—: {list(df.columns)}ï¼‰")
    df = df.fillna("").drop_duplicates(subset=list(required)).reset_index(drop=True)
    if limit and limit > 0:
        df = df.head(limit).copy()
    return df

# ------------------ TXTãƒ­ãƒ¼ãƒ‰ï¼ˆç”Ÿãƒ†ã‚­ã‚¹ãƒˆï¼‰ ------------------
# (detect_language, chunk_japanese_text, chunk_english_text ã¯ helper_rag / helper_text ã«ã‚ã‚‹ã‚‚ã®ã‚’ä½¿ç”¨ã™ã¹ãã ãŒã€
# ã“ã“ã§ã¯ç‹¬ç«‹æ€§ã‚’ä¿ã¤ãŸã‚ã€ã¾ãŸã¯ helper_rag ã‚’ä½¿ã†ã‚ˆã†ã«å¤‰æ›´ã™ã‚‹ã®ãŒæœ›ã¾ã—ã„)
# ç°¡æ˜“çš„ã« helper_rag ãŒã‚ã‚Œã°ãã¡ã‚‰ã‚’ä½¿ã„ã€ãªã‘ã‚Œã°ç°¡æ˜“å®Ÿè£…ã‚’ä½¿ã†

def load_txt(path: str, limit: int = 0, chunk_size: int = 500) -> pd.DataFrame:
    if not os.path.exists(path):
        raise FileNotFoundError(f"TXT not found: {path}")

    # ç°¡æ˜“ãƒãƒ£ãƒ³ã‚¯åˆ†å‰²ï¼ˆå›ºå®šé•·ï¼‰
    import tiktoken
    enc = tiktoken.get_encoding("cl100k_base")
    with open(path, "r", encoding="utf-8") as f:
        full_text = f.read()
    tokens = enc.encode(full_text)
    
    chunks = []
    for i in range(0, len(tokens), chunk_size):
        chunk_tokens = tokens[i:i+chunk_size]
        chunks.append(enc.decode(chunk_tokens))

    if limit and limit > 0:
        chunks = chunks[:limit]

    return pd.DataFrame({"text": chunks})

# ------------------ Qdrant: ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ä½œæˆï¼ˆNamed Vectorså¯¾å¿œï¼‰ ------------------
def create_or_recreate_collection(client: QdrantClient, name: str, recreate: bool,
                                  embeddings_cfg: Dict[str, Dict[str, Any]]):
    # embeddings_cfg: dict[name] = {"model": "...", "dims": int}
    if len(embeddings_cfg) == 1:
        dims = list(embeddings_cfg.values())[0]["dims"]
        vectors_config = models.VectorParams(size=dims, distance=models.Distance.COSINE)
    else:
        # Named vectors
        vectors_config = {
            k: models.VectorParams(size=v["dims"], distance=models.Distance.COSINE)
            for k, v in embeddings_cfg.items()
        }
    if recreate:
        client.recreate_collection(collection_name=name, vectors_config=vectors_config)
    else:
        # ç„¡ã‘ã‚Œã°ä½œæˆ
        try:
            client.get_collection(name)
        except Exception:
            client.create_collection(collection_name=name, vectors_config=vectors_config)
    # ã‚ˆãä½¿ã†payloadã®ç´¢å¼•ï¼ˆä»»æ„ï¼‰
    try:
        client.create_payload_index(name, field_name="domain", field_schema=models.PayloadSchemaType.KEYWORD)
    except Exception:
        pass
    try:
        client.create_payload_index(name, field_name="generation_method", field_schema=models.PayloadSchemaType.KEYWORD)
    except Exception:
        pass

# ------------------ ãƒã‚¤ãƒ³ãƒˆæ§‹ç¯‰ï¼ˆNamed Vectorså¯¾å¿œï¼‰ ------------------
def build_points(df: pd.DataFrame, vectors_by_name: Dict[str, List[List[float]]], domain: str, source_file: str,
                 generation_method: str = None, data_type: str = "qa") -> List[models.PointStruct]:
    n = len(df)
    for name, vecs in vectors_by_name.items():
        if len(vecs) != n:
            raise ValueError(f"vectors length mismatch for '{name}': df={n}, vecs={len(vecs)}")
    now_iso = datetime.now(timezone.utc).isoformat()
    points: List[models.PointStruct] = []

    for i, row in enumerate(df.itertuples(index=False)):
        if data_type == "qa":
            payload = {
                "domain": domain,
                "generation_method": generation_method or "unknown",
                "question": getattr(row, "question"),
                "answer": getattr(row, "answer"),
                "source": os.path.basename(source_file),
                "created_at": now_iso,
                "schema": "qa:v1",
            }
        else:  # raw text
            payload = {
                "domain": domain,
                "generation_method": generation_method or "unknown",
                "text": getattr(row, "text"),
                "source": os.path.basename(source_file),
                "created_at": now_iso,
                "schema": "raw:v1",
            }

        pid = abs(hash(f"{domain}-{generation_method}-{source_file}-{i}")) & 0x7FFFFFFFFFFFFFFF
        if len(vectors_by_name) == 1:
            vec = list(vectors_by_name.values())[0][i]
            points.append(models.PointStruct(id=pid, vector=vec, payload=payload))
        else:
            vecs_dict = {name: vecs[i] for name, vecs in vectors_by_name.items()}
            points.append(models.PointStruct(id=pid, vector=vecs_dict, payload=payload))
    return points

def upsert_points(client: QdrantClient, collection: str, points: List[models.PointStruct], batch_size: int = 128) -> int:
    count = 0
    for chunk in batched(points, batch_size):
        client.upsert(collection_name=collection, points=chunk)
        count += len(chunk)
    return count

# ------------------ æ¤œç´¢ï¼ˆNamed Vectorså¯¾å¿œï¼‰ ------------------
def embed_one(text: str, model: str, provider: str = "gemini") -> List[float]:
    return embed_texts([text], model=model, batch_size=1, provider=provider)[0]

def search(client: QdrantClient, collection: str, query: str, using_vec: str, model_for_using: str,
           topk: int = 5, domain: Optional[str] = None, generation_method: Optional[str] = None, provider: str = "gemini"):
    qvec = embed_one(query, model=model_for_using, provider=provider)
    qfilter = None
    filter_conditions = []
    if domain:
        filter_conditions.append(models.FieldCondition(key="domain", match=models.MatchValue(value=domain)))
    if generation_method:
        filter_conditions.append(models.FieldCondition(key="generation_method", match=models.MatchValue(value=generation_method)))
    if filter_conditions:
        qfilter = models.Filter(must=filter_conditions)
    
    try:
        hits = client.search(collection_name=collection, query_vector=qvec, limit=topk,
                           query_filter=qfilter)
    except Exception as e:
        print(f"æ¤œç´¢ã‚¨ãƒ©ãƒ¼: {e}")
        hits = []
    
    return hits

# ------------------ ãƒ¡ã‚¤ãƒ³ ------------------
def main():
    cfg = load_config("config.yml")
    rag_cfg = cfg.get("rag", {})
    
    # åŸ‹ã‚è¾¼ã¿è¨­å®šã‚’Geminiå‘ã‘ã«ä¸Šæ›¸ãã¾ãŸã¯å–å¾—
    embeddings_cfg: Dict[str, Dict[str, Any]] = cfg.get("embeddings", {})
    if not embeddings_cfg or "primary" not in embeddings_cfg:
        embeddings_cfg = DEFAULTS["embeddings"]
    
    # primaryãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ãŒopenaiãªã‚‰geminiã«æ›¸ãæ›ãˆã‚‹ï¼ˆå®‰å…¨ç­–ï¼‰
    if embeddings_cfg["primary"].get("provider") == "openai":
        print("[INFO] è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã®OpenAIãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã‚’Geminiã«ã‚ªãƒ¼ãƒãƒ¼ãƒ©ã‚¤ãƒ‰ã—ã¾ã™ã€‚")
        embeddings_cfg["primary"] = DEFAULTS["embeddings"]["primary"]

    qdrant_url = (cfg.get("qdrant", {}) or {}).get("url", "http://localhost:6333")

    ap = argparse.ArgumentParser(
        description="3ã¤ã®Q&Aãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ãã‚Œãã‚Œç‹¬ç«‹ã—ãŸQdrantã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ã«ç™»éŒ²"
    )
    ap.add_argument("--recreate", action="store_true",
                    help="Drop & create collection before upsert.")
    ap.add_argument("--collection", default=None,
                    help="ç‰¹å®šã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ã®ã¿å‡¦ç†ï¼ˆæŒ‡å®šãªã—ã§å…¨ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ï¼‰")
    ap.add_argument("--input-file", default=None,
                    help="ãƒ­ãƒ¼ã‚«ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç™»éŒ²ï¼ˆCSV/TXT/JSON/JSONLï¼‰")
    ap.add_argument("--qdrant-url", default=qdrant_url)
    ap.add_argument("--batch-size", type=int, default=32)
    ap.add_argument("--limit", type=int, default=0,
                    help="Row limit per CSV for development (0=all)")
    ap.add_argument("--include-answer", action="store_true",
                    default=rag_cfg.get("include_answer_in_embedding", False),
                    help="Use 'question\nanswer' as embedding input.")
    ap.add_argument("--search", default=None, help="Run search only.")
    ap.add_argument("--topk", type=int, default=5)
    args = ap.parse_args()

    # Qdrant client
    client = QdrantClient(url=args.qdrant_url, timeout=300)

    # ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã¨ãƒ¢ãƒ‡ãƒ«ã®å–å¾—
    provider = embeddings_cfg["primary"].get("provider", "gemini")
    model = embeddings_cfg["primary"].get("model", "gemini-embedding-001")

    # æ¤œç´¢ã®ã¿
    if args.search:
        if not args.collection:
            print("[ERROR] æ¤œç´¢ã«ã¯ --collection ã®æŒ‡å®šãŒå¿…è¦ã§ã™")
            return

        hits = search(client, args.collection, args.search, "primary", model, topk=args.topk, provider=provider)

        print(f"\n[Search] collection={args.collection} query={args.search!r}")
        for h in hits:
            method = h.payload.get('generation_method', 'unknown')
            question = h.payload.get('question', '')[:80]
            answer = h.payload.get('answer', '')[:80]
            print(f"score={h.score:.4f}  method={method}  Q: {question}  A: {answer}...")
        return

    # å‡¦ç†å¯¾è±¡ã®ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ã‚’æ±ºå®š
    if args.input_file:
        if not os.path.exists(args.input_file):
            print(f"[ERROR] ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {args.input_file}")
            return

        file_path = Path(args.input_file)
        file_basename = file_path.stem
        file_ext = file_path.suffix.lower().lstrip('.')

        clean_name = file_basename.replace('qa_pairs_', '').replace('a02_qa_pairs_', '').replace('a03_qa_pairs_', '').replace('a10_qa_pairs_', '')
        collection_name = f"qa_{clean_name}"

        if 'cc_news' in file_basename.lower():
            domain = 'cc_news'
        elif 'livedoor' in file_basename.lower():
            domain = 'livedoor'
        elif 'upload' in file_basename.lower():
            domain = 'custom_upload'
        else:
            domain = 'custom'

        if file_ext in ['txt', 'text']:
            data_type = 'raw'
            generation_method = 'raw_text'
        else:
            data_type = 'qa'
            generation_method = 'custom_upload'

        target_mappings = [{
            "csv_file": args.input_file,
            "collection": collection_name,
            "generation_method": generation_method,
            "domain": domain,
            "description": f"ã‚«ã‚¹ã‚¿ãƒ ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ ({file_basename})",
            "type": data_type
        }]

        print("\n[INFO] ãƒ­ãƒ¼ã‚«ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ç™»éŒ²ãƒ¢ãƒ¼ãƒ‰")
        print(f"  ãƒ•ã‚¡ã‚¤ãƒ«: {args.input_file}")
        print(f"  ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³: {collection_name}")
        print(f"  ãƒ‰ãƒ¡ã‚¤ãƒ³: {domain}")
        print(f"  ã‚¿ã‚¤ãƒ—: {data_type}")

    elif args.collection:
        target_mappings = [m for m in COLLECTION_MAPPINGS if m["collection"] == args.collection]
        if not target_mappings:
            print(f"[ERROR] ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ '{args.collection}' ã¯å®šç¾©ã•ã‚Œã¦ã„ã¾ã›ã‚“")
            print(f"åˆ©ç”¨å¯èƒ½ãªã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³: {[m['collection'] for m in COLLECTION_MAPPINGS]}")
            return
    else:
        target_mappings = COLLECTION_MAPPINGS

    # ã‚¤ãƒ³ã‚¸ã‚§ã‚¹ãƒˆå‡¦ç†
    print(f"\n[INFO] å‡¦ç†å¯¾è±¡: {len(target_mappings)} ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³")
    print("=" * 80)

    total = 0
    for mapping in target_mappings:
        csv_file = mapping["csv_file"]
        collection_name = mapping["collection"]
        generation_method = mapping["generation_method"]
        domain = mapping["domain"]
        description = mapping["description"]

        print(f"\nğŸ“¦ ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³: {collection_name}")
        print(f"   èª¬æ˜: {description}")
        print(f"   ã‚½ãƒ¼ã‚¹: {csv_file}")
        print("-" * 80)

        if not os.path.exists(csv_file):
            print(f"[WARN] ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {csv_file} (ã‚¹ã‚­ãƒƒãƒ—)")
            continue

        # ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ä½œæˆ
        create_or_recreate_collection(client, collection_name, args.recreate, embeddings_cfg)

        # ãƒ‡ãƒ¼ã‚¿ã‚¿ã‚¤ãƒ—ã«å¿œã˜ã¦ãƒ­ãƒ¼ãƒ‰
        data_type = mapping.get("type", "qa")
        if data_type == "raw":
            df = load_txt(csv_file, limit=args.limit)
            print(f"   ãƒ‡ãƒ¼ã‚¿ä»¶æ•°: {len(df):,}ä»¶")
            texts = df["text"].tolist()
        else:
            df = load_csv(csv_file, limit=args.limit)
            print(f"   ãƒ‡ãƒ¼ã‚¿ä»¶æ•°: {len(df):,}ä»¶")
            texts = build_inputs(df, include_answer=args.include_answer)

        # åŸ‹ã‚è¾¼ã¿ç”Ÿæˆ
        vectors_by_name: Dict[str, List[List[float]]] = {}
        for name, vcfg in embeddings_cfg.items():
            current_provider = vcfg.get("provider", "gemini")
            current_model = vcfg.get("model", "gemini-embedding-001")
            print(f"   åŸ‹ã‚è¾¼ã¿ç”Ÿæˆä¸­: {name} (provider={current_provider}, model={current_model})... ", end="", flush=True)
            
            vectors_by_name[name] = embed_texts(
                texts, 
                model=current_model, 
                batch_size=args.batch_size,
                provider=current_provider
            )
            print("âœ“")

        # ãƒã‚¤ãƒ³ãƒˆæ§‹ç¯‰ã¨ã‚¢ãƒƒãƒ—ã‚µãƒ¼ãƒˆ
        points = build_points(df, vectors_by_name, domain, csv_file, generation_method, data_type=data_type)
        print("   ã‚¢ãƒƒãƒ—ã‚µãƒ¼ãƒˆä¸­... ", end="", flush=True)
        n = upsert_points(client, collection_name, points, batch_size=args.batch_size)
        print(f"âœ“ {n:,}ä»¶")

        total += n

    print("\n" + "=" * 80)
    print(f"âœ… å®Œäº†: ç·ç™»éŒ²ä»¶æ•° {total:,}ä»¶")

    # ç™»éŒ²ã•ã‚ŒãŸã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ä¸€è¦§ã‚’è¡¨ç¤º
    print("\n[INFO] ç™»éŒ²ã•ã‚ŒãŸã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ä¸€è¦§:")
    print("-" * 80)
    all_collections = client.get_collections()
    for col in all_collections.collections:
        print(f"  â€¢ {col.name}")
    print("-" * 80)

    # æ¤œè¨¼æ¤œç´¢
    print("\n[INFO] æ¤œè¨¼æ¤œç´¢ã‚’å®Ÿè¡Œä¸­...")
    
    for mapping in target_mappings:
        collection_name = mapping["collection"]
        try:
            info = client.get_collection(collection_name)
            print(f"\n  {collection_name}: {info.points_count:,}ä»¶ç™»éŒ²æ¸ˆã¿")

            hits = search(client, collection_name, "æ°—å€™å¤‰å‹•", "primary", model, topk=2, provider=provider)
            if hits:
                for h in hits[:1]:
                    q = h.payload.get('question', '')[:50]
                    print(f"    ã‚µãƒ³ãƒ—ãƒ«æ¤œç´¢çµæœ: score={h.score:.4f}  Q: {q}...")
        except Exception as e:
            print(f"    æ¤œè¨¼ã‚¨ãƒ©ãƒ¼: {e}")

if __name__ == "__main__":
    main()