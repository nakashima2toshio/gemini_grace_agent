#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
qdrant_service.py - Qdrantæ“ä½œã‚µãƒ¼ãƒ“ã‚¹
======================================
Qdrantãƒ™ã‚¯ãƒˆãƒ«ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã®æ“ä½œã‚’æ‹…å½“

æ©Ÿèƒ½:
- ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯ï¼ˆQdrantHealthCheckerï¼‰
- ãƒ‡ãƒ¼ã‚¿å–å¾—ï¼ˆQdrantDataFetcherï¼‰
- ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ç®¡ç†ï¼ˆCRUDï¼‰
- åŸ‹ã‚è¾¼ã¿ç”Ÿæˆãƒ»ç™»éŒ²
- æ¤œç´¢æ©Ÿèƒ½
"""

import os
import socket
import time
import logging
import traceback
import glob
from datetime import datetime, timezone
from typing import Dict, List, Any, Optional, Tuple, Iterable

import pandas as pd
import tiktoken
from helper_embedding import create_embedding_client, get_embedding_dimensions
from qdrant_client_wrapper import (
    embed_sparse_texts_unified, 
    create_or_recreate_collection
)
from qdrant_client import QdrantClient
from qdrant_client.http import models
from qdrant_client.http.exceptions import UnexpectedResponse

logger = logging.getLogger(__name__)


# ===================================================================
# Qdrantè¨­å®š
# ===================================================================

QDRANT_CONFIG = {
    "name": "Qdrant",
    "host": "localhost",
    "port": 6333,
    "icon": "ğŸ¯",
    "url": "http://localhost:6333",
    "health_check_endpoint": "/collections",
    "docker_image": "qdrant/qdrant",
}

# ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³å›ºæœ‰ã®åŸ‹ã‚è¾¼ã¿è¨­å®š (Deprecated: Use get_collection_embedding_params instead)
COLLECTION_EMBEDDINGS_SEARCH = {}

# ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ã¨CSVãƒ•ã‚¡ã‚¤ãƒ«ã®å¯¾å¿œè¡¨ (Deprecated: Use get_dynamic_collection_mapping instead)
COLLECTION_CSV_MAPPING = {}


def map_collection_to_csv(collection_name: str, qa_output_dir: str = "qa_output") -> Optional[str]:
    """
    ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³åã‹ã‚‰å¯¾å¿œã™ã‚‹CSVãƒ•ã‚¡ã‚¤ãƒ«åã‚’æ¨æ¸¬ã—ã¦è¿”ã™

    Args:
        collection_name: ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³å
        qa_output_dir: CSVãƒ•ã‚¡ã‚¤ãƒ«ãŒæ ¼ç´ã•ã‚Œã¦ã„ã‚‹ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª

    Returns:
        ãƒ•ã‚¡ã‚¤ãƒ«åï¼ˆãƒ‘ã‚¹ãªã—ï¼‰ã€è¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯None
    """
    # ãƒ‘ã‚¿ãƒ¼ãƒ³1: å®Œå…¨ä¸€è‡´ (qa_output/{collection_name}.csv)
    exact_match = os.path.join(qa_output_dir, f"{collection_name}.csv")
    if os.path.exists(exact_match):
        return os.path.basename(exact_match)

    # ãƒ‘ã‚¿ãƒ¼ãƒ³2: æ¥é ­è¾ 'qa_' ã‚’é™¤å¤– (qa_{name} -> {name}.csv)
    if collection_name.startswith("qa_"):
        stripped_name = collection_name[3:]
        stripped_match = os.path.join(qa_output_dir, f"{stripped_name}.csv")
        if os.path.exists(stripped_match):
            return os.path.basename(stripped_match)

    return None


def get_dynamic_collection_mapping(
    client: QdrantClient, qa_output_dir: str = "qa_output"
) -> Dict[str, str]:
    """
    Qdrantã®ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ä¸€è¦§ã¨ãƒ­ãƒ¼ã‚«ãƒ«ã®CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‹•çš„ã«ãƒãƒƒãƒ”ãƒ³ã‚°
    
    æ”¹å–„ç‰ˆ: ãƒšã‚¤ãƒ­ãƒ¼ãƒ‰ã®'source'ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‚’å„ªå…ˆçš„ã«å‚ç…§ã™ã‚‹

    Args:
        client: QdrantClientã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹
        qa_output_dir: CSVãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª

    Returns:
        {ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³å: CSVãƒ•ã‚¡ã‚¤ãƒ«å} ã®è¾æ›¸
    """
    mapping = {}
    try:
        # ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ä¸€è¦§å–å¾—
        collections_resp = client.get_collections()
        for collection in collections_resp.collections:
            col_name = collection.name
            csv_file = None
            
            # æ–¹æ³•1: ãƒšã‚¤ãƒ­ãƒ¼ãƒ‰ã‹ã‚‰ã‚½ãƒ¼ã‚¹æƒ…å ±ã‚’å–å¾—ï¼ˆç¢ºå®Ÿï¼‰
            try:
                # 1ä»¶ã ã‘å–å¾—ã—ã¦sourceã‚’ç¢ºèª
                points, _ = client.scroll(
                    collection_name=col_name,
                    limit=1,
                    with_payload=["source"],
                    with_vectors=False
                )
                if points and points[0].payload:
                    source_val = points[0].payload.get("source")
                    if source_val:
                        csv_file = source_val
            except Exception:
                pass # ãƒšã‚¤ãƒ­ãƒ¼ãƒ‰å–å¾—å¤±æ•—æ™‚ã¯æ¬¡ã®æ–¹æ³•ã¸

            # æ–¹æ³•2: åå‰ã‹ã‚‰æ¨æ¸¬ï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰
            if not csv_file:
                csv_file = map_collection_to_csv(col_name, qa_output_dir)
            
            if csv_file:
                mapping[col_name] = csv_file
                
    except Exception as e:
        logger.error(f"å‹•çš„ãƒãƒƒãƒ”ãƒ³ã‚°ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")

    return mapping


def get_collection_embedding_params(
    client: QdrantClient, collection_name: str
) -> Dict[str, Any]:
    """
    ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ã®è¨­å®šï¼ˆãƒ™ã‚¯ãƒˆãƒ«æ¬¡å…ƒæ•°ï¼‰ã‹ã‚‰åŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ«è¨­å®šã‚’æ¨è«–

    Args:
        client: QdrantClient
        collection_name: ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³å

    Returns:
        {"model": str, "dims": int}
    """
    # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®šï¼ˆGeminiï¼‰
    default_params = {"model": "gemini-embedding-001", "dims": 3072}

    try:
        info = client.get_collection(collection_name)
        vectors_config = info.config.params.vectors

        size = 0
        if hasattr(vectors_config, "size"):
            size = vectors_config.size
        elif isinstance(vectors_config, dict):
            # ãƒãƒ«ãƒãƒ™ã‚¯ãƒˆãƒ«ã®å ´åˆã¯æœ€åˆã®ã‚‚ã®ã‚’æ¡ç”¨
            first_key = next(iter(vectors_config))
            config = vectors_config[first_key]
            if hasattr(config, "size"):
                size = config.size

        if size == 1536:
            return {"model": "text-embedding-3-small", "dims": 1536}
        elif size == 3072:
            return {"model": "gemini-embedding-001", "dims": 3072}
        elif size > 0:
            # æœªçŸ¥ã®æ¬¡å…ƒæ•°ã®å ´åˆã¯ã‚µã‚¤ã‚ºã ã‘æ›´æ–°ã—ã¦ãƒ¢ãƒ‡ãƒ«ã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆï¼ˆã¾ãŸã¯æ±ç”¨ï¼‰
            return {"model": "unknown-embedding-model", "dims": size}

        return default_params

    except Exception as e:
        logger.warning(f"ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³è¨­å®šå–å¾—å¤±æ•— ({collection_name}): {e}")
        return default_params


# ===================================================================
# ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£é–¢æ•°
# ===================================================================

def batched(seq: Iterable, size: int):
    """ã‚¤ãƒ†ãƒ©ãƒ–ãƒ«ã‚’ãƒãƒƒãƒã«åˆ†å‰²"""
    buf = []
    for x in seq:
        buf.append(x)
        if len(buf) >= size:
            yield buf
            buf = []
    if buf:
        yield buf


# ===================================================================
# Qdrantãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚«ãƒ¼
# ===================================================================

class QdrantHealthChecker:
    """Qdrantã‚µãƒ¼ãƒãƒ¼ã®æ¥ç¶šçŠ¶æ…‹ã‚’ãƒã‚§ãƒƒã‚¯"""

    def __init__(self, debug_mode: bool = False):
        self.debug_mode = debug_mode
        self.client = None

    def check_port(self, host: str, port: int, timeout: float = 2.0) -> bool:
        """ãƒãƒ¼ãƒˆãŒé–‹ã„ã¦ã„ã‚‹ã‹ãƒã‚§ãƒƒã‚¯"""
        try:
            sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
            sock.settimeout(timeout)
            result = sock.connect_ex((host, port))
            sock.close()
            return result == 0
        except Exception as e:
            if self.debug_mode:
                logger.error(f"Port check failed for {host}:{port}: {e}")
            return False

    def check_qdrant(self) -> Tuple[bool, str, Optional[Dict]]:
        """Qdrantæ¥ç¶šãƒã‚§ãƒƒã‚¯"""
        start_time = time.time()

        # ã¾ãšãƒãƒ¼ãƒˆãƒã‚§ãƒƒã‚¯
        if not self.check_port(QDRANT_CONFIG["host"], QDRANT_CONFIG["port"]):
            return False, "Connection refused (port closed)", None

        try:
            self.client = QdrantClient(url=QDRANT_CONFIG["url"], timeout=5)

            # ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³å–å¾—
            collections = self.client.get_collections()

            metrics = {
                "collection_count": len(collections.collections),
                "collections": [c.name for c in collections.collections],
                "response_time_ms": round((time.time() - start_time) * 1000, 2),
            }

            return True, "Connected", metrics

        except Exception as e:
            error_msg = str(e)
            if self.debug_mode:
                error_msg = f"{error_msg}\n{traceback.format_exc()}"
            return False, error_msg, None


# ===================================================================
# Qdrantãƒ‡ãƒ¼ã‚¿ãƒ•ã‚§ãƒƒãƒãƒ£ãƒ¼
# ===================================================================

class QdrantDataFetcher:
    """Qdrantã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—"""

    def __init__(self, client: QdrantClient):
        self.client = client

    def fetch_collections(self) -> pd.DataFrame:
        """ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ä¸€è¦§ã‚’å–å¾—"""
        try:
            collections_response = self.client.get_collections()
            collections = collections_response.collections # Actual list of collections
            
            data = []
            for collection in collections: # Iterate through each collection object
                try:
                    info = self.client.get_collection(collection.name) # Try to get detailed info
                    data.append(
                        {
                            "Collection": collection.name,
                            "Vectors Count": info.vectors_count,
                            "Points Count": info.points_count,
                            "Indexed Vectors": info.indexed_vectors_count,
                            "Status": info.status,
                        }
                    )
                except Exception as inner_e:
                    logger.warning(f"Failed to fetch details for collection '{collection.name}': {inner_e}")
                    data.append(
                        {
                            "Collection": collection.name,
                            "Vectors Count": "N/A",
                            "Points Count": "N/A",
                            "Indexed Vectors": "N/A",
                            "Status": "Error",
                        }
                    )

            return (
                pd.DataFrame(data)
                if data
                else pd.DataFrame({"Info": ["No collections found"]})
            )

        except Exception as outer_e:
            logger.error(f"Failed to list collections from Qdrant: {outer_e}")
            return pd.DataFrame({"Error": [str(outer_e)]})

    def fetch_collection_points(
        self, collection_name: str, limit: int = 50
    ) -> pd.DataFrame:
        """ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ã®è©³ç´°ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—"""
        try:
            # ã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«ã‚’ä½¿ã£ã¦ãƒã‚¤ãƒ³ãƒˆã‚’å–å¾—
            points_result = self.client.scroll(
                collection_name=collection_name,
                limit=limit,
                with_payload=True,
                with_vectors=False,
            )

            points = points_result[0]  # scrollã¯ (points, next_offset) ã®ã‚¿ãƒ—ãƒ«ã‚’è¿”ã™

            if not points:
                return pd.DataFrame({"Info": ["No points found in collection"]})

            # ãƒã‚¤ãƒ³ãƒˆã‚’DataFrameã«å¤‰æ›
            data = []
            for point in points:
                row = {"ID": point.id}

                # payloadã®å„ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‚’åˆ—ã¨ã—ã¦è¿½åŠ 
                if point.payload:
                    for key, value in point.payload.items():
                        # é•·ã™ãã‚‹æ–‡å­—åˆ—ã¯åˆ‡ã‚Šè©°ã‚
                        if isinstance(value, str) and len(value) > 200:
                            row[key] = value[:200] + "..."
                        elif isinstance(value, (list, dict)):
                            row[key] = (
                                str(value)[:200] + "..."
                                if len(str(value)) > 200
                                else str(value)
                            )
                        else:
                            row[key] = value

                data.append(row)

            return pd.DataFrame(data)

        except Exception as e:
            return pd.DataFrame({"Error": [str(e)]})

    def fetch_collection_info(self, collection_name: str) -> Dict[str, Any]:
        """ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ã®è©³ç´°æƒ…å ±ã‚’å–å¾—"""
        try:
            collection_info = self.client.get_collection(collection_name)

            # configã®æ§‹é€ ã‚’å®‰å…¨ã«ã‚¢ã‚¯ã‚»ã‚¹
            vector_config = collection_info.config.params.vectors

            # vector_configã®å‹ã‚’åˆ¤å®šã—ã¦é©åˆ‡ã«å‡¦ç†
            if hasattr(vector_config, "size"):
                # å˜ä¸€ãƒ™ã‚¯ãƒˆãƒ«è¨­å®š
                vector_size = vector_config.size
                distance = vector_config.distance
            elif hasattr(vector_config, "__iter__"):
                # Named vectorsè¨­å®šã®å ´åˆ
                vector_sizes = {}
                distances = {}
                for name, config in (
                    vector_config.items() if isinstance(vector_config, dict) else []
                ):
                    vector_sizes[name] = (
                        config.size if hasattr(config, "size") else "N/A"
                    )
                    distances[name] = (
                        config.distance if hasattr(config, "distance") else "N/A"
                    )
                vector_size = vector_sizes if vector_sizes else "N/A"
                distance = distances if distances else "N/A"
            else:
                vector_size = "N/A"
                distance = "N/A"

            return {
                "vectors_count": collection_info.vectors_count,
                "points_count": collection_info.points_count,
                "indexed_vectors": collection_info.indexed_vectors_count,
                "status": collection_info.status,
                "config": {
                    "vector_size": vector_size,
                    "distance": distance,
                },
            }
        except Exception as e:
            return {"error": str(e)}

    def fetch_collection_source_info(
        self, collection_name: str, sample_size: int = 200
    ) -> Dict[str, Any]:
        """ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ã®ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹æƒ…å ±ã‚’å–å¾—"""
        try:
            collection_info = self.client.get_collection(collection_name)
            total_points = collection_info.points_count

            # ã‚µãƒ³ãƒ—ãƒ«ãƒã‚¤ãƒ³ãƒˆã‚’å–å¾—
            points_result = self.client.scroll(
                collection_name=collection_name,
                limit=min(sample_size, total_points),
                with_payload=True,
                with_vectors=False,
            )

            points = points_result[0]

            if not points:
                return {"total_points": total_points, "sources": {}, "sample_size": 0}

            # sourceã¨generation_methodã‚’é›†è¨ˆ
            source_stats = {}
            for point in points:
                if point.payload:
                    source = point.payload.get("source", "unknown")
                    method = point.payload.get("generation_method", "unknown")
                    domain = point.payload.get("domain", "unknown")

                    if source not in source_stats:
                        source_stats[source] = {
                            "sample_count": 0,
                            "method": method,
                            "domain": domain,
                        }
                    source_stats[source]["sample_count"] += 1

            # å…¨ä½“ã®ãƒ‡ãƒ¼ã‚¿æ•°ã‚’æ¨å®š
            sample_total = len(points)
            for source, stats in source_stats.items():
                ratio = stats["sample_count"] / sample_total
                stats["estimated_total"] = int(total_points * ratio)
                stats["percentage"] = ratio * 100

            return {
                "total_points": total_points,
                "sources": source_stats,
                "sample_size": sample_total,
            }

        except Exception as e:
            return {"error": str(e)}


# ===================================================================
# ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ç®¡ç†é–¢æ•°
# ===================================================================

def get_collection_stats(
    client: QdrantClient, collection_name: str
) -> Optional[Dict[str, Any]]:
    """ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ã®çµ±è¨ˆæƒ…å ±ã‚’å–å¾—"""
    try:
        collection_info = client.get_collection(collection_name)
        total_points = collection_info.points_count

        # ãƒ™ã‚¯ãƒˆãƒ«è¨­å®šæƒ…å ±ã‚’å–å¾—
        vectors_config = collection_info.config.params.vectors
        vector_info = {}

        if isinstance(vectors_config, dict):
            # Named Vectors
            for name, config in vectors_config.items():
                vector_info[name] = {
                    "size": config.size,
                    "distance": str(config.distance),
                }
        elif hasattr(vectors_config, "size"):
            # Single Vector
            vector_info["default"] = {
                "size": vectors_config.size,
                "distance": str(vectors_config.distance),
            }

        return {
            "total_points": total_points,
            "vector_config": vector_info,
            "status": collection_info.status,
        }

    except UnexpectedResponse as e:
        if "doesn't exist" in str(e) or "not found" in str(e).lower():
            return None
        raise
    except Exception as e:
        logger.error(f"çµ±è¨ˆæƒ…å ±å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
        return None


def get_all_collections(client: QdrantClient) -> List[Dict[str, Any]]:
    """å…¨ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ã®æƒ…å ±ã‚’å–å¾—"""
    try:
        collections_response = client.get_collections()
        collections = collections_response.collections
        collection_list = []

        for collection in collections:
            try:
                info = client.get_collection(collection.name)
                collection_list.append(
                    {
                        "name": collection.name,
                        "points_count": info.points_count,
                        "status": info.status,
                    }
                )
            except Exception as inner_e:
                logger.warning(f"Failed to get info for collection '{collection.name}': {inner_e}")
                collection_list.append(
                    {"name": collection.name, "points_count": 0, "status": "Error"}
                )
        return collection_list
    except Exception as outer_e:
        logger.error(f"Failed to list collections from Qdrant: {outer_e}")
        return []


def delete_all_collections(client: QdrantClient, excluded: List[str] = None) -> int:
    """å…¨ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ã‚’å‰Šé™¤"""
    excluded = excluded or []
    collections = get_all_collections(client)

    if not collections:
        return 0

    to_delete = [c for c in collections if c["name"] not in excluded]

    if not to_delete:
        return 0

    deleted_count = 0
    failed_count = 0

    for col in to_delete:
        try:
            client.delete_collection(collection_name=col["name"])
            deleted_count += 1
        except Exception as e:
            logger.error(f"ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³å‰Šé™¤ã‚¨ãƒ©ãƒ¼ {col['name']}: {e}")
            failed_count += 1

    return deleted_count


# ===================================================================
# ãƒ‡ãƒ¼ã‚¿å‡¦ç†ãƒ»ç™»éŒ²é–¢æ•°
# ===================================================================

def load_csv_for_qdrant(
    path: str, required=("question", "answer"), limit: int = 0
) -> pd.DataFrame:
    """CSVã‚’ãƒ­ãƒ¼ãƒ‰ï¼ˆQdrantç™»éŒ²ç”¨ï¼‰"""
    if not os.path.exists(path):
        raise FileNotFoundError(f"CSV not found: {path}")
    df = pd.read_csv(path)

    # åˆ—åãƒãƒƒãƒ”ãƒ³ã‚°
    column_mappings = {
        "Question": "question",
        "Response": "answer",
        "Answer": "answer",
        "correct_answer": "answer",
    }
    df = df.rename(columns=column_mappings)

    for col in required:
        if col not in df.columns:
            raise ValueError(
                f"{path} ã«ã¯ '{col}' åˆ—ãŒå¿…è¦ã§ã™ï¼ˆåˆ—: {list(df.columns)}ï¼‰"
            )

    df = df.fillna("").drop_duplicates(subset=list(required)).reset_index(drop=True)

    if limit and limit > 0:
        df = df.head(limit).copy()

    return df


def build_inputs_for_embedding(df: pd.DataFrame, include_answer: bool) -> List[str]:
    """åŸ‹ã‚è¾¼ã¿ç”¨å…¥åŠ›ãƒ†ã‚­ã‚¹ãƒˆã‚’æ§‹ç¯‰"""
    if include_answer:
        return (df["question"].astype(str) + "\n" + df["answer"].astype(str)).tolist()
    return df["question"].astype(str).tolist()


def embed_texts_for_qdrant(
    texts: List[str], model: str = "gemini-embedding-001", batch_size: int = 100
) -> List[List[float]]:
    """ãƒ†ã‚­ã‚¹ãƒˆã‚’ãƒãƒƒãƒå‡¦ç†ã§Embeddingã«å¤‰æ›ï¼ˆGemini APIä½¿ç”¨ï¼‰"""
    # Gemini Embeddingã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’ä½¿ç”¨
    embedding_client = create_embedding_client(provider="gemini")
    dims = get_embedding_dimensions("gemini")  # 3072

    # ç©ºæ–‡å­—åˆ—ãƒ»ç©ºç™½ã®ã¿ã®æ–‡å­—åˆ—ã‚’é™¤å¤–
    valid_texts = []
    valid_indices = []
    for i, text in enumerate(texts):
        if text and text.strip():
            valid_texts.append(text)
            valid_indices.append(i)

    if not valid_texts:
        logger.warning("å…¨ã¦ã®ãƒ†ã‚­ã‚¹ãƒˆãŒç©ºæ–‡å­—åˆ—ã§ã™ã€‚ãƒ€ãƒŸãƒ¼ãƒ™ã‚¯ãƒˆãƒ«ã‚’è¿”ã—ã¾ã™ã€‚")
        return [[0.0] * dims] * len(texts)

    # Gemini Embeddingã§ãƒãƒƒãƒå‡¦ç†
    valid_vecs = embedding_client.embed_texts(valid_texts, batch_size=batch_size)

    # å…ƒã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã«åˆã‚ã›ã¦ãƒ™ã‚¯ãƒˆãƒ«ã‚’å†é…ç½®
    vecs: List[List[float]] = []
    valid_vec_idx = 0
    for i in range(len(texts)):
        if i in valid_indices:
            vecs.append(valid_vecs[valid_vec_idx])
            valid_vec_idx += 1
        else:
            vecs.append([0.0] * dims)

    return vecs


def create_or_recreate_collection_for_qdrant(
    client: QdrantClient, name: str, recreate: bool, vector_size: int = 3072, use_sparse: bool = False
):
    """
    ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ä½œæˆã¾ãŸã¯å†ä½œæˆ
    
    Args:
        client: QdrantClient
        name: ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³å
        recreate: å†ä½œæˆãƒ•ãƒ©ã‚°
        vector_size: ãƒ™ã‚¯ãƒˆãƒ«æ¬¡å…ƒæ•°
        use_sparse: Sparse Vector (Hybrid Search) ã‚’æœ‰åŠ¹ã«ã™ã‚‹ã‹
    """
    # Dense Vectorè¨­å®š
    vectors_config = models.VectorParams(
        size=vector_size, distance=models.Distance.COSINE
    )
    
    # Hybrid Search (Named Vectors) ã®å ´åˆã€"default" ã¨ã„ã†åå‰ã§Denseã‚’è¨­å®šã™ã‚‹ã®ãŒãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹ã ãŒ
    # æ—¢å­˜ã¨ã®äº’æ›æ€§ã®ãŸã‚ã€vectors_configã‚’è¾æ›¸ã«ã™ã‚‹
    if use_sparse:
        vectors_config = {
            "default": models.VectorParams(
                size=vector_size, 
                distance=models.Distance.COSINE
            )
        }
    
    # Sparse Vectorè¨­å®š
    sparse_vectors_config = None
    if use_sparse:
        sparse_vectors_config = {
            "text-sparse": models.SparseVectorParams(
                index=models.SparseIndexParams(
                    on_disk=False, 
                )
            )
        }

    if recreate:
        try:
            client.delete_collection(collection_name=name)
        except Exception:
            pass
        client.create_collection(
            collection_name=name, 
            vectors_config=vectors_config,
            sparse_vectors_config=sparse_vectors_config
        )
    else:
        try:
            client.get_collection(name)
        except Exception:
            client.create_collection(
                collection_name=name, 
                vectors_config=vectors_config,
                sparse_vectors_config=sparse_vectors_config
            )

    # ãƒšã‚¤ãƒ­ãƒ¼ãƒ‰ç´¢å¼•ã‚’ä½œæˆ
    try:
        client.create_payload_index(
            name, field_name="domain", field_schema=models.PayloadSchemaType.KEYWORD
        )
    except Exception:
        pass


def build_points_for_qdrant(
    df: pd.DataFrame, 
    vectors: List[List[float]], 
    domain: str, 
    source_file: str,
    sparse_vectors: Optional[List[models.SparseVector]] = None
) -> List[models.PointStruct]:
    """
    Qdrantãƒã‚¤ãƒ³ãƒˆã‚’æ§‹ç¯‰

    Args:
        df: DataFrame
        vectors: DenseåŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«
        domain: ãƒ‰ãƒ¡ã‚¤ãƒ³å
        source_file: ã‚½ãƒ¼ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«å
        sparse_vectors: SparseåŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ« (Optional)

    Returns:
        PointStructã®ãƒªã‚¹ãƒˆ
    """
    n = len(df)
    if len(vectors) != n:
        raise ValueError(f"vectors length mismatch: df={n}, vecs={len(vectors)}")
    
    if sparse_vectors and len(sparse_vectors) != n:
        raise ValueError(f"sparse_vectors length mismatch: df={n}, sparse_vecs={len(sparse_vectors)}")

    now_iso = datetime.now(timezone.utc).isoformat()
    points: List[models.PointStruct] = []

    for i, row in enumerate(df.itertuples(index=False)):
        payload = {
            "domain": domain,
            "question": getattr(row, "question"),
            "answer": getattr(row, "answer"),
            "source": os.path.basename(source_file),
            "created_at": now_iso,
            "schema": "qa:v1",
        }

        pid = abs(hash(f"{domain}-{source_file}-{i}")) & 0x7FFFFFFFFFFFFFFF
        
        # ãƒ™ã‚¯ãƒˆãƒ«æ§‹é€ ã®æ§‹ç¯‰
        if sparse_vectors:
            # Hybrid Searchç”¨ Named Vectors
            # "default": Dense Vector (Gemini/OpenAI)
            # "text-sparse": Sparse Vector (Splade)
            vector_struct = {
                "default": vectors[i],
                "text-sparse": sparse_vectors[i]
            }
        else:
            # Single Dense Vector (Legacy)
            vector_struct = vectors[i]

        points.append(models.PointStruct(id=pid, vector=vector_struct, payload=payload))

    return points


def upsert_points_to_qdrant(
    client: QdrantClient,
    collection: str,
    points: List[models.PointStruct],
    batch_size: int = 128,
) -> int:
    """ãƒã‚¤ãƒ³ãƒˆã‚’Qdrantã«ã‚¢ãƒƒãƒ—ã‚µãƒ¼ãƒˆ"""
    count = 0
    for chunk in batched(points, batch_size):
        client.upsert(collection_name=collection, points=chunk)
        count += len(chunk)
    return count


# ===================================================================
# æ¤œç´¢é–¢æ•°
# ===================================================================

def embed_query_for_search(
    query: str, model: str = "gemini-embedding-001", dims: Optional[int] = None
) -> List[float]:
    """
    æ¤œç´¢ã‚¯ã‚¨ãƒªã‚’ãƒ™ã‚¯ãƒˆãƒ«åŒ–
    
    æ¬¡å…ƒæ•°(dims)ã¾ãŸã¯ãƒ¢ãƒ‡ãƒ«å(model)ã«åŸºã¥ã„ã¦ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã‚’è‡ªå‹•é¸æŠã—ã¾ã™ã€‚
    """
    # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯Gemini
    provider = "gemini"
    
    # æ¬¡å…ƒæ•°ã«ã‚ˆã‚‹åˆ¤å®š
    if dims == 1536:
        provider = "openai"
    elif dims == 3072:
        provider = "gemini"
    
    # ãƒ¢ãƒ‡ãƒ«åã«ã‚ˆã‚‹åˆ¤å®š (æ¬¡å…ƒæ•°ãŒæŒ‡å®šã•ã‚Œã¦ã„ãªã„å ´åˆã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯)
    elif model:
        if "text-embedding-3" in model or "text-embedding-ada" in model:
            provider = "openai"
        elif "gemini" in model:
            provider = "gemini"
            
    logger.info(f"embed_query_for_search: query='{query}', model='{model}', dims={dims} -> provider='{provider}'")
    
    embedding_client = create_embedding_client(provider=provider)
    vector = embedding_client.embed_text(query)
    
    logger.info(f"embed_query_for_search: generated vector dim={len(vector)}")
    return vector


# ===================================================================
# ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³çµ±åˆé–¢æ•°
# ===================================================================

def scroll_all_points_with_vectors(
    client: QdrantClient,
    collection_name: str,
    batch_size: int = 100,
    progress_callback: Optional[callable] = None,
) -> List[models.Record]:
    """ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ã‹ã‚‰å…¨ãƒã‚¤ãƒ³ãƒˆï¼ˆãƒ™ã‚¯ãƒˆãƒ«å«ã‚€ï¼‰ã‚’å–å¾—

    Args:
        client: QdrantClient
        collection_name: ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³å
        batch_size: 1å›ã®ã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«ã§å–å¾—ã™ã‚‹ä»¶æ•°
        progress_callback: é€²æ—ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯ (å–å¾—æ¸ˆã¿ä»¶æ•°, ç·ä»¶æ•°)

    Returns:
        å…¨ãƒã‚¤ãƒ³ãƒˆã®ãƒªã‚¹ãƒˆ
    """
    all_points = []
    offset = None

    # ç·ä»¶æ•°ã‚’å–å¾—
    collection_info = client.get_collection(collection_name)
    total_points = collection_info.points_count

    while True:
        points, next_offset = client.scroll(
            collection_name=collection_name,
            limit=batch_size,
            offset=offset,
            with_payload=True,
            with_vectors=True,
        )

        if not points:
            break

        all_points.extend(points)

        if progress_callback:
            progress_callback(len(all_points), total_points)

        if next_offset is None:
            break

        offset = next_offset

    return all_points


def merge_collections(
    client: QdrantClient,
    source_collections: List[str],
    target_collection: str,
    recreate: bool = True,
    vector_size: int = 3072,
    progress_callback: Optional[callable] = None,
) -> Dict[str, Any]:
    """è¤‡æ•°ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ã‚’çµ±åˆã—ã¦æ–°ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ã«ç™»éŒ²

    Args:
        client: QdrantClient
        source_collections: çµ±åˆå…ƒã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³åã®ãƒªã‚¹ãƒˆ
        target_collection: çµ±åˆå…ˆã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³å
        recreate: æ—¢å­˜ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ã‚’å‰Šé™¤ã—ã¦å†ä½œæˆã™ã‚‹ã‹
        vector_size: ãƒ™ã‚¯ãƒˆãƒ«ã‚µã‚¤ã‚º
        progress_callback: é€²æ—ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯ (ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸, ç¾åœ¨å€¤, æœ€å¤§å€¤)

    Returns:
        çµ±åˆçµæœã®è¾æ›¸
    """
    result = {
        "source_collections": source_collections,
        "target_collection": target_collection,
        "points_per_collection": {},
        "total_points": 0,
        "success": False,
        "error": None,
    }

    try:
        # ã‚¹ãƒ†ãƒƒãƒ—1: çµ±åˆå…ˆã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ã‚’ä½œæˆ
        if progress_callback:
            progress_callback(f"ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ '{target_collection}' ã‚’ä½œæˆä¸­...", 0, 100)

        create_or_recreate_collection_for_qdrant(
            client, target_collection, recreate, vector_size
        )

        # ã‚¹ãƒ†ãƒƒãƒ—2: å„ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ã‹ã‚‰ãƒã‚¤ãƒ³ãƒˆã‚’å–å¾—ã—ã¦çµ±åˆ
        all_points = []
        collection_count = len(source_collections)

        for idx, src_collection in enumerate(source_collections):
            if progress_callback:
                progress_callback(
                    f"ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ '{src_collection}' ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿å–å¾—ä¸­...",
                    int((idx / collection_count) * 50),
                    100,
                )

            # ãƒã‚¤ãƒ³ãƒˆã‚’å–å¾—
            points = scroll_all_points_with_vectors(client, src_collection)
            result["points_per_collection"][src_collection] = len(points)

            # ãƒã‚¤ãƒ³ãƒˆIDã‚’å†ç”Ÿæˆï¼ˆé‡è¤‡å›é¿ï¼‰
            for i, point in enumerate(points):
                # å…ƒã®payloadã«ã‚½ãƒ¼ã‚¹ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³æƒ…å ±ã‚’è¿½åŠ 
                payload = dict(point.payload) if point.payload else {}
                payload["_source_collection"] = src_collection
                payload["_original_id"] = point.id

                # æ–°ã—ã„IDã‚’ç”Ÿæˆ
                new_id = abs(
                    hash(f"{target_collection}-{src_collection}-{point.id}-{i}")
                ) & 0x7FFFFFFFFFFFFFFF

                all_points.append(
                    models.PointStruct(
                        id=new_id,
                        vector=point.vector,
                        payload=payload,
                    )
                )

        result["total_points"] = len(all_points)

        # ã‚¹ãƒ†ãƒƒãƒ—3: çµ±åˆå…ˆã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ã«ã‚¢ãƒƒãƒ—ã‚µãƒ¼ãƒˆ
        if progress_callback:
            progress_callback("çµ±åˆãƒ‡ãƒ¼ã‚¿ã‚’ã‚¢ãƒƒãƒ—ã‚µãƒ¼ãƒˆä¸­...", 50, 100)

        if all_points:
            upserted = 0
            batch_size = 128
            for chunk in batched(all_points, batch_size):
                client.upsert(collection_name=target_collection, points=chunk)
                upserted += len(chunk)
                if progress_callback:
                    progress = 50 + int((upserted / len(all_points)) * 50)
                    progress_callback(
                        f"ã‚¢ãƒƒãƒ—ã‚µãƒ¼ãƒˆä¸­... ({upserted}/{len(all_points)})",
                        progress,
                        100,
                    )

        result["success"] = True

        if progress_callback:
            progress_callback("çµ±åˆå®Œäº†", 100, 100)

    except Exception as e:
        result["error"] = str(e)
        logger.error(f"ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³çµ±åˆã‚¨ãƒ©ãƒ¼: {e}")

    return result