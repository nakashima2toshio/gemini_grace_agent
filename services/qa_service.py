#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
qa_service.py - Q/Aç”Ÿæˆã‚µãƒ¼ãƒ“ã‚¹
================================
Q/Aãƒšã‚¢ã®ç”Ÿæˆã¨ä¿å­˜ã«é–¢ã™ã‚‹ãƒ“ã‚¸ãƒã‚¹ãƒ­ã‚¸ãƒƒã‚¯

æ©Ÿèƒ½:
- make_qa.py (QAPipeline) ã®å®Ÿè¡Œ
- OpenAI APIã«ã‚ˆã‚‹Q/Aç”Ÿæˆ
- Q/Aãƒšã‚¢ã®ä¿å­˜
"""

import os
import sys
import re
import json
import logging
import subprocess
import threading
import queue
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Any, Optional

import pandas as pd
from helper_llm import create_llm_client

# ãƒ¢ãƒ‡ãƒ«ã‹ã‚‰ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from models import QAPair, QAPairsResponse

# ãƒ­ã‚°è¨­å®š
logger = logging.getLogger(__name__)


def run_advanced_qa_generation(
    dataset: Optional[str],
    input_file: Optional[str],
    use_celery: bool,
    celery_workers: int,
    batch_chunks: int,
    max_docs: int,
    merge_chunks: bool,
    min_tokens: int,
    max_tokens: int,
    coverage_threshold: float,
    model: str,
    analyze_coverage: bool,
    log_callback,
    progress_callback=None,
) -> Dict[str, Any]:
    """
    Q/Aç”Ÿæˆã‚’å®Ÿè¡Œï¼ˆç›´æ¥ã‚¤ãƒ³ãƒãƒ¼ãƒˆãƒ¢ãƒ¼ãƒ‰ï¼‰
    
    ãƒ—ãƒ­ã‚»ã‚¹é–“é€šä¿¡ã®å•é¡Œã‚’å›é¿ã™ã‚‹ãŸã‚ã€ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã¨ã—ã¦ã‚¤ãƒ³ãƒãƒ¼ãƒˆã—ã¦ç›´æ¥å®Ÿè¡Œã—ã¾ã™ã€‚
    """
    try:
        # ãƒ«ãƒ¼ãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ãƒ‘ã‚¹ã«è¿½åŠ ã—ã¦ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
        sys.path.append(os.getcwd())
        import qa_generator_runner
        
        log_callback("ğŸš€ Q/Aç”Ÿæˆãƒ—ãƒ­ã‚»ã‚¹ã‚’ç›´æ¥å®Ÿè¡Œã—ã¾ã™...")
        
        result = qa_generator_runner.run_qa_generator(
            dataset=dataset,
            input_file=input_file,
            model=model,
            max_docs=max_docs,
            analyze_coverage=analyze_coverage,
            batch_chunks=batch_chunks,
            merge_chunks=merge_chunks,
            min_tokens=min_tokens,
            max_tokens=max_tokens,
            use_celery=use_celery,
            celery_workers=celery_workers,
            coverage_threshold=coverage_threshold,
            log_callback=log_callback
        )
        
        return result

    except Exception as e:
        log_callback(f"âŒ å®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {str(e)}")
        import traceback
        log_callback(traceback.format_exc())
        return {"success": False, "error": str(e)}


def generate_qa_pairs(
    text: str,
    dataset_type: str,
    chunk_id: str,
    model: str = "gemini-2.0-flash",
    qa_per_chunk: int = 3,
    log_callback=None,
) -> List[QAPair]:
    """
    ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰Q/Aãƒšã‚¢ã‚’ç”Ÿæˆï¼ˆGemini APIä½¿ç”¨ï¼‰

    Args:
        text: å¯¾è±¡ãƒ†ã‚­ã‚¹ãƒˆ
        dataset_type: ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚¿ã‚¤ãƒ—
        chunk_id: ãƒãƒ£ãƒ³ã‚¯ID
        model: ä½¿ç”¨ã™ã‚‹ãƒ¢ãƒ‡ãƒ«ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: gemini-2.0-flashï¼‰
        qa_per_chunk: ãƒãƒ£ãƒ³ã‚¯ã‚ãŸã‚Šã®Q/Aæ•°
        log_callback: ãƒ­ã‚°ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯é–¢æ•°

    Returns:
        Q/Aãƒšã‚¢ã®ãƒªã‚¹ãƒˆ
    """
    # Geminiã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’ä½¿ç”¨
    client = create_llm_client(provider="gemini")

    prompt = f"""ã‚ãªãŸã¯æ•™è‚²ç”¨Q/Aãƒšã‚¢ç”Ÿæˆã®å°‚é–€å®¶ã§ã™ã€‚

ä»¥ä¸‹ã®ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰ã€{qa_per_chunk}å€‹ã®è³ªå•ã¨å›ç­”ã®ãƒšã‚¢ã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚

ãƒ†ã‚­ã‚¹ãƒˆ:
{text}

è¦ä»¶:
1. è³ªå•ã¯å…·ä½“çš„ã§æ˜ç¢ºãªã‚‚ã®ã«ã™ã‚‹
2. å›ç­”ã¯ãƒ†ã‚­ã‚¹ãƒˆã®å†…å®¹ã«åŸºã¥ã„ãŸæ­£ç¢ºãªã‚‚ã®ã«ã™ã‚‹
3. è³ªå•ã‚¿ã‚¤ãƒ—ã¯ä»¥ä¸‹ã‹ã‚‰é¸æŠ: factual, conceptual, application, analysis
4. ãƒ†ã‚­ã‚¹ãƒˆã®é‡è¦ãªæƒ…å ±ã‚’ç¶²ç¾…ã™ã‚‹ã‚ˆã†ã«ã™ã‚‹

JSONå½¢å¼ã§å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚
"""

    try:
        # Geminiæ§‹é€ åŒ–å‡ºåŠ›APIã‚’ä½¿ç”¨
        qa_response = client.generate_structured(
            prompt=prompt,
            response_schema=QAPairsResponse,
            model=model
        )

        # Q/Aãƒšã‚¢ã«ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’è¿½åŠ 
        result_pairs = []
        for qa in qa_response.qa_pairs:
            qa_pair = QAPair(
                question=qa.question,
                answer=qa.answer,
                question_type=qa.question_type,
                source_chunk_id=chunk_id,
                dataset_type=dataset_type,
                auto_generated=True
            )
            result_pairs.append(qa_pair)

        if log_callback:
            log_callback(f"    â””â”€ {len(result_pairs)}å€‹ã®Q/Aãƒšã‚¢ã‚’ç”Ÿæˆ")

        return result_pairs

    except Exception as e:
        logger.error(f"Q/Aç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")
        if log_callback:
            log_callback(f"    â””â”€ ã‚¨ãƒ©ãƒ¼: {str(e)}")
        return []


def save_qa_pairs_to_file(
    qa_pairs: List[QAPair], dataset_type: str, log_callback=None
) -> Dict[str, str]:
    """
    Q/Aãƒšã‚¢ã‚’CSVã¨JSONã§ä¿å­˜

    Args:
        qa_pairs: Q/Aãƒšã‚¢ã®ãƒªã‚¹ãƒˆ
        dataset_type: ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚¿ã‚¤ãƒ—
        log_callback: ãƒ­ã‚°ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯é–¢æ•°

    Returns:
        ä¿å­˜ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã®è¾æ›¸
    """
    qa_output_dir = Path("qa_output")
    qa_output_dir.mkdir(exist_ok=True)

    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    saved_files = {}

    # DataFrameã«å¤‰æ›
    qa_data = []
    for qa in qa_pairs:
        qa_data.append(
            {
                "question": qa.question,
                "answer": qa.answer,
                "question_type": qa.question_type,
                "source_chunk_id": qa.source_chunk_id,
                "dataset_type": qa.dataset_type,
                "auto_generated": qa.auto_generated,
            }
        )

    df_qa = pd.DataFrame(qa_data)

    # CSVãƒ•ã‚¡ã‚¤ãƒ«
    csv_filename = f"qa_pairs_{dataset_type}_{timestamp}.csv"
    csv_path = qa_output_dir / csv_filename
    df_qa.to_csv(csv_path, index=False, encoding="utf-8-sig")
    saved_files["csv"] = str(csv_path)

    if log_callback:
        log_callback(f"  ğŸ“„ CSVä¿å­˜: {csv_filename}")

    # JSONãƒ•ã‚¡ã‚¤ãƒ«
    json_filename = f"qa_pairs_{dataset_type}_{timestamp}.json"
    json_path = qa_output_dir / json_filename

    json_data = {
        "dataset_type": dataset_type,
        "created_at": datetime.now().isoformat(),
        "total_pairs": len(qa_pairs),
        "qa_pairs": qa_data,
    }

    with open(json_path, "w", encoding="utf-8") as f:
        json.dump(json_data, f, ensure_ascii=False, indent=2)

    saved_files["json"] = str(json_path)

    if log_callback:
        log_callback(f"  ğŸ“‹ JSONä¿å­˜: {json_filename}")

    return saved_files