#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
dataset_service.py - ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæ“ä½œã‚µãƒ¼ãƒ“ã‚¹
=============================================
ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã€èª­ã¿è¾¼ã¿ã€å‰å‡¦ç†ã‚’æ‹…å½“

æ©Ÿèƒ½:
- HuggingFaceãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
- Livedoorã‚³ãƒ¼ãƒ‘ã‚¹ã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãƒ»èª­ã¿è¾¼ã¿
- ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿
- ãƒ†ã‚­ã‚¹ãƒˆã®å‰å‡¦ç†ãƒ»æŠ½å‡º
"""

import logging
import json
import tarfile
import urllib.request
from pathlib import Path
from typing import Dict, Any, Optional, Callable

import pandas as pd

from helper_text import clean_text  # helper_ragã§ã¯ãªãhelper_textã‹ã‚‰ç›´æ¥ã‚¤ãƒ³ãƒãƒ¼ãƒˆï¼ˆå¾ªç’°å‚ç…§å›é¿ï¼‰

logger = logging.getLogger(__name__)


def download_livedoor_corpus(save_dir: str = "datasets") -> str:
    """
    Livedoorãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚³ãƒ¼ãƒ‘ã‚¹ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰

    Args:
        save_dir: ä¿å­˜ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª

    Returns:
        è§£å‡å¾Œã®ãƒ‡ãƒ¼ã‚¿ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãƒ‘ã‚¹
    """
    save_path = Path(save_dir)
    save_path.mkdir(exist_ok=True)

    url = "https://www.rondhuit.com/download/ldcc-20140209.tar.gz"
    tar_filename = "ldcc-20140209.tar.gz"
    tar_path = save_path / tar_filename

    # ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
    if not tar_path.exists():
        logger.info(f"Livedoorãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚³ãƒ¼ãƒ‘ã‚¹ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ä¸­: {url}")
        urllib.request.urlretrieve(url, tar_path)
        logger.info(f"ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å®Œäº†: {tar_path}")

    # è§£å‡
    extract_path = save_path / "livedoor"
    if not extract_path.exists():
        logger.info("ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–ã‚’è§£å‡ä¸­...")
        with tarfile.open(tar_path, "r:gz") as tar:
            tar.extractall(save_path)
        logger.info("è§£å‡å®Œäº†")

    # textãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’æ¢ã™
    text_dir = extract_path / "text"
    if not text_dir.exists():
        text_dir = save_path / "text"

    return str(text_dir)


def load_livedoor_corpus(data_dir: str) -> pd.DataFrame:
    """
    Livedoorã‚³ãƒ¼ãƒ‘ã‚¹ã‚’èª­ã¿è¾¼ã¿

    Args:
        data_dir: ãƒ‡ãƒ¼ã‚¿ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãƒ‘ã‚¹

    Returns:
        DataFrameã¨ã—ã¦èª­ã¿è¾¼ã¾ã‚ŒãŸãƒ‡ãƒ¼ã‚¿
    """
    data_path = Path(data_dir)
    records = []

    # ã‚«ãƒ†ã‚´ãƒªãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’èµ°æŸ»
    for category_dir in data_path.iterdir():
        if not category_dir.is_dir():
            continue

        if category_dir.name in ["CHANGES.txt", "README.txt", "LICENSE.txt"]:
            continue

        category = category_dir.name

        # è¨˜äº‹ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿
        for article_file in category_dir.glob("*.txt"):
            if article_file.name.startswith("LICENSE"):
                continue

            try:
                with open(article_file, "r", encoding="utf-8") as f:
                    lines = f.readlines()

                # Livedoorå½¢å¼: 1è¡Œç›®=URL, 2è¡Œç›®=æ—¥ä»˜, 3è¡Œç›®=ã‚¿ã‚¤ãƒˆãƒ«, æ®‹ã‚Š=æœ¬æ–‡
                if len(lines) >= 4:
                    url = lines[0].strip()
                    date = lines[1].strip()
                    title = lines[2].strip()
                    content = "".join(lines[3:]).strip()

                    records.append(
                        {
                            "url": url,
                            "date": date,
                            "title": title,
                            "content": content,
                            "category": category,
                        }
                    )
            except Exception as e:
                logger.warning(f"ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼ {article_file}: {e}")
                continue

    df = pd.DataFrame(records)
    logger.info(f"Livedoorã‚³ãƒ¼ãƒ‘ã‚¹èª­ã¿è¾¼ã¿å®Œäº†: {len(df)} ä»¶")
    return df


def download_hf_dataset(
    dataset_name: str,
    config_name: Optional[str],
    split: str,
    sample_size: int,
    log_callback: Callable[[str], None],
) -> pd.DataFrame:
    """
    HuggingFaceã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰

    Args:
        dataset_name: ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå
        config_name: ã‚³ãƒ³ãƒ•ã‚£ã‚°å
        split: ãƒ‡ãƒ¼ã‚¿åˆ†å‰²
        sample_size: ã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚º
        log_callback: ãƒ­ã‚°ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯é–¢æ•°

    Returns:
        DataFrame
    """
    from datasets import load_dataset as hf_load_dataset

    samples = []

    if dataset_name == "wikimedia/wikipedia":
        actual_config = config_name if config_name else "20231101.ja"
        log_callback(f"ğŸ“¥ {dataset_name} ã‚’ãƒ­ãƒ¼ãƒ‰ä¸­ (config: {actual_config})...")
        dataset = hf_load_dataset(
            dataset_name, actual_config, split=split, streaming=True
        )

        for i, item in enumerate(dataset):
            if i >= sample_size:
                break
            samples.append(item)
            if (i + 1) % 100 == 0:
                log_callback(f"é€²æ—: {i + 1}/{sample_size} ä»¶")

    elif dataset_name == "range3/cc100-ja":
        log_callback(f"ğŸ“¥ {dataset_name} ã‚’ãƒ­ãƒ¼ãƒ‰ä¸­...")
        dataset = hf_load_dataset(dataset_name, split=split, streaming=True)

        for i, item in enumerate(dataset):
            if i >= sample_size:
                break
            samples.append(item)
            if (i + 1) % 100 == 0:
                log_callback(f"é€²æ—: {i + 1}/{sample_size} ä»¶")

    elif dataset_name == "cc_news":
        log_callback(f"ğŸ“¥ {dataset_name} ã‚’ãƒ­ãƒ¼ãƒ‰ä¸­...")
        if config_name:
            dataset = hf_load_dataset(
                dataset_name, config_name, split=split, streaming=True
            )
        else:
            dataset = hf_load_dataset(dataset_name, split=split, streaming=True)

        for i, item in enumerate(dataset):
            if i >= sample_size:
                break
            samples.append(item)
            if (i + 1) % 50 == 0:
                log_callback(f"é€²æ—: {i + 1}/{sample_size} ä»¶")

    else:
        raise ValueError(f"æœªå¯¾å¿œã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ: {dataset_name}")

    df = pd.DataFrame(samples)
    log_callback(f"âœ… {len(df)} ä»¶ã®ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ­ãƒ¼ãƒ‰ã—ã¾ã—ãŸ")
    return df


def extract_text_content(df: pd.DataFrame, config: Dict[str, Any]) -> pd.DataFrame:
    """
    ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’æŠ½å‡º

    Args:
        df: å…ƒã®DataFrame
        config: ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆè¨­å®šï¼ˆtext_field, title_fieldã‚’å«ã‚€ï¼‰

    Returns:
        Combined_Textã‚«ãƒ©ãƒ ã‚’å«ã‚€DataFrame
    """
    text_field = config["text_field"]
    title_field = config.get("title_field")

    df_processed = df.copy()

    # ã‚¿ã‚¤ãƒˆãƒ«ã¨ãƒ†ã‚­ã‚¹ãƒˆã‚’çµåˆ
    if title_field and title_field in df.columns and text_field in df.columns:
        df_processed["Combined_Text"] = df_processed.apply(
            lambda row: f"{clean_text(str(row.get(title_field, '')))} {clean_text(str(row.get(text_field, '')))}".strip(),
            axis=1,
        )
    elif text_field in df.columns:
        df_processed["Combined_Text"] = df_processed[text_field].apply(
            lambda x: clean_text(str(x)) if x is not None else ""
        )
    else:
        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: åˆ©ç”¨å¯èƒ½ãªãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‚’æ¢ã™
        text_candidates = ["text", "content", "body", "document", "abstract"]
        found_field = None
        for field in text_candidates:
            if field in df.columns:
                found_field = field
                break

        if found_field:
            df_processed["Combined_Text"] = df_processed[found_field].apply(
                lambda x: clean_text(str(x)) if x is not None else ""
            )
        else:
            df_processed["Combined_Text"] = df_processed.apply(
                lambda row: " ".join([str(v) for v in row.values if v is not None]),
                axis=1,
            )

    # ç©ºã®ãƒ†ã‚­ã‚¹ãƒˆã‚’é™¤å¤–
    df_processed = df_processed[df_processed["Combined_Text"].str.strip() != ""]

    return df_processed


def load_uploaded_file(uploaded_file) -> pd.DataFrame:
    """
    ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿

    Args:
        uploaded_file: Streamlitã®file_uploaderã§å–å¾—ã—ãŸãƒ•ã‚¡ã‚¤ãƒ«

    Returns:
        Combined_Textã‚«ãƒ©ãƒ ã‚’å«ã‚€DataFrame
    """
    file_extension = uploaded_file.name.split(".")[-1].lower()

    try:
        if file_extension == "csv":
            # CSVãƒ•ã‚¡ã‚¤ãƒ«
            df = pd.read_csv(uploaded_file)

        elif file_extension in ["txt", "text"]:
            # ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆ1è¡Œ1ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆï¼‰
            content = uploaded_file.read().decode("utf-8")
            lines = [line.strip() for line in content.split("\n") if line.strip()]
            df = pd.DataFrame({"text": lines})

        elif file_extension == "json":
            # JSONãƒ•ã‚¡ã‚¤ãƒ«
            content = uploaded_file.read().decode("utf-8")
            data = json.loads(content)

            if isinstance(data, list):
                df = pd.DataFrame(data)
            elif isinstance(data, dict):
                df = pd.DataFrame([data])
            else:
                raise ValueError(
                    "JSONãƒ•ã‚¡ã‚¤ãƒ«ã¯ãƒªã‚¹ãƒˆã¾ãŸã¯ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã§ã‚ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™"
                )

        elif file_extension == "jsonl":
            # JSON Linesãƒ•ã‚¡ã‚¤ãƒ«
            content = uploaded_file.read().decode("utf-8")
            lines = [json.loads(line) for line in content.split("\n") if line.strip()]
            df = pd.DataFrame(lines)

        else:
            raise ValueError(f"æœªå¯¾å¿œã®ãƒ•ã‚¡ã‚¤ãƒ«å½¢å¼: {file_extension}")

        # Combined_Textã‚«ãƒ©ãƒ ã®ä½œæˆ
        if "Combined_Text" not in df.columns:
            # ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‚’æ¢ã™
            text_candidates = [
                "text",
                "content",
                "body",
                "document",
                "answer",
                "question",
            ]
            found_field = None

            for field in text_candidates:
                if field in df.columns:
                    found_field = field
                    break

            if found_field:
                df["Combined_Text"] = df[found_field].apply(
                    lambda x: clean_text(str(x)) if x is not None else ""
                )
            else:
                # å…¨ã‚«ãƒ©ãƒ ã‚’çµåˆ
                df["Combined_Text"] = df.apply(
                    lambda row: " ".join([str(v) for v in row.values if v is not None]),
                    axis=1,
                )

        # ç©ºã®ãƒ†ã‚­ã‚¹ãƒˆã‚’é™¤å¤–
        df = df[df["Combined_Text"].str.strip() != ""]
        df = df.reset_index(drop=True)

        return df

    except Exception as e:
        logger.error(f"ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
        raise
