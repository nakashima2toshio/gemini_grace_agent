#!/usr/bin/env python
# -*- coding: utf-8 -*-
'''
qa_generation/evaluation.py - ã‚«ãƒãƒ¬ãƒƒã‚¸åˆ†æãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«
'''

import logging
import numpy as np
import tiktoken
from typing import List, Dict, Optional
from qa_generation.semantic import SemanticCoverage
from qa_generation.config import OPTIMAL_THRESHOLDS

logger = logging.getLogger(__name__)

def get_optimal_thresholds(dataset_type: str) -> Dict[str, float]:
    '''ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆåˆ¥ã®æœ€é©é–¾å€¤ã‚’å–å¾—
    Args:
        dataset_type: ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚¿ã‚¤ãƒ—
    Returns:
        é–¾å€¤è¾æ›¸ {strict, standard, lenient}
    '''
    return OPTIMAL_THRESHOLDS.get(dataset_type, {
        "strict": 0.8,
        "standard": 0.7,
        "lenient": 0.6
    })


def multi_threshold_coverage(coverage_matrix: np.ndarray, chunks: List[Dict],
                             qa_pairs: List[Dict], thresholds: Dict[str, float]) -> Dict:
    '''è¤‡æ•°é–¾å€¤ã§ã‚«ãƒãƒ¬ãƒ¼ã‚¸ã‚’è©•ä¾¡
    Args:
        coverage_matrix: ã‚«ãƒãƒ¬ãƒ¼ã‚¸è¡Œåˆ—
        chunks: ãƒãƒ£ãƒ³ã‚¯ãƒªã‚¹ãƒˆ
        qa_pairs: Q/Aãƒšã‚¢ãƒªã‚¹ãƒˆ
        thresholds: é–¾å€¤è¾æ›¸
    Returns:
        å¤šæ®µéšã‚«ãƒãƒ¬ãƒ¼ã‚¸çµæœ
    '''
    results = {}
    max_similarities = coverage_matrix.max(axis=1)

    for level, threshold in thresholds.items():
        covered = sum(1 for s in max_similarities if s >= threshold)
        uncovered_chunks = [
            {
                "chunk_id": chunks[i].get("id", f"chunk_{i}"),
                "similarity": float(max_similarities[i]),
                "gap": float(threshold - max_similarities[i])
            }
            for i, sim in enumerate(max_similarities)
            if sim < threshold
        ]

        results[level] = {
            "threshold": threshold,
            "covered_chunks": covered,
            "coverage_rate": covered / len(chunks) if chunks else 0,
            "uncovered_count": len(uncovered_chunks),
            "uncovered_chunks": uncovered_chunks
        }

    return results


def analyze_chunk_characteristics_coverage(chunks: List[Dict], coverage_matrix: np.ndarray,
                                          qa_pairs: List[Dict], threshold: float = 0.7) -> Dict:
    '''ãƒãƒ£ãƒ³ã‚¯ç‰¹æ€§åˆ¥ã®ã‚«ãƒãƒ¬ãƒ¼ã‚¸åˆ†æ
    Args:
        chunks: ãƒãƒ£ãƒ³ã‚¯ãƒªã‚¹ãƒˆ
        coverage_matrix: ã‚«ãƒãƒ¬ãƒ¼ã‚¸è¡Œåˆ—
        qa_pairs: Q/Aãƒšã‚¢ãƒªã‚¹ãƒˆ
        threshold: åˆ¤å®šé–¾å€¤
    Returns:
        ãƒãƒ£ãƒ³ã‚¯ç‰¹æ€§åˆ¥ã‚«ãƒãƒ¬ãƒ¼ã‚¸çµæœ
    '''
    tokenizer = tiktoken.get_encoding("cl100k_base")
    results = {
        "by_length": {},      # é•·ã•åˆ¥
        "by_position": {},    # ä½ç½®åˆ¥
        "summary": {}
    }

    # 1. é•·ã•åˆ¥åˆ†æ
    for i, chunk in enumerate(chunks):
        token_count = len(tokenizer.encode(chunk['text']))
        length_category = (
            "short" if token_count < 100 else
            "medium" if token_count < 200 else
            "long"
        )

        if length_category not in results["by_length"]:
            results["by_length"][length_category] = {
                "count": 0,
                "covered": 0,
                "avg_similarity": 0.0,
                "similarities": []
            }

        max_sim = coverage_matrix[i].max()
        results["by_length"][length_category]["count"] += 1
        results["by_length"][length_category]["similarities"].append(float(max_sim))

        if max_sim >= threshold:
            results["by_length"][length_category]["covered"] += 1

    # å¹³å‡é¡ä¼¼åº¦ã¨ã‚«ãƒãƒ¬ãƒ¼ã‚¸ç‡ã‚’è¨ˆç®—
    for length_cat in results["by_length"]:
        data = results["by_length"][length_cat]
        data["avg_similarity"] = float(np.mean(data["similarities"])) if data["similarities"] else 0.0
        data["coverage_rate"] = data["covered"] / data["count"] if data["count"] > 0 else 0.0
        del data["similarities"]

    # 2. ä½ç½®åˆ¥åˆ†æï¼ˆæ–‡æ›¸ã®å‰åŠ/ä¸­ç›¤/å¾ŒåŠï¼‰
    total_chunks = len(chunks)
    for i, chunk in enumerate(chunks):
        position = (
            "beginning" if i < total_chunks * 0.33 else
            "middle" if i < total_chunks * 0.67 else
            "end"
        )

        if position not in results["by_position"]:
            results["by_position"][position] = {
                "count": 0,
                "covered": 0,
                "avg_similarity": 0.0,
                "similarities": []
            }

        max_sim = coverage_matrix[i].max()
        results["by_position"][position]["count"] += 1
        results["by_position"][position]["similarities"].append(float(max_sim))

        if max_sim >= threshold:
            results["by_position"][position]["covered"] += 1

    # å¹³å‡é¡ä¼¼åº¦ã¨ã‚«ãƒãƒ¬ãƒ¼ã‚¸ç‡ã‚’è¨ˆç®—
    for position in results["by_position"]:
        data = results["by_position"][position]
        data["avg_similarity"] = float(np.mean(data["similarities"])) if data["similarities"] else 0.0
        data["coverage_rate"] = data["covered"] / data["count"] if data["count"] > 0 else 0.0
        del data["similarities"]

    # 3. ã‚µãƒãƒªãƒ¼æƒ…å ±
    results["summary"] = {
        "total_chunks": len(chunks),
        "total_qa_pairs": len(qa_pairs),
        "threshold_used": threshold,
        "insights": []
    }

    # ã‚¤ãƒ³ã‚µã‚¤ãƒˆç”Ÿæˆ
    for length_cat, data in results["by_length"].items():
        if data["coverage_rate"] < 0.7:
            results["summary"]["insights"].append(
                f"{length_cat}ãƒãƒ£ãƒ³ã‚¯ã®ã‚«ãƒãƒ¬ãƒ¼ã‚¸ãŒä½ã„ï¼ˆ{data['coverage_rate']:.1%}ï¼‰"
            )

    for position, data in results["by_position"].items():
        if data["coverage_rate"] < 0.7:
            results["summary"]["insights"].append(
                f"æ–‡æ›¸{position}éƒ¨åˆ†ã®ã‚«ãƒãƒ¬ãƒ¼ã‚¸ãŒä½ã„ï¼ˆ{data['coverage_rate']:.1%}ï¼‰"
            )

    return results


def analyze_coverage(chunks: List[Dict], qa_pairs: List[Dict], dataset_type: str = "wikipedia_ja",
                     custom_threshold: Optional[float] = None) -> Dict:
    '''ç”Ÿæˆã•ã‚ŒãŸQ/Aãƒšã‚¢ã®ã‚«ãƒãƒ¬ãƒ¼ã‚¸ã‚’åˆ†æï¼ˆå¤šæ®µéšã‚«ãƒãƒ¬ãƒ¼ã‚¸åˆ†æå¯¾å¿œï¼‰
    Args:
        chunks: ãƒãƒ£ãƒ³ã‚¯ãƒªã‚¹ãƒˆ
        qa_pairs: Q/Aãƒšã‚¢ãƒªã‚¹ãƒˆ
        dataset_type: ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚¿ã‚¤ãƒ—ï¼ˆé–¾å€¤è‡ªå‹•è¨­å®šã«ä½¿ç”¨ï¼‰
        custom_threshold: ã‚«ã‚¹ã‚¿ãƒ é–¾å€¤ï¼ˆæŒ‡å®šæ™‚ã¯ã“ã‚Œã‚’ä½¿ç”¨ï¼‰
    Returns:
        ã‚«ãƒãƒ¬ãƒ¼ã‚¸åˆ†æçµæœï¼ˆå¤šæ®µéšè©•ä¾¡ã€ãƒãƒ£ãƒ³ã‚¯ç‰¹æ€§åˆ†æã‚’å«ã‚€ï¼‰
    '''
    analyzer = SemanticCoverage()

    # åŸ‹ã‚è¾¼ã¿ç”Ÿæˆï¼ˆãƒãƒƒãƒAPIæœ€é©åŒ–ç‰ˆï¼‰
    logger.info("=" * 60)
    logger.info("ã‚«ãƒãƒ¬ãƒ¼ã‚¸åˆ†æ: åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«ç”Ÿæˆé–‹å§‹")
    logger.info("=" * 60)

    # ãƒãƒ£ãƒ³ã‚¯ã®åŸ‹ã‚è¾¼ã¿ç”Ÿæˆï¼ˆæ—¢å­˜ãƒ¡ã‚½ãƒƒãƒ‰ä½¿ç”¨ï¼‰
    logger.info(f"[Step 1/3] ãƒãƒ£ãƒ³ã‚¯åŸ‹ã‚è¾¼ã¿ç”Ÿæˆ: {len(chunks)}ä»¶")
    doc_embeddings = analyzer.generate_embeddings(chunks)
    logger.info(f"[Step 1/3] ãƒãƒ£ãƒ³ã‚¯åŸ‹ã‚è¾¼ã¿å®Œäº†: {len(doc_embeddings)}ä»¶")

    # Q&Aãƒšã‚¢ã®åŸ‹ã‚è¾¼ã¿ç”Ÿæˆï¼ˆãƒãƒƒãƒAPIä½¿ç”¨ã§é«˜é€ŸåŒ–ï¼‰
    qa_texts = [f"{qa['question']} {qa['answer']}" for qa in qa_pairs]
    logger.info(f"[Step 2/3] Q/Aãƒšã‚¢åŸ‹ã‚è¾¼ã¿ç”Ÿæˆ: {len(qa_texts)}ä»¶")
    qa_embeddings = analyzer.generate_embeddings_batch(qa_texts, batch_size=2048)
    logger.info(f"[Step 2/3] Q/Aãƒšã‚¢åŸ‹ã‚è¾¼ã¿å®Œäº†: {len(qa_embeddings)}ä»¶")

    if len(qa_embeddings) == 0:
        return {
            "coverage_rate": 0.0,
            "covered_chunks": 0,
            "total_chunks": len(chunks),
            "uncovered_chunks": chunks,
            "multi_threshold": {},
            "chunk_analysis": {}
        }

    # ã‚«ãƒãƒ¬ãƒ¼ã‚¸è¡Œåˆ—è¨ˆç®—
    logger.info("ã‚«ãƒãƒ¬ãƒ¼ã‚¸è¡Œåˆ—è¨ˆç®—ä¸­...")
    coverage_matrix = np.zeros((len(chunks), len(qa_pairs)))
    for i in range(len(doc_embeddings)):
        for j in range(len(qa_embeddings)):
            similarity = analyzer.cosine_similarity(doc_embeddings[i], qa_embeddings[j])
            coverage_matrix[i, j] = similarity

    # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆåˆ¥æœ€é©é–¾å€¤ã‚’å–å¾—
    thresholds = get_optimal_thresholds(dataset_type)

    # ã‚«ã‚¹ã‚¿ãƒ é–¾å€¤ãŒæŒ‡å®šã•ã‚Œã¦ã„ã‚‹å ´åˆã¯ä¸Šæ›¸ã
    if custom_threshold is not None:
        standard_threshold = custom_threshold
        logger.info(f"ã‚«ã‚¹ã‚¿ãƒ é–¾å€¤ã‚’ä½¿ç”¨: {custom_threshold}")
    else:
        standard_threshold = thresholds["standard"]

    # åŸºæœ¬ã‚«ãƒãƒ¬ãƒ¼ã‚¸ï¼ˆæ¨™æº–é–¾å€¤ï¼‰
    max_similarities = coverage_matrix.max(axis=1)
    covered_count = sum(1 for s in max_similarities if s >= standard_threshold)
    coverage_rate = covered_count / len(chunks) if chunks else 0

    # æœªã‚«ãƒãƒ¼ãƒãƒ£ãƒ³ã‚¯ã®ç‰¹å®š
    uncovered_chunks = []
    for i, (chunk, sim) in enumerate(zip(chunks, max_similarities)):
        if sim < standard_threshold:
            uncovered_chunks.append({
                'chunk': chunk,
                'similarity': float(sim),
                'gap': float(standard_threshold - sim)
            })

    # ææ¡ˆ1ã®æ©Ÿèƒ½: å¤šæ®µéšã‚«ãƒãƒ¬ãƒ¼ã‚¸åˆ†æ
    logger.info("å¤šæ®µéšã‚«ãƒãƒ¬ãƒ¼ã‚¸åˆ†æå®Ÿè¡Œä¸­...")
    multi_threshold_results = multi_threshold_coverage(coverage_matrix, chunks, qa_pairs, thresholds)

    # ææ¡ˆ1ã®æ©Ÿèƒ½: ãƒãƒ£ãƒ³ã‚¯ç‰¹æ€§åˆ¥åˆ†æ
    logger.info("ãƒãƒ£ãƒ³ã‚¯ç‰¹æ€§åˆ¥åˆ†æå®Ÿè¡Œä¸­...")
    chunk_characteristics = analyze_chunk_characteristics_coverage(
        chunks, coverage_matrix, qa_pairs, standard_threshold
    )

    # çµæœã‚’çµ±åˆ
    results = {
        # åŸºæœ¬ãƒ¡ãƒˆãƒªã‚¯ã‚¹
        "coverage_rate": coverage_rate,
        "covered_chunks": covered_count,
        "total_chunks": len(chunks),
        "uncovered_chunks": uncovered_chunks,
        "max_similarities": max_similarities.tolist(),
        "threshold": standard_threshold,

        # ææ¡ˆ1: å¤šæ®µéšã‚«ãƒãƒ¬ãƒ¼ã‚¸
        "multi_threshold": multi_threshold_results,

        # ææ¡ˆ1: ãƒãƒ£ãƒ³ã‚¯ç‰¹æ€§åˆ¥åˆ†æ
        "chunk_analysis": chunk_characteristics,

        # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæƒ…å ±
        "dataset_type": dataset_type,
        "optimal_thresholds": thresholds
    }

    # åˆ†æçµæœã®ã‚µãƒãƒªãƒ¼ã‚’ãƒ­ã‚°å‡ºåŠ›
    logger.info(
        f"\n    å¤šæ®µéšã‚«ãƒãƒ¬ãƒ¼ã‚¸åˆ†æçµæœ:"
        f"\n    - Strict  (é–¾å€¤{thresholds['strict']:.2f}): {multi_threshold_results['strict']['coverage_rate']:.1%}"
        f"\n    - Standard(é–¾å€¤{thresholds['standard']:.2f}): {multi_threshold_results['standard']['coverage_rate']:.1%}"
        f"\n    - Lenient (é–¾å€¤{thresholds['lenient']:.2f}): {multi_threshold_results['lenient']['coverage_rate']:.1%}"
        f"\n"
        f"\n    ãƒãƒ£ãƒ³ã‚¯ç‰¹æ€§åˆ¥ã‚«ãƒãƒ¬ãƒ¼ã‚¸:"
        f"\n    é•·ã•åˆ¥:"
        f"\n    - Short ãƒãƒ£ãƒ³ã‚¯: {chunk_characteristics['by_length'].get('short', {}).get('coverage_rate', 0):.1%}"
        f"\n    - Medium ãƒãƒ£ãƒ³ã‚¯: {chunk_characteristics['by_length'].get('medium', {}).get('coverage_rate', 0):.1%}"
        f"\n    - Long ãƒãƒ£ãƒ³ã‚¯: {chunk_characteristics['by_length'].get('long', {}).get('coverage_rate', 0):.1%}"
        f"\n"
        f"\n    ä½ç½®åˆ¥:"
        f"\n    - Beginning (å‰åŠ): {chunk_characteristics['by_position'].get('beginning', {}).get('coverage_rate', 0):.1%}"
        f"\n    - Middle (ä¸­ç›¤): {chunk_characteristics['by_position'].get('middle', {}).get('coverage_rate', 0):.1%}"
        f"\n    - End (å¾ŒåŠ): {chunk_characteristics['by_position'].get('end', {}).get('coverage_rate', 0):.1%}"
    )

    # ã‚¤ãƒ³ã‚µã‚¤ãƒˆãŒã‚ã‚‹å ´åˆã¯è¡¨ç¤º
    if chunk_characteristics['summary']['insights']:
        logger.info("\nğŸ“Š åˆ†æã‚¤ãƒ³ã‚µã‚¤ãƒˆ:")
        for insight in chunk_characteristics['summary']['insights']:
            logger.info(f"  â€¢ {insight}")

    return results