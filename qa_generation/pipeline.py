#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
qa_generation/pipeline.py - Q/Aç”Ÿæˆãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³åˆ¶å¾¡ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«
"""

import sys
import logging
from typing import List, Dict, Optional, Any
import pandas as pd

from config import DATASET_CONFIGS
from helper_llm import LLMClient
from qa_generation.config import LOCAL_DATASET_EXTENSIONS
from qa_generation.structure import create_document_chunks, merge_small_chunks
from qa_generation.generation import QAGenerator, generate_qa_dataset
from qa_generation.evaluation import analyze_coverage
from celery_tasks import submit_unified_qa_generation, collect_results, check_celery_workers

logger = logging.getLogger(__name__)

class QAPipeline:
    """Q/Aç”Ÿæˆãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³"""

    def __init__(self,
                 dataset_name: Optional[str] = None,
                 input_file: Optional[str] = None,
                 model: str = "gemini-2.0-flash",
                 output_dir: str = "qa_output/pipeline",
                 max_docs: Optional[int] = None,
                 client: Optional[LLMClient] = None):
        """
        Args:
            dataset_name: ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå (cc_news, wikipedia_ja, etc.)
            input_file: ãƒ­ãƒ¼ã‚«ãƒ«å…¥åŠ›ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
            model: ä½¿ç”¨ã™ã‚‹ãƒ¢ãƒ‡ãƒ«
            output_dir: å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
            max_docs: æœ€å¤§å‡¦ç†æ–‡æ›¸æ•°
            client: LLMã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆï¼ˆDIç”¨ï¼‰
        """
        self.dataset_name = dataset_name
        self.input_file = input_file
        self.model = model
        self.output_dir = output_dir
        self.max_docs = max_docs
        self.client = client  # Can be injected for testing or reuse

        self.config = self._load_config()

    def _load_config(self) -> Dict[str, Any]:
        """è¨­å®šã‚’ãƒ­ãƒ¼ãƒ‰"""
        if self.input_file:
            # ãƒ­ãƒ¼ã‚«ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ç”¨ã®å‹•çš„è¨­å®š
            from pathlib import Path
            file_basename = Path(self.input_file).stem
            lang = "ja" # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ
            return {
                "name": f"ãƒ­ãƒ¼ã‚«ãƒ«ãƒ•ã‚¡ã‚¤ãƒ« ({file_basename})",
                "text_column": "Combined_Text",
                "title_column": None,
                "lang": lang,
                "chunk_size": 300,
                "qa_per_chunk": 3,
                "type": "custom_upload"
            }
        elif self.dataset_name:
            if self.dataset_name not in DATASET_CONFIGS:
                raise ValueError(f"æœªå¯¾å¿œã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ: {self.dataset_name}")
            
            config = DATASET_CONFIGS[self.dataset_name].copy() # config.pyã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæƒ…å ±
            # a02ã®æ‹¡å¼µè¨­å®šã‚’ãƒãƒ¼ã‚¸ (qa_generation.config.LOCAL_DATASET_EXTENSIONS)
            if self.dataset_name in LOCAL_DATASET_EXTENSIONS:
                config.update(LOCAL_DATASET_EXTENSIONS[self.dataset_name])
            
            config["type"] = self.dataset_name
            return config
        else:
            raise ValueError("dataset_name ã¾ãŸã¯ input_file ã‚’æŒ‡å®šã—ã¦ãã ã•ã„")

        def load_data(self) -> pd.DataFrame:
            """ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã‚€"""
            # å¾ªç’°å‚ç…§ã‚’é¿ã‘ã‚‹ãŸã‚ã€ã“ã“ã§ã‚¤ãƒ³ãƒãƒ¼ãƒˆ (a02ã«æ®‹ã£ã¦ã„ã‚‹é–¢æ•°ã‚’ä½¿ç”¨...
            # å°†æ¥çš„ã«ã¯ qa_generation/data_io.py ã«ç§»å‹•ã™ã¹ã)
            from qa_generation.data_io import load_uploaded_file, load_preprocessed_data
            
            logger.info("\n[1/4] ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿...")        if self.input_file:
            df = load_uploaded_file(self.input_file)
            if self.max_docs and len(df) > self.max_docs:
                df = df.head(self.max_docs)
                logger.info(f"  ğŸ“Š æœ€å¤§æ–‡æ›¸æ•°åˆ¶é™: {len(df)} ä»¶ã«åˆ¶é™")
            return df
        else:
            return load_preprocessed_data(self.dataset_name)

    def create_chunks(self, df: pd.DataFrame) -> List[Dict]:
        """ãƒãƒ£ãƒ³ã‚¯ã‚’ä½œæˆã™ã‚‹"""
        logger.info("\n[2/4] ãƒãƒ£ãƒ³ã‚¯ä½œæˆ...")
        dataset_type = self.config.get("type", "unknown")
        # ãƒ­ãƒ¼ã‚«ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã®å ´åˆã€max_docsã¯èª­ã¿è¾¼ã¿æ™‚ã«é©ç”¨æ¸ˆã¿
        max_docs_for_chunks = None if self.input_file else self.max_docs
        
        chunks = create_document_chunks(df, dataset_type, max_docs_for_chunks, config=self.config)
        
        if not chunks:
            logger.error("ãƒãƒ£ãƒ³ã‚¯ãŒä½œæˆã•ã‚Œã¾ã›ã‚“ã§ã—ãŸ")
            # ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã¨ã—ã¦ã¯ã“ã“ã§ä¾‹å¤–ã‚’æŠ•ã’ã‚‹ã¹ã
            raise RuntimeError("Chunk creation failed")
            
        return chunks

    def generate_qa(self, chunks: List[Dict],
                    use_celery: bool = False,
                    celery_workers: int = 8,
                    batch_chunks: int = 3,
                    merge_chunks: bool = True,
                    min_tokens: int = 150,
                    max_tokens: int = 400) -> List[Dict]:
        """Q/Aãƒšã‚¢ã‚’ç”Ÿæˆã™ã‚‹"""
        logger.info("\n[3/4] Q/Aãƒšã‚¢ç”Ÿæˆ...")
        
        if use_celery:
            return self._generate_with_celery(
                chunks, celery_workers, batch_chunks, merge_chunks, min_tokens, max_tokens
            )
        else:
            return self._generate_sync(
                chunks, batch_chunks, merge_chunks, min_tokens, max_tokens
            )

    def _generate_with_celery(self, chunks: List[Dict], workers: int, batch_size: int,
                              merge: bool, min_tokens: int, max_tokens: int) -> List[Dict]:
        """Celeryã‚’ä½¿ç”¨ã—ãŸéåŒæœŸç”Ÿæˆ"""
        logger.info(f"Celeryä¸¦åˆ—å‡¦ç†ãƒ¢ãƒ¼ãƒ‰: ãƒ¯ãƒ¼ã‚«ãƒ¼æ•°={workers}")
        logger.info("Celeryãƒ¯ãƒ¼ã‚«ãƒ¼ã®çŠ¶æ…‹ã‚’ç¢ºèªä¸­...")
        if not check_celery_workers(workers):
            raise RuntimeError("Celery workers are not running")
            
        if merge:
            processed_chunks = merge_small_chunks(chunks, min_tokens, max_tokens)
        else:
            processed_chunks = chunks

        tasks = submit_unified_qa_generation(
            processed_chunks, self.config, self.model, provider="gemini"
        )

        timeout_seconds = min(max(len(tasks) * 10, 600), 1800)
        logger.info(f"çµæœåé›†ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ: {timeout_seconds}ç§’ï¼ˆ{len(tasks)}ã‚¿ã‚¹ã‚¯ï¼‰")
        return collect_results(tasks, timeout=timeout_seconds)

    def _generate_sync(self, chunks: List[Dict], batch_size: int,
                       merge: bool, min_tokens: int, max_tokens: int) -> List[Dict]:
        """åŒæœŸç”Ÿæˆ"""
        logger.info("é€šå¸¸å‡¦ç†ãƒ¢ãƒ¼ãƒ‰")
        dataset_type = self.config.get("type", "unknown")
        
        return generate_qa_dataset(
            chunks,
            dataset_type,
            self.model,
            chunk_batch_size=batch_size,
            merge_chunks=merge,
            min_tokens=min_tokens,
            max_tokens=max_tokens,
            config=self.config,
            client=self.client
        )

    def evaluate_coverage(self, chunks: List[Dict], qa_pairs: List[Dict], threshold: Optional[float] = None) -> Dict:
        """ã‚«ãƒãƒ¬ãƒƒã‚¸ã‚’è©•ä¾¡ã™ã‚‹"""
        logger.info("\n[4/4] ã‚«ãƒãƒ¬ãƒ¼ã‚¸åˆ†æ...")
        dataset_type = self.config.get("type", "unknown")
        return analyze_coverage(chunks, qa_pairs, dataset_type, custom_threshold=threshold)

    def save(self, qa_pairs: List[Dict], coverage_results: Dict) -> Dict[str, str]:
        """çµæœã‚’ä¿å­˜ã™ã‚‹"""
        # å¾ªç’°å‚ç…§å›é¿
        from qa_generation.data_io import save_results
        
        logger.info("\nçµæœã‚’ä¿å­˜ä¸­...")
        dataset_type = self.config.get("type", "unknown")
        return save_results(qa_pairs, coverage_results, dataset_type, self.output_dir)

    def run(
            self,
            use_celery: bool = False,
            celery_workers: int = 8,
            batch_chunks: int = 3,
            merge_chunks: bool = True,
            min_tokens: int = 150,
            max_tokens: int = 400,
            analyze_coverage: bool = True,
            coverage_threshold: Optional[float] = None):
        """ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å®Ÿè¡Œã®ã‚·ãƒ§ãƒ¼ãƒˆã‚«ãƒƒãƒˆ"""
        try:
            df = self.load_data()
            chunks = self.create_chunks(df)
            qa_pairs = self.generate_qa(
                chunks, use_celery, celery_workers, batch_chunks, merge_chunks, min_tokens, max_tokens
            )
            
            if not qa_pairs:
                logger.warning("Q/Aãƒšã‚¢ãŒç”Ÿæˆã•ã‚Œã¾ã›ã‚“ã§ã—ãŸ")
                
            coverage_results = {}
            if analyze_coverage and qa_pairs:
                coverage_results = self.evaluate_coverage(chunks, qa_pairs, coverage_threshold)
            else:
                coverage_results = {
                    "coverage_rate": 0,
                    "covered_chunks": 0,
                    "total_chunks": len(chunks),
                    "uncovered_chunks": []
                }
                
            saved_files = self.save(qa_pairs, coverage_results)
            
            return {
                "saved_files": saved_files,
                "qa_count": len(qa_pairs),
                "coverage_results": coverage_results,
                "success": True
            }
            
        except Exception as e:
            logger.error(f"ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {e}")
            raise
