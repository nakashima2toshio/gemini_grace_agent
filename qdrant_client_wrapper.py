#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
qdrant_client_wrapper.py - Qdrantæ“ä½œãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£
===================================================
Qdrantãƒ™ã‚¯ãƒˆãƒ«ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã¨ã®æ“ä½œã‚’ä¸€å…ƒç®¡ç†

ä½¿ç”¨ç®‡æ‰€:
- rag_qa_pair_qdrant.py
- a42_qdrant_registration.py
- a50_rag_search_local_qdrant.py
"""

import os
import logging
import socket
import time
import traceback
from typing import Dict, List, Optional, Any, Tuple, Iterable
from datetime import datetime, timezone

import pandas as pd
import tiktoken
from qdrant_client import QdrantClient
from qdrant_client.http import models
from qdrant_client.http.exceptions import UnexpectedResponse

# Gemini 3 Migration: EmbeddingæŠ½è±¡åŒ–ãƒ¬ã‚¤ãƒ¤ãƒ¼
from helper_embedding import (
    create_embedding_client,
    get_embedding_dimensions,
    EmbeddingClient,
    DEFAULT_GEMINI_EMBEDDING_DIMS,
    DEFAULT_OPENAI_EMBEDDING_DIMS,
)
from helper_embedding_sparse import get_sparse_embedding_client

# å…±é€šãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«
try:
    from config import QdrantConfig
except ImportError:
    # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯è¨­å®š
    class QdrantConfig:
        HOST = "localhost"
        PORT = 6333
        URL = "http://localhost:6333"
        DOCKER_IMAGE = "qdrant/qdrant"
        HEALTH_CHECK_ENDPOINT = "/collections"
        DEFAULT_TIMEOUT = 30
        DEFAULT_VECTOR_SIZE = 1536
        DEFAULT_EMBEDDING_MODEL = "text-embedding-3-small"

# ãƒ­ã‚°è¨­å®š
logger = logging.getLogger(__name__)


# ===================================================================
# å®šæ•°
# ===================================================================

# Qdrantè¨­å®š
QDRANT_CONFIG = {
    "name": "Qdrant",
    "host": QdrantConfig.HOST,
    "port": QdrantConfig.PORT,
    "icon": "ğŸ¯",
    "url": QdrantConfig.URL,
    "health_check_endpoint": QdrantConfig.HEALTH_CHECK_ENDPOINT,
    "docker_image": QdrantConfig.DOCKER_IMAGE,
}

# ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆåŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ«
DEFAULT_EMBEDDING_MODEL = QdrantConfig.DEFAULT_EMBEDDING_MODEL
DEFAULT_VECTOR_SIZE = QdrantConfig.DEFAULT_VECTOR_SIZE

# =====================================================
# Gemini 3 Migration: ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼è¨­å®š
# =====================================================
DEFAULT_EMBEDDING_PROVIDER = os.getenv("EMBEDDING_PROVIDER", "gemini")  # "gemini" or "openai"

# ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼åˆ¥ã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®š
PROVIDER_DEFAULTS = {
    "gemini": {
        "model": "gemini-embedding-001",
        "dims": DEFAULT_GEMINI_EMBEDDING_DIMS,  # 3072
    },
    "openai": {
        "model": "text-embedding-3-small",
        "dims": DEFAULT_OPENAI_EMBEDDING_DIMS,  # 1536
    },
    "fastembed": {
        "model": "BAAI/bge-small-en-v1.5",
        "dims": 384,
    },
}

# ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³å›ºæœ‰ã®åŸ‹ã‚è¾¼ã¿è¨­å®šï¼ˆãƒ¬ã‚¬ã‚·ãƒ¼: OpenAIç”¨ï¼‰
COLLECTION_EMBEDDINGS = {
    "qa_corpus": {"model": "text-embedding-3-small", "dims": 1536},
    "qa_cc_news_a02_llm": {"model": "text-embedding-3-small", "dims": 1536},
    "qa_cc_news_a03_rule": {"model": "text-embedding-3-small", "dims": 1536},
    "qa_cc_news_a10_hybrid": {"model": "text-embedding-3-small", "dims": 1536},
    "qa_livedoor_a02_20_llm": {"model": "text-embedding-3-small", "dims": 1536},
    "qa_livedoor_a03_rule": {"model": "text-embedding-3-small", "dims": 1536},
    "qa_livedoor_a10_hybrid": {"model": "text-embedding-3-small", "dims": 1536},
}

# Gemini 3å¯¾å¿œã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³è¨­å®šï¼ˆ3072æ¬¡å…ƒï¼‰
COLLECTION_EMBEDDINGS_GEMINI = {
    "qa_corpus_gemini": {"provider": "gemini", "model": "gemini-embedding-001", "dims": 3072},
    "qa_cc_news_gemini": {"provider": "gemini", "model": "gemini-embedding-001", "dims": 3072},
    "qa_livedoor_gemini": {"provider": "gemini", "model": "gemini-embedding-001", "dims": 3072},
}

# ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³åã¨CSVãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒãƒƒãƒ”ãƒ³ã‚°
COLLECTION_CSV_MAPPING = {
    "qa_cc_news_a02_llm": "a02_qa_pairs_cc_news.csv",
    "qa_cc_news_a03_rule": "a03_qa_pairs_cc_news.csv",
    "qa_cc_news_a10_hybrid": "a10_qa_pairs_cc_news.csv",
    "qa_livedoor_a02_20_llm": "a02_qa_pairs_livedoor.csv",
    "qa_livedoor_a03_rule": "a03_qa_pairs_livedoor.csv",
    "qa_livedoor_a10_hybrid": "a10_qa_pairs_livedoor.csv",
}


# ===================================================================
# ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£é–¢æ•°
# ===================================================================

def batched(seq: Iterable, size: int):
    """
    ã‚¤ãƒ†ãƒ©ãƒ–ãƒ«ã‚’ãƒãƒƒãƒã«åˆ†å‰²

    Args:
        seq: åˆ†å‰²å¯¾è±¡ã®ã‚¤ãƒ†ãƒ©ãƒ–ãƒ«
        size: ãƒãƒƒãƒã‚µã‚¤ã‚º

    Yields:
        ãƒãƒƒãƒãƒªã‚¹ãƒˆ
    """
    buf = []
    for x in seq:
        buf.append(x)
        if len(buf) >= size:
            yield buf
            buf = []
    if buf:
        yield buf


# ===================================================================
# Qdrantæ¥ç¶šãƒ»ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯
# ===================================================================

class QdrantHealthChecker:
    """Qdrantã‚µãƒ¼ãƒãƒ¼ã®æ¥ç¶šçŠ¶æ…‹ã‚’ãƒã‚§ãƒƒã‚¯"""

    def __init__(self, debug_mode: bool = False):
        self.debug_mode = debug_mode
        self.client = None

    def check_port(self, host: str, port: int, timeout: float = 2.0) -> bool:
        """
        ãƒãƒ¼ãƒˆãŒé–‹ã„ã¦ã„ã‚‹ã‹ãƒã‚§ãƒƒã‚¯

        Args:
            host: ãƒ›ã‚¹ãƒˆå
            port: ãƒãƒ¼ãƒˆç•ªå·
            timeout: ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆç§’æ•°

        Returns:
            ãƒãƒ¼ãƒˆãŒé–‹ã„ã¦ã„ã‚‹ã‹ã©ã†ã‹
        """
        try:
            sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
            sock.settimeout(timeout)
            result = sock.connect_ex((host, port))
            sock.close()
            return result == 0
        except Exception as e:
            if self.debug_mode:
                logger.error(f"Port check failed for {host}:{port}: {e}")
            return False

    def check_qdrant(self) -> Tuple[bool, str, Optional[Dict]]:
        """
        Qdrantæ¥ç¶šãƒã‚§ãƒƒã‚¯
        Returns:
            (æ¥ç¶šæˆåŠŸãƒ•ãƒ©ã‚°, ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸, ãƒ¡ãƒˆãƒªã‚¯ã‚¹)
        """
        start_time = time.time()

        # ãƒãƒ¼ãƒˆãƒã‚§ãƒƒã‚¯
        if not self.check_port(QDRANT_CONFIG["host"], QDRANT_CONFIG["port"]):
            return False, "Connection refused (port closed)", None

        try:
            self.client = QdrantClient(url=QDRANT_CONFIG["url"], timeout=5)

            # ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³å–å¾—
            collections = self.client.get_collections()

            metrics = {
                "collection_count": len(collections.collections),
                "collections": [c.name for c in collections.collections],
                "response_time_ms": round((time.time() - start_time) * 1000, 2),
            }

            return True, "Connected", metrics

        except Exception as e:
            error_msg = str(e)
            if self.debug_mode:
                error_msg = f"{error_msg}\n{traceback.format_exc()}"
            return False, error_msg, None

    def get_client(self) -> Optional[QdrantClient]:
        """æ¥ç¶šæ¸ˆã¿ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’å–å¾—"""
        return self.client


def create_qdrant_client(url: str = None, timeout: int = 30) -> QdrantClient:
    """
    Qdrantã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’ä½œæˆ
    Args:
        url: Qdrantã‚µãƒ¼ãƒãƒ¼URLï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: localhost:6333ï¼‰
        timeout: ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆç§’æ•°
    Returns:
        QdrantClientã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹
    """
    url = url or QDRANT_CONFIG["url"]
    return QdrantClient(url=url, timeout=timeout)


# ===================================================================
# ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ç®¡ç†
# ===================================================================

def get_collection_stats(
    client: QdrantClient, collection_name: str
) -> Optional[Dict[str, Any]]:
    """
    ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ã®çµ±è¨ˆæƒ…å ±ã‚’å–å¾—

    Args:
        client: Qdrantã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ
        collection_name: ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³å

    Returns:
        çµ±è¨ˆæƒ…å ±è¾æ›¸ï¼ˆå­˜åœ¨ã—ãªã„å ´åˆã¯Noneï¼‰
    """
    try:
        collection_info = client.get_collection(collection_name)
        total_points = collection_info.points_count

        # ãƒ™ã‚¯ãƒˆãƒ«è¨­å®šæƒ…å ±ã‚’å–å¾—
        vectors_config = collection_info.config.params.vectors
        vector_info = {}

        if isinstance(vectors_config, dict):
            # Named Vectors
            for name, config in vectors_config.items():
                vector_info[name] = {
                    "size": config.size,
                    "distance": str(config.distance),
                }
        elif hasattr(vectors_config, "size"):
            # Single Vector
            vector_info["default"] = {
                "size": vectors_config.size,
                "distance": str(vectors_config.distance),
            }

        return {
            "total_points": total_points,
            "vector_config": vector_info,
            "status": collection_info.status,
        }

    except UnexpectedResponse as e:
        if "doesn't exist" in str(e) or "not found" in str(e).lower():
            return None
        raise
    except Exception as e:
        logger.error(f"çµ±è¨ˆæƒ…å ±å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
        return None


def get_all_collections(client: QdrantClient) -> List[Dict[str, Any]]:
    """
    å…¨ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ã®æƒ…å ±ã‚’å–å¾—
    Args:
        client: Qdrantã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ
    Returns:
        ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³æƒ…å ±ã®ãƒªã‚¹ãƒˆ
    """
    collections = client.get_collections()
    collection_list = []

    for collection in collections.collections:
        try:
            info = client.get_collection(collection.name)
            collection_list.append(
                {
                    "name": collection.name,
                    "points_count": info.points_count,
                    "status": info.status,
                }
            )
        except Exception:
            collection_list.append(
                {"name": collection.name, "points_count": 0, "status": "unknown"}
            )

    return collection_list


def delete_all_collections(client: QdrantClient, excluded: List[str] = None) -> int:
    """
    å…¨ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ã‚’å‰Šé™¤

    Args:
        client: Qdrantã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ
        excluded: é™¤å¤–ã™ã‚‹ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³åã®ãƒªã‚¹ãƒˆ

    Returns:
        å‰Šé™¤ã•ã‚ŒãŸã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³æ•°
    """
    excluded = excluded or []
    collections = get_all_collections(client)

    if not collections:
        return 0

    to_delete = [c for c in collections if c["name"] not in excluded]

    if not to_delete:
        return 0

    deleted_count = 0

    for col in to_delete:
        try:
            client.delete_collection(collection_name=col["name"])
            deleted_count += 1
        except Exception as e:
            logger.error(f"ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³å‰Šé™¤ã‚¨ãƒ©ãƒ¼ {col['name']}: {e}")

    return deleted_count


def create_or_recreate_collection(
    client: QdrantClient,
    name: str,
    recreate: bool = False,
    vector_size: int = DEFAULT_VECTOR_SIZE,
    use_sparse: bool = False
):
    """
    ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ä½œæˆã¾ãŸã¯å†ä½œæˆ

    Args:
        client: Qdrantã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ
        name: ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³å
        recreate: å†ä½œæˆãƒ•ãƒ©ã‚°
        vector_size: ãƒ™ã‚¯ãƒˆãƒ«æ¬¡å…ƒæ•°
        use_sparse: Sparse Vector (Hybrid Search) ã‚’æœ‰åŠ¹ã«ã™ã‚‹ã‹
    """
    # Dense Vectorè¨­å®š
    vectors_config = models.VectorParams(
        size=vector_size, distance=models.Distance.COSINE
    )
    
    # Sparse Vectorè¨­å®š
    sparse_vectors_config = None
    if use_sparse:
        sparse_vectors_config = {
            "text-sparse": models.SparseVectorParams(
                index=models.SparseIndexParams(
                    on_disk=False, # ãƒ¡ãƒ¢ãƒªä¸Šã«ä¿æŒã—ã¦é«˜é€ŸåŒ–ï¼ˆå¤§è¦æ¨¡ãªã‚‰Trueï¼‰
                )
            )
        }

    if recreate:
        try:
            client.delete_collection(collection_name=name)
        except Exception:
            pass
        client.create_collection(
            collection_name=name, 
            vectors_config=vectors_config,
            sparse_vectors_config=sparse_vectors_config
        )
    else:
        try:
            client.get_collection(name)
        except Exception:
            client.create_collection(
                collection_name=name, 
                vectors_config=vectors_config,
                sparse_vectors_config=sparse_vectors_config
            )

    # ãƒšã‚¤ãƒ­ãƒ¼ãƒ‰ç´¢å¼•ã‚’ä½œæˆ
    try:
        client.create_payload_index(
            name, field_name="domain", field_schema=models.PayloadSchemaType.KEYWORD
        )
    except Exception:
        pass


# ===================================================================
# ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ãƒ»å¤‰æ›
# ===================================================================

def load_csv_for_qdrant(
    path: str, required=("question", "answer"), limit: int = 0
) -> pd.DataFrame:
    """
    CSVã‚’ãƒ­ãƒ¼ãƒ‰ï¼ˆQdrantç™»éŒ²ç”¨ï¼‰

    Args:
        path: CSVãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
        required: å¿…é ˆã‚«ãƒ©ãƒ 
        limit: è¡Œæ•°åˆ¶é™ï¼ˆ0=ç„¡åˆ¶é™ï¼‰

    Returns:
        DataFrame
    """
    if not os.path.exists(path):
        raise FileNotFoundError(f"CSV not found: {path}")
    df = pd.read_csv(path)

    # åˆ—åãƒãƒƒãƒ”ãƒ³ã‚°
    column_mappings = {
        "Question": "question",
        "Response": "answer",
        "Answer": "answer",
        "correct_answer": "answer",
    }
    df = df.rename(columns=column_mappings)

    for col in required:
        if col not in df.columns:
            raise ValueError(
                f"{path} ã«ã¯ '{col}' åˆ—ãŒå¿…è¦ã§ã™ï¼ˆåˆ—: {list(df.columns)}ï¼‰"
            )

    df = df.fillna("").drop_duplicates(subset=list(required)).reset_index(drop=True)

    if limit and limit > 0:
        df = df.head(limit).copy()

    return df


def build_inputs_for_embedding(df: pd.DataFrame, include_answer: bool) -> List[str]:
    """
    åŸ‹ã‚è¾¼ã¿ç”¨å…¥åŠ›ãƒ†ã‚­ã‚¹ãƒˆã‚’æ§‹ç¯‰

    Args:
        df: DataFrame
        include_answer: answerã‚’å«ã‚ã‚‹ã‹ã©ã†ã‹

    Returns:
        ãƒ†ã‚­ã‚¹ãƒˆã®ãƒªã‚¹ãƒˆ
    """
    if include_answer:
        return (df["question"].astype(str) + "\n" + df["answer"].astype(str)).tolist()
    return df["question"].astype(str).tolist()


# ===================================================================
# åŸ‹ã‚è¾¼ã¿ç”Ÿæˆ
# ===================================================================

def embed_texts(
    texts: List[str],
    model: str = DEFAULT_EMBEDDING_MODEL,
    batch_size: int = 128
) -> List[List[float]]:
    """
    ãƒ†ã‚­ã‚¹ãƒˆã‚’ãƒãƒƒãƒå‡¦ç†ã§Embeddingã«å¤‰æ›ï¼ˆGemini APIä½¿ç”¨ï¼‰

    Args:
        texts: ãƒ†ã‚­ã‚¹ãƒˆãƒªã‚¹ãƒˆ
        model: åŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ«åï¼ˆäº’æ›æ€§ã®ãŸã‚ä¿æŒã€Geminiã‚’ä½¿ç”¨ï¼‰
        batch_size: ãƒãƒƒãƒã‚µã‚¤ã‚º

    Returns:
        åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«ã®ãƒªã‚¹ãƒˆ
    """
    # Geminiçµ±åˆé–¢æ•°ã«å§”è­²
    return embed_texts_unified(texts, provider="gemini", batch_size=batch_size)


def embed_query(
    text: str,
    model: str = DEFAULT_EMBEDDING_MODEL,
    dims: Optional[int] = None
) -> List[float]:
    """
    ã‚¯ã‚¨ãƒªãƒ†ã‚­ã‚¹ãƒˆã‚’åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«ã«å¤‰æ›ï¼ˆGemini APIä½¿ç”¨ï¼‰

    Args:
        text: åŸ‹ã‚è¾¼ã‚€ãƒ†ã‚­ã‚¹ãƒˆ
        model: ä½¿ç”¨ã™ã‚‹åŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ«ï¼ˆäº’æ›æ€§ã®ãŸã‚ä¿æŒã€Geminiã‚’ä½¿ç”¨ï¼‰
        dims: ãƒ™ã‚¯ãƒˆãƒ«ã®æ¬¡å…ƒæ•°ï¼ˆGeminiã§ã¯3072æ¬¡å…ƒï¼‰

    Returns:
        åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«
    """
    # Geminiçµ±åˆé–¢æ•°ã«å§”è­²
    return embed_query_unified(text, provider="gemini")


# =====================================================
# Gemini 3 Migration: æŠ½è±¡åŒ–ãƒ¬ã‚¤ãƒ¤ãƒ¼ã‚’ä½¿ç”¨ã—ãŸåŸ‹ã‚è¾¼ã¿é–¢æ•°
# =====================================================

def embed_texts_unified(
    texts: List[str],
    provider: str = None,
    batch_size: int = 100
) -> List[List[float]]:
    """
    ãƒ†ã‚­ã‚¹ãƒˆã‚’Embeddingã«å¤‰æ›ï¼ˆãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼æŠ½è±¡åŒ–ç‰ˆï¼‰

    Gemini 3 Migrationå¯¾å¿œ: OpenAIã¨Geminiã®ä¸¡æ–¹ã«å¯¾å¿œ

    Args:
        texts: ãƒ†ã‚­ã‚¹ãƒˆãƒªã‚¹ãƒˆ
        provider: "gemini" or "openai"ï¼ˆNoneã®å ´åˆã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆï¼‰
        batch_size: ãƒãƒƒãƒã‚µã‚¤ã‚º

    Returns:
        åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«ã®ãƒªã‚¹ãƒˆï¼ˆGemini: 3072æ¬¡å…ƒ, OpenAI: 1536æ¬¡å…ƒï¼‰

    Example:
        # Gemini Embeddingï¼ˆ3072æ¬¡å…ƒï¼‰
        vectors = embed_texts_unified(texts, provider="gemini")

        # OpenAI Embeddingï¼ˆ1536æ¬¡å…ƒï¼‰
        vectors = embed_texts_unified(texts, provider="openai")
    """
    provider = provider or DEFAULT_EMBEDDING_PROVIDER
    embedding_client = create_embedding_client(provider=provider)

    # ç©ºæ–‡å­—åˆ—ãƒ»ç©ºç™½ã®ã¿ã®æ–‡å­—åˆ—ã‚’é™¤å¤–ã—ã¦å‡¦ç†
    valid_texts = []
    valid_indices = []
    for i, text in enumerate(texts):
        if text and text.strip():
            valid_texts.append(text)
            valid_indices.append(i)

    if not valid_texts:
        logger.warning("å…¨ã¦ã®ãƒ†ã‚­ã‚¹ãƒˆãŒç©ºæ–‡å­—åˆ—ã§ã™ã€‚ãƒ€ãƒŸãƒ¼ãƒ™ã‚¯ãƒˆãƒ«ã‚’è¿”ã—ã¾ã™ã€‚")
        dims = get_embedding_dimensions(provider)
        return [[0.0] * dims] * len(texts)

    # æŠ½è±¡åŒ–ãƒ¬ã‚¤ãƒ¤ãƒ¼ã‚’ä½¿ç”¨ã—ã¦Embeddingç”Ÿæˆ
    valid_vecs = embedding_client.embed_texts(valid_texts, batch_size=batch_size)

    # å…ƒã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã«åˆã‚ã›ã¦ãƒ™ã‚¯ãƒˆãƒ«ã‚’å†é…ç½®
    dims = embedding_client.dimensions
    vecs: List[List[float]] = []
    valid_vec_idx = 0
    for i in range(len(texts)):
        if i in valid_indices:
            vecs.append(valid_vecs[valid_vec_idx])
            valid_vec_idx += 1
        else:
            vecs.append([0.0] * dims)

    return vecs


def embed_query_unified(
    text: str,
    provider: str = None
) -> List[float]:
    """
    ã‚¯ã‚¨ãƒªãƒ†ã‚­ã‚¹ãƒˆã‚’åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«ã«å¤‰æ›ï¼ˆãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼æŠ½è±¡åŒ–ç‰ˆï¼‰

    Gemini 3 Migrationå¯¾å¿œ: OpenAIã¨Geminiã®ä¸¡æ–¹ã«å¯¾å¿œ

    Args:
        text: åŸ‹ã‚è¾¼ã‚€ãƒ†ã‚­ã‚¹ãƒˆ
        provider: "gemini" or "openai"ï¼ˆNoneã®å ´åˆã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆï¼‰

    Returns:
        åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«ï¼ˆGemini: 3072æ¬¡å…ƒ, OpenAI: 1536æ¬¡å…ƒï¼‰

    Example:
        # Gemini Embeddingï¼ˆ3072æ¬¡å…ƒï¼‰
        vector = embed_query_unified("æ¤œç´¢ã‚¯ã‚¨ãƒª", provider="gemini")
    """
    provider = provider or DEFAULT_EMBEDDING_PROVIDER
    embedding_client = create_embedding_client(provider=provider)
    return embedding_client.embed_text(text)


def embed_sparse_texts_unified(
    texts: List[str],
    model_name: str = None,
    batch_size: int = 4,
    progress_callback: Any = None
) -> List[models.SparseVector]:
    """
    ãƒ†ã‚­ã‚¹ãƒˆã‚’Sparse Embedding (ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒ™ã‚¯ãƒˆãƒ«) ã«å¤‰æ›

    Args:
        texts: ãƒ†ã‚­ã‚¹ãƒˆãƒªã‚¹ãƒˆ
        model_name: ä½¿ç”¨ã™ã‚‹Sparseãƒ¢ãƒ‡ãƒ«ï¼ˆNoneã®å ´åˆã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆï¼‰
        batch_size: ãƒãƒƒãƒã‚µã‚¤ã‚º
        progress_callback: é€²æ—ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯é–¢æ•° (current, total) -> None

    Returns:
        Qdrantç”¨SparseVectorã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã®ãƒªã‚¹ãƒˆ
    """
    sparse_client = get_sparse_embedding_client(model_name)
    
    # ç©ºæ–‡å­—åˆ—ãƒ»ç©ºç™½ã®ã¿ã®æ–‡å­—åˆ—ã‚’é™¤å¤–ã—ã¦å‡¦ç†
    valid_texts = []
    valid_indices = []
    for i, text in enumerate(texts):
        if text and text.strip():
            valid_texts.append(text)
            valid_indices.append(i)

    if not valid_texts:
        return [models.SparseVector(indices=[], values=[])] * len(texts)

    # Sparse Embeddingç”Ÿæˆ
    raw_sparse_vecs = sparse_client.embed_texts(
        valid_texts, 
        batch_size=batch_size,
        progress_callback=progress_callback
    )

    # Qdrantãƒ¢ãƒ‡ãƒ«ã«å¤‰æ›ã—ã¦å…ƒã®é †åºã«æˆ»ã™
    sparse_vecs: List[models.SparseVector] = []
    valid_vec_idx = 0
    
    for i in range(len(texts)):
        if i in valid_indices:
            raw = raw_sparse_vecs[valid_vec_idx]
            sparse_vecs.append(models.SparseVector(
                indices=raw["indices"],
                values=raw["values"]
            ))
            valid_vec_idx += 1
        else:
            sparse_vecs.append(models.SparseVector(indices=[], values=[]))

    return sparse_vecs


def embed_sparse_query_unified(
    text: str,
    model_name: str = None
) -> models.SparseVector:
    """
    ã‚¯ã‚¨ãƒªãƒ†ã‚­ã‚¹ãƒˆã‚’Sparse Embeddingã«å¤‰æ›

    Args:
        text: ã‚¯ã‚¨ãƒªãƒ†ã‚­ã‚¹ãƒˆ
        model_name: ä½¿ç”¨ã™ã‚‹Sparseãƒ¢ãƒ‡ãƒ«

    Returns:
        Qdrantç”¨SparseVector
    """
    sparse_client = get_sparse_embedding_client(model_name)
    raw = sparse_client.embed_text(text)
    return models.SparseVector(
        indices=raw["indices"],
        values=raw["values"]
    )


def create_collection_for_provider(
    client: QdrantClient,
    name: str,
    provider: str = None,
    recreate: bool = False,
    use_sparse: bool = False
):
    """
    ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã«å¿œã˜ãŸæ¬¡å…ƒæ•°ã§ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ã‚’ä½œæˆ

    Gemini 3 Migrationå¯¾å¿œ: æ¬¡å…ƒæ•°ã‚’è‡ªå‹•è¨­å®š

    Args:
        client: Qdrantã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ
        name: ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³å
        provider: "gemini" or "openai"
        recreate: å†ä½œæˆãƒ•ãƒ©ã‚°
        use_sparse: Hybrid Searchç”¨Sparse Vectorã‚’æœ‰åŠ¹åŒ–

    Example:
        # Geminiç”¨ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ï¼ˆ3072æ¬¡å…ƒ + Sparseï¼‰
        create_collection_for_provider(client, "qa_gemini", provider="gemini", use_sparse=True)
    """
    provider = provider or DEFAULT_EMBEDDING_PROVIDER
    vector_size = get_embedding_dimensions(provider)

    logger.info(f"Creating collection '{name}' with {vector_size} dimensions (provider: {provider}, sparse: {use_sparse})")

    create_or_recreate_collection(
        client=client,
        name=name,
        recreate=recreate,
        vector_size=vector_size,
        use_sparse=use_sparse
    )


def get_provider_vector_size(provider: str = None) -> int:
    """
    ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã«å¿œã˜ãŸãƒ™ã‚¯ãƒˆãƒ«æ¬¡å…ƒæ•°ã‚’å–å¾—

    Args:
        provider: "gemini" or "openai"

    Returns:
        æ¬¡å…ƒæ•°ï¼ˆGemini: 3072, OpenAI: 1536ï¼‰
    """
    provider = provider or DEFAULT_EMBEDDING_PROVIDER
    return get_embedding_dimensions(provider)


# ===================================================================
# ãƒã‚¤ãƒ³ãƒˆä½œæˆãƒ»ã‚¢ãƒƒãƒ—ã‚µãƒ¼ãƒˆ
# ===================================================================

def build_points(
    df: pd.DataFrame,
    vectors: List[List[float]],
    domain: str,
    source_file: str
) -> List[models.PointStruct]:
    """
    Qdrantãƒã‚¤ãƒ³ãƒˆã‚’æ§‹ç¯‰

    Args:
        df: DataFrame
        vectors: åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«
        domain: ãƒ‰ãƒ¡ã‚¤ãƒ³å
        source_file: ã‚½ãƒ¼ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«å

    Returns:
        PointStructã®ãƒªã‚¹ãƒˆ
    """
    n = len(df)
    if len(vectors) != n:
        raise ValueError(f"vectors length mismatch: df={n}, vecs={len(vectors)}")

    now_iso = datetime.now(timezone.utc).isoformat()
    points: List[models.PointStruct] = []

    for i, row in enumerate(df.itertuples(index=False)):
        payload = {
            "domain": domain,
            "question": getattr(row, "question"),
            "answer": getattr(row, "answer"),
            "source": os.path.basename(source_file),
            "created_at": now_iso,
            "schema": "qa:v1",
        }

        pid = abs(hash(f"{domain}-{source_file}-{i}")) & 0x7FFFFFFFFFFFFFFF
        points.append(models.PointStruct(id=pid, vector=vectors[i], payload=payload))

    return points


def upsert_points(
    client: QdrantClient,
    collection: str,
    points: List[models.PointStruct],
    batch_size: int = 128,
) -> int:
    """
    ãƒã‚¤ãƒ³ãƒˆã‚’Qdrantã«ã‚¢ãƒƒãƒ—ã‚µãƒ¼ãƒˆ

    Args:
        client: Qdrantã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ
        collection: ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³å
        points: ãƒã‚¤ãƒ³ãƒˆãƒªã‚¹ãƒˆ
        batch_size: ãƒãƒƒãƒã‚µã‚¤ã‚º

    Returns:
        ã‚¢ãƒƒãƒ—ã‚µãƒ¼ãƒˆã•ã‚ŒãŸãƒã‚¤ãƒ³ãƒˆæ•°
    """
    count = 0
    for chunk in batched(points, batch_size):
        client.upsert(collection_name=collection, points=chunk)
        count += len(chunk)
    return count


# ===================================================================
# ãƒ‡ãƒ¼ã‚¿å–å¾—
# ===================================================================

class QdrantDataFetcher:
    """Qdrantã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—"""

    def __init__(self, client: QdrantClient):
        self.client = client

    def fetch_collections(self) -> pd.DataFrame:
        """ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ä¸€è¦§ã‚’å–å¾—"""
        try:
            collections = self.client.get_collections()

            data = []
            for collection in collections.collections:
                try:
                    info = self.client.get_collection(collection.name)
                    data.append(
                        {
                            "Collection": collection.name,
                            "Vectors Count": info.vectors_count,
                            "Points Count": info.points_count,
                            "Indexed Vectors": info.indexed_vectors_count,
                            "Status": info.status,
                        }
                    )
                except Exception:
                    data.append(
                        {
                            "Collection": collection.name,
                            "Vectors Count": "N/A",
                            "Points Count": "N/A",
                            "Indexed Vectors": "N/A",
                            "Status": "Error",
                        }
                    )

            return (
                pd.DataFrame(data)
                if data
                else pd.DataFrame({"Info": ["No collections found"]})
            )

        except Exception as e:
            return pd.DataFrame({"Error": [str(e)]})

    def fetch_collection_points(
        self, collection_name: str, limit: int = 50
    ) -> pd.DataFrame:
        """ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ã®è©³ç´°ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—"""
        try:
            # ã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«ã‚’ä½¿ã£ã¦ãƒã‚¤ãƒ³ãƒˆã‚’å–å¾—
            points_result = self.client.scroll(
                collection_name=collection_name,
                limit=limit,
                with_payload=True,
                with_vectors=False,
            )

            points = points_result[0]  # scrollã¯ (points, next_offset) ã®ã‚¿ãƒ—ãƒ«ã‚’è¿”ã™

            if not points:
                return pd.DataFrame({"Info": ["No points found in collection"]})

            # ãƒã‚¤ãƒ³ãƒˆã‚’DataFrameã«å¤‰æ›
            data = []
            for point in points:
                row = {"ID": point.id}

                # payloadã®å„ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‚’åˆ—ã¨ã—ã¦è¿½åŠ 
                if point.payload:
                    for key, value in point.payload.items():
                        # é•·ã™ãã‚‹æ–‡å­—åˆ—ã¯åˆ‡ã‚Šè©°ã‚
                        if isinstance(value, str) and len(value) > 200:
                            row[key] = value[:200] + "..."
                        elif isinstance(value, (list, dict)):
                            row[key] = (
                                str(value)[:200] + "..."
                                if len(str(value)) > 200
                                else str(value)
                            )
                        else:
                            row[key] = value

                data.append(row)

            return pd.DataFrame(data)

        except Exception as e:
            return pd.DataFrame({"Error": [str(e)]})

    def fetch_collection_info(self, collection_name: str) -> Dict[str, Any]:
        """ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ã®è©³ç´°æƒ…å ±ã‚’å–å¾—"""
        try:
            collection_info = self.client.get_collection(collection_name)

            # configã®æ§‹é€ ã‚’å®‰å…¨ã«ã‚¢ã‚¯ã‚»ã‚¹
            vector_config = collection_info.config.params.vectors

            # vector_configã®å‹ã‚’åˆ¤å®šã—ã¦é©åˆ‡ã«å‡¦ç†
            if hasattr(vector_config, "size"):
                # å˜ä¸€ãƒ™ã‚¯ãƒˆãƒ«è¨­å®š
                vector_size = vector_config.size
                distance = vector_config.distance
            elif hasattr(vector_config, "__iter__"):
                # Named vectorsè¨­å®šã®å ´åˆ
                vector_sizes = {}
                distances = {}
                for name, config in (
                    vector_config.items() if isinstance(vector_config, dict) else []
                ):
                    vector_sizes[name] = (
                        config.size if hasattr(config, "size") else "N/A"
                    )
                    distances[name] = (
                        config.distance if hasattr(config, "distance") else "N/A"
                    )
                vector_size = vector_sizes if vector_sizes else "N/A"
                distance = distances if distances else "N/A"
            else:
                vector_size = "N/A"
                distance = "N/A"

            return {
                "vectors_count": collection_info.vectors_count,
                "points_count": collection_info.points_count,
                "indexed_vectors": collection_info.indexed_vectors_count,
                "status": collection_info.status,
                "config": {
                    "vector_size": vector_size,
                    "distance": distance,
                },
            }
        except Exception as e:
            return {"error": str(e)}

    def fetch_collection_source_info(
        self, collection_name: str, sample_size: int = 200
    ) -> Dict[str, Any]:
        """ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ã®ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹æƒ…å ±ã‚’å–å¾—"""
        try:
            collection_info = self.client.get_collection(collection_name)
            total_points = collection_info.points_count

            # ã‚µãƒ³ãƒ—ãƒ«ãƒã‚¤ãƒ³ãƒˆã‚’å–å¾—
            points_result = self.client.scroll(
                collection_name=collection_name,
                limit=min(sample_size, total_points),
                with_payload=True,
                with_vectors=False,
            )

            points = points_result[0]

            if not points:
                return {"total_points": total_points, "sources": {}, "sample_size": 0}

            # sourceã¨generation_methodã‚’é›†è¨ˆ
            source_stats = {}
            for point in points:
                if point.payload:
                    source = point.payload.get("source", "unknown")
                    method = point.payload.get("generation_method", "unknown")
                    domain = point.payload.get("domain", "unknown")

                    if source not in source_stats:
                        source_stats[source] = {
                            "sample_count": 0,
                            "method": method,
                            "domain": domain,
                        }
                    source_stats[source]["sample_count"] += 1

            # å…¨ä½“ã®ãƒ‡ãƒ¼ã‚¿æ•°ã‚’æ¨å®š
            sample_total = len(points)
            for source, stats in source_stats.items():
                ratio = stats["sample_count"] / sample_total
                stats["estimated_total"] = int(total_points * ratio)
                stats["percentage"] = ratio * 100

            return {
                "total_points": total_points,
                "sources": source_stats,
                "sample_size": sample_total,
            }

        except Exception as e:
            return {"error": str(e)}


# ===================================================================
# æ¤œç´¢
# ===================================================================

def search_collection(
    client: QdrantClient,
    collection_name: str,
    query_vector: List[float],
    sparse_vector: Optional[models.SparseVector] = None,
    limit: int = 5,
    hybrid_alpha: float = 0.5
) -> List[Dict[str, Any]]:
    """
    ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ã‚’æ¤œç´¢ï¼ˆDense ã¾ãŸã¯ Hybridï¼‰
    """
    logger.info(f"search_collection: collection='{collection_name}', query_vec_dim={len(query_vector)}, limit={limit}, sparse={sparse_vector is not None}")
    
    try:
        # ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ã®æƒ…å ±ã‚’å–å¾—ã—ã¦ã€åå‰ä»˜ããƒ™ã‚¯ãƒˆãƒ«ãŒå¿…è¦ã‹ç¢ºèª
        collection_info = client.get_collection(collection_name)
        vectors_config = collection_info.config.params.vectors
        
        # åå‰ä»˜ããƒ™ã‚¯ãƒˆãƒ«ï¼ˆè¾æ›¸å½¢å¼ï¼‰ã‹ã©ã†ã‹ã®åˆ¤å®š
        is_named_vector = isinstance(vectors_config, dict)
        dense_vector_name = "default" if is_named_vector else None

        if sparse_vector:
            # Hybrid Search (Dense + Sparse)
            prefetch = [
                models.Prefetch(
                    query=query_vector,
                    using=dense_vector_name or "", # åå‰ãªã—ã®å ´åˆã¯ç©ºæ–‡å­—
                    limit=limit * 2,
                ),
                models.Prefetch(
                    query=sparse_vector,
                    using="text-sparse",
                    limit=limit * 2,
                ),
            ]
            
            response = client.query_points(
                collection_name=collection_name,
                prefetch=prefetch,
                query=models.FusionQuery(
                    fusion=models.Fusion.RRF,
                ),
                limit=limit,
            )
            hits = response.points
            
        else:
            # Standard Dense Search
            # åå‰ä»˜ããƒ™ã‚¯ãƒˆãƒ«ã®å ´åˆã¯ models.NamedVector ã¾ãŸã¯ query(..., using=...) ã‚’ä½¿ç”¨
            if is_named_vector:
                response = client.query_points(
                    collection_name=collection_name,
                    query=query_vector,
                    using=dense_vector_name,
                    limit=limit
                )
            else:
                response = client.query_points(
                    collection_name=collection_name,
                    query=query_vector,
                    limit=limit
                )
            hits = response.points

    except Exception as e:
        logger.error(f"Search failed: {e}")
        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: æœ€ã‚‚ã‚·ãƒ³ãƒ—ãƒ«ãªæ¤œç´¢
        try:
            hits = client.search(
                collection_name=collection_name,
                query_vector=query_vector,
                limit=limit
            )
        except Exception as fallback_e:
             logger.error(f"Fallback search also failed: {fallback_e}")
             return []

    logger.info(f"search_collection: found {len(hits)} hits")
    
    results = []
    for h in hits:
        results.append({
            "score": h.score,
            "id": h.id,
            "payload": h.payload
        })

    return results

    logger.info(f"search_collection: found {len(hits)} hits")
    
    results = []
    for h in hits:
        results.append({
            "score": h.score,
            "id": h.id,
            "payload": h.payload
        })

    return results


# ===================================================================
# å¾Œæ–¹äº’æ›æ€§ã®ãŸã‚ã®ã‚¨ã‚¤ãƒªã‚¢ã‚¹
# ===================================================================

# æ—§é–¢æ•°åã§ã‚¤ãƒ³ãƒãƒ¼ãƒˆã—ã¦ã„ã‚‹å ´åˆã®äº’æ›æ€§ç¶­æŒ
embed_texts_for_qdrant = embed_texts
create_or_recreate_collection_for_qdrant = create_or_recreate_collection
build_points_for_qdrant = build_points
upsert_points_to_qdrant = upsert_points
embed_query_for_search = embed_query


# ===================================================================
# ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ
# ===================================================================

__all__ = [
    # å®šæ•°
    "QDRANT_CONFIG",
    "DEFAULT_EMBEDDING_MODEL",
    "DEFAULT_VECTOR_SIZE",
    "COLLECTION_EMBEDDINGS",
    "COLLECTION_CSV_MAPPING",

    # Gemini 3 Migration: ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼è¨­å®š
    "DEFAULT_EMBEDDING_PROVIDER",
    "PROVIDER_DEFAULTS",
    "COLLECTION_EMBEDDINGS_GEMINI",

    # ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£
    "batched",

    # ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆãƒ»ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯
    "QdrantHealthChecker",
    "create_qdrant_client",

    # ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ç®¡ç†
    "get_collection_stats",
    "get_all_collections",
    "delete_all_collections",
    "create_or_recreate_collection",

    # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
    "load_csv_for_qdrant",
    "build_inputs_for_embedding",

    # åŸ‹ã‚è¾¼ã¿ï¼ˆãƒ¬ã‚¬ã‚·ãƒ¼: OpenAIç”¨ï¼‰
    "embed_texts",
    "embed_query",

    # Gemini 3 Migration: åŸ‹ã‚è¾¼ã¿ï¼ˆæŠ½è±¡åŒ–ç‰ˆï¼‰
    "embed_texts_unified",
    "embed_query_unified",
    "create_collection_for_provider",
    "get_provider_vector_size",

    # ãƒã‚¤ãƒ³ãƒˆæ“ä½œ
    "build_points",
    "upsert_points",

    # ãƒ‡ãƒ¼ã‚¿å–å¾—
    "QdrantDataFetcher",

    # æ¤œç´¢
    "search_collection",

    # å¾Œæ–¹äº’æ›æ€§ã‚¨ã‚¤ãƒªã‚¢ã‚¹
    "embed_texts_for_qdrant",
    "create_or_recreate_collection_for_qdrant",
    "build_points_for_qdrant",
    "upsert_points_to_qdrant",
    "embed_query_for_search",
]