# Gemini vs OpenAI Embedding モデル比較調査

本ドキュメントでは、RAGシステムにおけるEmbedding（ベクトル化）モデルの選定基準として、OpenAIの `text-embedding-3-small`、'text-embedding-3-large' と Google Geminiの最新モデル `gemini-embedding-001` の比較調査結果をまとめる。

## 目次

- [1. 現状の構成と目的](#1-現状の構成と目的)
- [2. モデル比較表](#2-モデル比較表)
- [3. 詳細解説](#3-詳細解説)
  - [3.1 ベクトル化できる文章量（入力トークン制限）](#31-ベクトル化できる文章量入力トークン制限)
  - [3.2 バッチ処理（スピードとスループット）](#32-バッチ処理スピードとスループット)
  - [3.3 次元数とデータベース容量・精度](#33-次元数とデータベース容量精度)
  - [3.4 コストパフォーマンス](#34-コストパフォーマンス)
- [4. 結論と推奨](#4-結論と推奨)

---

## 1. 現状の構成と目的

現在、本プロジェクト（`gemini3_rag_qa`）では、以下の構成でEmbedding処理を実装している。

-   **使用モデル**: `gemini-embedding-001` (Google)
-   **設定**: デフォルトの **3072次元** を採用
-   **実装**: `helper_embedding.py` にてOpenAI互換のインターフェースでラップし、Qdrantへの格納を行っている。

本調査は、この構成の妥当性を再確認し、OpenAIモデルとの差異を明確にすることを目的とする。

---

## 2. モデル比較表

### 2.1 スタンダードモデル比較 (Small vs Gemini)

最新のモデル仕様（2025年時点）に基づく比較結果は以下の通りである。

| 比較項目 | OpenAI<br>(text-embedding-3-small) | Gemini<br>(gemini-embedding-001) | 備考・勝者 |
| :--- | :--- | :--- | :--- |
| **最大入力トークン数** | **8,191 tokens** | 2,048 tokens | **OpenAI**<br>OpenAIは約4倍長いテキストを一度に処理可能。 |
| **一度のバッチ上限数** | **2,048 件** (配列) | 100 件 (batchEmbedContents) | **OpenAI**<br>Geminiは小分けにしてリクエストする必要がある。 |
| **出力次元数** | 1536 (短縮可能) | **デフォルト 3072**<br>(MRLにより 768, 1536 へ短縮可能) | **Gemini**<br>より高次元で詳細な表現が可能。MRLにより柔軟性も高い。 |
| **コスト (Paid)** | **$0.02 / 1M tokens** | ~$0.025 / 1M **chars**<br>(Vertex AI) | **OpenAI**<br>1トークン≒4文字換算でOpenAIが安価。<br>※AI Studioなら無料枠あり。 |
| **日本語性能 (MTEB)** | 高い | **非常に高い** | **Gemini**<br>Googleの最新モデルとして多言語検索タスクで非常に優秀。 |
| **API速度 (レイテンシ)** | 非常に高速・安定 | 高速 | **OpenAI**<br>バッチ処理効率も含めるとOpenAIがスループットを出しやすい。 |

### 2.2 ハイエンドモデル比較 (Large vs Gemini)

両社の最上位（高精度）設定における比較。

| 比較項目 | OpenAI<br>(text-embedding-3-large) | Gemini<br>(gemini-embedding-001) | 備考・勝者 |
| :--- | :--- | :--- | :--- |
| **出力次元数** | **デフォルト 3072**<br>(256, 1024等へ短縮可能) | **デフォルト 3072**<br>(768, 1536等へ短縮可能) | **引き分け**<br>両者ともデフォルト3072次元の高次元ベクトルを提供し、MRLによる短縮に対応。 |
| **コスト (Paid)** | **$0.13 / 1M tokens** | ~$0.025 / 1M **chars**<br>(Vertex AI)<br>~$0.0375 / 1M **chars**<br>(Gemini API) | **Gemini**<br>OpenAIのLargeモデルはSmallの約6.5倍の価格。<br>Geminiはモデルサイズに関わらず一律料金（文字数ベース）のため、高次元利用ならGeminiが圧倒的に有利。 |
| **精度 (Retrieval)** | 非常に高い | 非常に高い | **Gemini (日本語)**<br>日本語を含む多言語タスクではGeminiが優勢な傾向。<br>英語タスクではOpenAIも極めて強力。 |
| **運用特性** | バッチ2048件 / 入力8191トークン | バッチ100件 / 入力2048トークン | **OpenAI**<br>運用面（バッチ処理・長文対応）での使い勝手はOpenAIが変わらず優位。 |

---

## 3. 詳細解説

### 3.1 ベクトル化できる文章量（入力トークン制限）

-   **OpenAI (Small/Large共通)**
    -   **8191 tokens** という非常に大きな入力ウィンドウを持つ。論文、契約書、長文の技術ドキュメントなどを、チャンク分割を最小限に抑えてベクトル化できる。
-   **Gemini (gemini-embedding-001)**
    -   **2048 tokens** が上限。一般的なRAGのチャンク（500〜1000文字程度）には十分だが、システム設計時には必ずトークン数チェックと適切な分割ロジック（`SemanticCoverage`等）を組み込む必要がある。

### 3.2 バッチ処理（スピードとスループット）

-   **OpenAI**
    -   **2048件** の一括リクエストが可能。初期データロード時や大量データの更新時に、API呼び出しのオーバーヘッドを劇的に削減できる。
-   **Gemini**
    -   `batchEmbedContents` の上限は **100件**。
    -   大量データを扱う場合、Python等のクライアント側で `chunks(data, 100)` のようにデータを分割し、ループ処理でAPIを叩く実装が必須となる。

### 3.3 次元数とデータベース容量・精度

-   **OpenAI vs Gemini (高次元対決)**
    -   `text-embedding-3-large` と `gemini-embedding-001` は、共にデフォルト **3072次元** を採用している。
    -   **MRL (Matryoshka Representation Learning)**: 両モデルともこの技術に対応しており、ベクトルの先頭部分だけを切り出して使用しても性能劣化が少ない。これにより、「保存は小さく（256/768次元）、必要に応じてフルサイズで再評価」といった柔軟な運用が可能。

### 3.4 コストパフォーマンス（決定的な差）

-   **Small利用時**: OpenAIの `text-embedding-3-small` が圧倒的に安価。
-   **Large利用時 (高精度ニーズ)**: **Geminiが圧倒的に有利**。
    -   OpenAIの `text-embedding-3-large` は、同社Smallモデルの約6.5倍の価格設定となっている。
    -   一方、Gemini (Vertex AI) はモデルサイズや出力次元数に関わらず一律の文字数課金であるため、3072次元の高精度ベクトルを利用する場合のコストパフォーマンスはGeminiが非常に高い。
    -   さらに、**Google AI Studio** を利用すれば、開発や小規模運用では無料枠内でこの高性能モデルを利用できる。

---

## 4. 結論と推奨

本プロジェクトにおける推奨方針は以下の通りである。

1.  **Gemini (`gemini-embedding-001`) の継続利用（推奨）**
    -   **理由1（性能）**: デフォルト3072次元の高次元ベクトルにより、日本語検索タスクにおいてOpenAIのLargeモデルと同等以上の性能が期待できる。
    -   **理由2（コスト）**: 高次元（3072次元）を利用する場合、OpenAIのLargeモデルと比較して圧倒的なコストメリットがある（特にVertex AIやAI Studio利用時）。
    -   **理由3（柔軟性）**: 将来的にコストや速度を優先したい場合でも、モデルを変更することなくMRL機能で次元数だけを削減（768/1536等）できる。

2.  **実装上の留意点**
    -   Geminiの弱点である「バッチサイズ（100件）」と「入力トークン制限（2048）」をカバーするため、`helper_embedding.py` での実装（チャンク分割とバッチループ）を維持・管理すること。

3.  **OpenAI Largeモデルの利用シーン**
    -   「8000トークンを超える超長文を一度にベクトル化したい」という特殊な要件がない限り、現時点での積極的な採用理由は薄い。コストと性能のバランスでGeminiが勝る。
