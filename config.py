#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
config.py - è¨­å®šãƒ»å®šæ•°ã®ä¸€å…ƒç®¡ç†
================================
ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆå…¨ä½“ã®è¨­å®šã¨å®šæ•°ã‚’ä¸€å…ƒç®¡ç†

ä½¿ç”¨ç®‡æ‰€:
- rag_qa_pair_qdrant.py
- celery_tasks.py
- a02_make_qa_para.py
- helper_rag.py
- helper_api.py
"""

from typing import Dict, List, Any, Optional, Type
from dataclasses import dataclass
from pathlib import Path


# ===================================================================
# ãƒ¢ãƒ‡ãƒ«è¨­å®š
# ===================================================================

class ModelConfig:
    """Gemini ãƒ¢ãƒ‡ãƒ«è¨­å®šï¼ˆGemini APIå­¦ç¿’ç”¨ï¼‰"""

    # åˆ©ç”¨å¯èƒ½ãªãƒ¢ãƒ‡ãƒ«ä¸€è¦§
    AVAILABLE_MODELS: List[str] = [
        "gemini-3-pro-preview",       # æœ€æ–°Proï¼ˆæ€è€ƒãƒ¢ãƒ¼ãƒ‰å¯¾å¿œï¼‰
        "gemini-2.5-flash-preview",   # é«˜é€Ÿãƒ»æ€è€ƒå¯¾å¿œ
        "gemini-2.0-flash",           # å®‰å®šç‰ˆï¼ˆæ¨å¥¨ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆï¼‰
        "gemini-1.5-pro",             # ãƒ¬ã‚¬ã‚·ãƒ¼
        "gemini-1.5-flash",           # ãƒ¬ã‚¬ã‚·ãƒ¼é«˜é€Ÿ
    ]

    # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ¢ãƒ‡ãƒ«
    DEFAULT_MODEL: str = "gemini-2.0-flash"

    # temperatureãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ã‚µãƒãƒ¼ãƒˆã—ãªã„ãƒ¢ãƒ‡ãƒ«
    # Geminiã§ã¯å…¨ãƒ¢ãƒ‡ãƒ«ã§temperatureãŒã‚µãƒãƒ¼ãƒˆã•ã‚Œã‚‹
    NO_TEMPERATURE_MODELS: List[str] = []

    # ãƒ¢ãƒ‡ãƒ«æ–™é‡‘ï¼ˆ$/1M tokensï¼‰
    MODEL_PRICING: Dict[str, Dict[str, float]] = {
        "gemini-3-pro-preview": {"input": 0.00125, "output": 0.010},
        "gemini-2.5-flash-preview": {"input": 0.00015, "output": 0.0035},
        "gemini-2.0-flash": {"input": 0.0001, "output": 0.0004},
        "gemini-1.5-pro": {"input": 0.00125, "output": 0.005},
        "gemini-1.5-flash": {"input": 0.000075, "output": 0.0003},
    }

    # ãƒ¢ãƒ‡ãƒ«åˆ¶é™
    MODEL_LIMITS: Dict[str, Dict[str, int]] = {
        "gemini-3-pro-preview": {"max_tokens": 1000000, "max_output": 64000},
        "gemini-2.5-flash-preview": {"max_tokens": 1000000, "max_output": 64000},
        "gemini-2.0-flash": {"max_tokens": 1000000, "max_output": 8192},
        "gemini-1.5-pro": {"max_tokens": 1000000, "max_output": 8192},
        "gemini-1.5-flash": {"max_tokens": 1000000, "max_output": 8192},
    }

    @classmethod
    def supports_temperature(cls, model: str) -> bool:
        """ãƒ¢ãƒ‡ãƒ«ãŒtemperatureãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ã‚µãƒãƒ¼ãƒˆã™ã‚‹ã‹ãƒã‚§ãƒƒã‚¯"""
        return model not in cls.NO_TEMPERATURE_MODELS

    @classmethod
    def get_model_limits(cls, model: str) -> Dict[str, int]:
        """ãƒ¢ãƒ‡ãƒ«ã®åˆ¶é™ã‚’å–å¾—"""
        return cls.MODEL_LIMITS.get(model, {"max_tokens": 128000, "max_output": 4096})

    @classmethod
    def get_model_pricing(cls, model: str) -> Dict[str, float]:
        """ãƒ¢ãƒ‡ãƒ«ã®æ–™é‡‘ã‚’å–å¾—"""
        return cls.MODEL_PRICING.get(model, {"input": 0.00015, "output": 0.0006})

    @classmethod
    def uses_max_completion_tokens(cls, model: str) -> bool:
        """max_completion_tokensã‚’ä½¿ç”¨ã™ã‚‹ãƒ¢ãƒ‡ãƒ«ã‹ã©ã†ã‹"""
        # Geminiã§ã¯å…¨ãƒ¢ãƒ‡ãƒ«ã§max_output_tokensã‚’ä½¿ç”¨
        return False


# ===================================================================
# ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆè¨­å®š
# ===================================================================

@dataclass
class DatasetInfo:
    """ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæƒ…å ±"""
    name: str
    icon: str
    description: str
    file: Optional[str] = None
    hf_dataset: Optional[str] = None
    hf_config: Optional[str] = None
    download_url: Optional[str] = None
    split: Optional[str] = "train"
    text_field: str = "text"
    title_field: Optional[str] = None
    text_column: Optional[str] = None  # a02_make_qa_paraç”¨
    sample_size: int = 1000
    min_text_length: int = 100
    chunk_size: int = 300
    qa_per_chunk: int = 3
    lang: str = "ja"


class DatasetConfig:
    """ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆè¨­å®š"""

    # HuggingFace/ãƒ­ãƒ¼ã‚«ãƒ«ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆè¨­å®š
    DATASETS: Dict[str, DatasetInfo] = {
        "wikipedia_ja": DatasetInfo(
            name="Wikipediaæ—¥æœ¬èªç‰ˆ",
            icon="ğŸ“š",
            description="Wikipediaæ—¥æœ¬èªç‰ˆã®è¨˜äº‹ãƒ‡ãƒ¼ã‚¿ï¼ˆç™¾ç§‘äº‹å…¸çš„çŸ¥è­˜ï¼‰",
            hf_dataset="wikimedia/wikipedia",
            hf_config="20231101.ja",
            text_field="text",
            title_field="title",
            text_column="Combined_Text",
            file="OUTPUT/preprocessed_wikipedia_ja.csv",
            sample_size=1000,
            min_text_length=200,
            chunk_size=250,
            qa_per_chunk=3,
            lang="ja",
        ),
        "japanese_text": DatasetInfo(
            name="æ—¥æœ¬èªWebãƒ†ã‚­ã‚¹ãƒˆï¼ˆCC100ï¼‰",
            icon="ğŸ“°",
            description="æ—¥æœ¬èªWebãƒ†ã‚­ã‚¹ãƒˆã‚³ãƒ¼ãƒ‘ã‚¹",
            hf_dataset="range3/cc100-ja",
            text_field="text",
            text_column="Combined_Text",
            file="OUTPUT/preprocessed_japanese_text.csv",
            sample_size=1000,
            min_text_length=10,
            chunk_size=200,
            qa_per_chunk=2,
            lang="ja",
        ),
        "cc_news": DatasetInfo(
            name="CC-Newsï¼ˆè‹±èªãƒ‹ãƒ¥ãƒ¼ã‚¹ï¼‰",
            icon="ğŸŒ",
            description="Common Crawlè‹±èªãƒ‹ãƒ¥ãƒ¼ã‚¹è¨˜äº‹",
            hf_dataset="cc_news",
            text_field="text",
            title_field="title",
            text_column="Combined_Text",
            file="OUTPUT/preprocessed_cc_news.csv",
            sample_size=500,
            min_text_length=100,
            chunk_size=300,
            qa_per_chunk=5,
            lang="en",
        ),
        "livedoor": DatasetInfo(
            name="Livedoorãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚³ãƒ¼ãƒ‘ã‚¹",
            icon="ğŸ“°",
            description="Livedoorãƒ‹ãƒ¥ãƒ¼ã‚¹æ—¥æœ¬èªè¨˜äº‹ï¼ˆ9ã‚«ãƒ†ã‚´ãƒªã€å…¨7,376ä»¶ï¼‰",
            download_url="https://www.rondhuit.com/download/ldcc-20140209.tar.gz",
            text_field="content",
            title_field="title",
            text_column="Combined_Text",
            file="OUTPUT/preprocessed_livedoor.csv",
            split=None,
            sample_size=7376,
            min_text_length=100,
            chunk_size=200,
            qa_per_chunk=4,
            lang="ja",
        ),
    }

    # RAGç”¨ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆè¨­å®š (helper_rag.pyäº’æ›)
    RAG_DATASETS: Dict[str, Dict[str, Any]] = {
        "customer_support_faq": {
            "name": "ã‚«ã‚¹ã‚¿ãƒãƒ¼ã‚µãƒãƒ¼ãƒˆãƒ»FAQ",
            "icon": "ğŸ’¬",
            "required_columns": ["question", "answer"],
            "description": "ã‚«ã‚¹ã‚¿ãƒãƒ¼ã‚µãƒãƒ¼ãƒˆFAQãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ",
            "combine_template": "{question} {answer}",
            "port": 8501
        },
        "medical_qa": {
            "name": "åŒ»ç™‚QAãƒ‡ãƒ¼ã‚¿",
            "icon": "ğŸ¥",
            "required_columns": ["Question", "Complex_CoT", "Response"],
            "description": "åŒ»ç™‚è³ªå•å›ç­”ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ",
            "combine_template": "{question} {complex_cot} {response}",
            "port": 8503
        },
        "sciq_qa": {
            "name": "ç§‘å­¦ãƒ»æŠ€è¡“QAï¼ˆSciQï¼‰",
            "icon": "ğŸ”¬",
            "required_columns": ["question", "correct_answer"],
            "description": "ç§‘å­¦ãƒ»æŠ€è¡“è³ªå•å›ç­”ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ",
            "combine_template": "{question} {correct_answer}",
            "port": 8504
        },
        "legal_qa": {
            "name": "æ³•å¾‹ãƒ»åˆ¤ä¾‹QA",
            "icon": "âš–ï¸",
            "required_columns": ["question", "answer"],
            "description": "æ³•å¾‹ãƒ»åˆ¤ä¾‹è³ªå•å›ç­”ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ",
            "combine_template": "{question} {answer}",
            "port": 8505
        },
        "trivia_qa": {
            "name": "é›‘å­¦QAï¼ˆTriviaQAï¼‰",
            "icon": "ğŸ¯",
            "required_columns": ["question", "answer"],
            "description": "é›‘å­¦è³ªå•å›ç­”ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ",
            "combine_template": "{question} {answer} {entity_pages} {search_results}",
            "port": 8506
        }
    }

    @classmethod
    def get_dataset(cls, dataset_type: str) -> Optional[DatasetInfo]:
        """ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæƒ…å ±ã‚’å–å¾—"""
        return cls.DATASETS.get(dataset_type)

    @classmethod
    def get_dataset_dict(cls, dataset_type: str) -> Dict[str, Any]:
        """ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæƒ…å ±ã‚’è¾æ›¸å½¢å¼ã§å–å¾—ï¼ˆå¾Œæ–¹äº’æ›æ€§ç”¨ï¼‰"""
        info = cls.DATASETS.get(dataset_type)
        if info is None:
            return {}

        return {
            "name": info.name,
            "icon": info.icon,
            "description": info.description,
            "file": info.file,
            "hf_dataset": info.hf_dataset,
            "hf_config": info.hf_config,
            "download_url": info.download_url,
            "split": info.split,
            "text_field": info.text_field,
            "title_field": info.title_field,
            "text_column": info.text_column,
            "sample_size": info.sample_size,
            "min_text_length": info.min_text_length,
            "chunk_size": info.chunk_size,
            "qa_per_chunk": info.qa_per_chunk,
            "lang": info.lang,
        }

    @classmethod
    def get_all_dataset_names(cls) -> List[str]:
        """å…¨ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆåã®ãƒªã‚¹ãƒˆã‚’å–å¾—"""
        return list(cls.DATASETS.keys())

    @classmethod
    def get_rag_config(cls, dataset_type: str) -> Dict[str, Any]:
        """RAGãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆè¨­å®šã‚’å–å¾—"""
        return cls.RAG_DATASETS.get(dataset_type, {
            "name": "æœªçŸ¥ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ",
            "icon": "â“",
            "required_columns": [],
            "description": "æœªçŸ¥ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ",
            "combine_template": "{}",
            "port": 8500
        })


# ===================================================================
# Q/Aç”Ÿæˆè¨­å®š
# ===================================================================

class QAGenerationConfig:
    """Q/Aç”Ÿæˆè¨­å®š"""

    # è³ªå•ã‚¿ã‚¤ãƒ—éšå±¤æ§‹é€ 
    QUESTION_TYPES_HIERARCHY: Dict[str, Dict[str, str]] = {
        "basic": {
            "definition": "å®šç¾©å‹ï¼ˆã€œã¨ã¯ä½•ã§ã™ã‹ï¼Ÿï¼‰",
            "identification": "è­˜åˆ¥å‹ï¼ˆã€œã®ä¾‹ã‚’æŒ™ã’ã¦ãã ã•ã„ï¼‰",
            "enumeration": "åˆ—æŒ™å‹ï¼ˆã€œã®ç¨®é¡/è¦ç´ ã¯ï¼Ÿï¼‰"
        },
        "understanding": {
            "cause_effect": "å› æœé–¢ä¿‚å‹ï¼ˆã€œã®çµæœ/å½±éŸ¿ã¯ï¼Ÿï¼‰",
            "process": "ãƒ—ãƒ­ã‚»ã‚¹å‹ï¼ˆã€œã¯ã©ã®ã‚ˆã†ã«è¡Œã‚ã‚Œã¾ã™ã‹ï¼Ÿï¼‰",
            "mechanism": "ãƒ¡ã‚«ãƒ‹ã‚ºãƒ å‹ï¼ˆã€œã®ä»•çµ„ã¿ã¯ï¼Ÿï¼‰",
            "comparison": "æ¯”è¼ƒå‹ï¼ˆã€œã¨ã€œã®é•ã„ã¯ï¼Ÿï¼‰"
        },
        "application": {
            "synthesis": "çµ±åˆå‹ï¼ˆã€œã‚’çµ„ã¿åˆã‚ã›ã‚‹ã¨ã©ã†ãªã‚Šã¾ã™ã‹ï¼Ÿï¼‰",
            "evaluation": "è©•ä¾¡å‹ï¼ˆã€œã®é•·æ‰€ã¨çŸ­æ‰€ã¯ï¼Ÿï¼‰",
            "prediction": "äºˆæ¸¬å‹ï¼ˆã€œã®å ´åˆã©ã†ãªã‚Šã¾ã™ã‹ï¼Ÿï¼‰",
            "practical": "å®Ÿè·µå‹ï¼ˆã€œã¯ã©ã®ã‚ˆã†ã«æ´»ç”¨ã•ã‚Œã¾ã™ã‹ï¼Ÿï¼‰"
        }
    }

    # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®ã‚«ãƒãƒ¬ãƒ¼ã‚¸é–¾å€¤
    DEFAULT_COVERAGE_THRESHOLD: float = 0.58

    # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®ãƒãƒƒãƒã‚µã‚¤ã‚º
    DEFAULT_BATCH_CHUNKS: int = 3

    # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®ãƒˆãƒ¼ã‚¯ãƒ³åˆ¶é™
    DEFAULT_MIN_TOKENS: int = 150
    DEFAULT_MAX_TOKENS: int = 400


# ===================================================================
# Qdrantè¨­å®š
# ===================================================================

class QdrantConfig:
    """Qdrantè¨­å®š"""

    HOST: str = "localhost"
    PORT: int = 6333
    URL: str = f"http://{HOST}:{PORT}"
    DOCKER_IMAGE: str = "qdrant/qdrant"
    HEALTH_CHECK_ENDPOINT: str = "/collections"
    DEFAULT_TIMEOUT: int = 30
    DEFAULT_VECTOR_SIZE: int = 1536  # text-embedding-3-small
    DEFAULT_EMBEDDING_MODEL: str = "text-embedding-3-small"


# ===================================================================
# ãƒ‘ã‚¹è¨­å®š
# ===================================================================

class PathConfig:
    """ãƒ‘ã‚¹è¨­å®š"""

    BASE_DIR: Path = Path(__file__).parent
    OUTPUT_DIR: Path = BASE_DIR / "OUTPUT"
    QA_OUTPUT_DIR: Path = BASE_DIR / "qa_output"
    DATASETS_DIR: Path = BASE_DIR / "datasets"
    TEMP_DIR: Path = BASE_DIR / "temp_uploads"
    LOG_DIR: Path = BASE_DIR / "logs"

    @classmethod
    def ensure_dirs(cls) -> None:
        """å¿…è¦ãªãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆ"""
        cls.OUTPUT_DIR.mkdir(exist_ok=True)
        cls.QA_OUTPUT_DIR.mkdir(exist_ok=True)
        cls.DATASETS_DIR.mkdir(exist_ok=True)
        cls.TEMP_DIR.mkdir(exist_ok=True)
        cls.LOG_DIR.mkdir(exist_ok=True)


# ===================================================================
# Celeryè¨­å®š
# ===================================================================

class CeleryConfig:
    """Celeryè¨­å®š"""

    BROKER_URL: str = "redis://localhost:6379/0"
    RESULT_BACKEND: str = "redis://localhost:6379/0"
    TASK_SERIALIZER: str = "json"
    ACCEPT_CONTENT: List[str] = ["json"]
    RESULT_SERIALIZER: str = "json"
    TIMEZONE: str = "Asia/Tokyo"
    ENABLE_UTC: bool = True
    TASK_TIME_LIMIT: int = 300  # 5åˆ†
    TASK_SOFT_TIME_LIMIT: int = 240  # 4åˆ†
    WORKER_CONCURRENCY: int = 8  # Gemini APIãƒ¬ãƒ¼ãƒˆåˆ¶é™å¯¾ç­–ã®ãŸã‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã‚’8ã«è¨­å®š
    WORKER_PREFETCH_MULTIPLIER: int = 1


# ===================================================================
# Gemini APIè¨­å®š
# ===================================================================

class GeminiConfig:
    """Gemini 3 APIè¨­å®š"""

    # åˆ©ç”¨å¯èƒ½ãªãƒ¢ãƒ‡ãƒ«ä¸€è¦§
    AVAILABLE_MODELS: List[str] = [
        "gemini-3-pro-preview",
        "gemini-3-pro-image-preview",
        "gemini-2.5-flash-preview",
        "gemini-2.5-pro-preview",
        "gemini-2.0-flash",
    ]

    # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ¢ãƒ‡ãƒ«
    DEFAULT_MODEL: str = "gemini-2.0-flash"

    # Embeddingãƒ¢ãƒ‡ãƒ«
    EMBEDDING_MODEL: str = "gemini-embedding-001"

    # Embeddingæ¬¡å…ƒæ•°ï¼ˆ3072: Gemini 3æœ€å¤§ç²¾åº¦ï¼‰
    EMBEDDING_DIMS: int = 3072

    # æ€è€ƒãƒ¬ãƒ™ãƒ«
    DEFAULT_THINKING_LEVEL: str = "low"  # "low" or "high"

    # æ¸©åº¦è¨­å®šï¼ˆGemini 3æ¨å¥¨: 1.0ï¼‰
    DEFAULT_TEMPERATURE: float = 1.0

    # ãƒ¢ãƒ‡ãƒ«æ–™é‡‘ï¼ˆ1000ãƒˆãƒ¼ã‚¯ãƒ³ã‚ãŸã‚Šã®ãƒ‰ãƒ«ï¼‰
    MODEL_PRICING: Dict[str, Dict[str, float]] = {
        "gemini-3-pro-preview": {"input": 0.002, "output": 0.012},
        "gemini-3-pro-image-preview": {"input": 0.002, "output": 0.012},
        "gemini-2.5-flash-preview": {"input": 0.00015, "output": 0.0006},
        "gemini-2.5-pro-preview": {"input": 0.00125, "output": 0.005},
        "gemini-2.0-flash": {"input": 0.0001, "output": 0.0004},
        "gemini-embedding-001": {"input": 0.0, "output": 0.0},  # ç„¡æ–™æ ã‚ã‚Š
    }

    # ãƒ¢ãƒ‡ãƒ«åˆ¶é™
    MODEL_LIMITS: Dict[str, Dict[str, int]] = {
        "gemini-3-pro-preview": {"max_input_tokens": 1000000, "max_output_tokens": 64000},
        "gemini-3-pro-image-preview": {"max_input_tokens": 65000, "max_output_tokens": 32000},
        "gemini-2.5-flash-preview": {"max_input_tokens": 1000000, "max_output_tokens": 64000},
        "gemini-2.5-pro-preview": {"max_input_tokens": 1000000, "max_output_tokens": 64000},
        "gemini-2.0-flash": {"max_input_tokens": 1000000, "max_output_tokens": 8192},
    }

    @classmethod
    def get_model_limits(cls, model: str) -> Dict[str, int]:
        """ãƒ¢ãƒ‡ãƒ«ã®åˆ¶é™ã‚’å–å¾—"""
        return cls.MODEL_LIMITS.get(
            model,
            {"max_input_tokens": 128000, "max_output_tokens": 8192}
        )

    @classmethod
    def get_model_pricing(cls, model: str) -> Dict[str, float]:
        """ãƒ¢ãƒ‡ãƒ«ã®æ–™é‡‘ã‚’å–å¾—"""
        return cls.MODEL_PRICING.get(model, {"input": 0.001, "output": 0.004})

    @classmethod
    def supports_thinking_level(cls, model: str) -> bool:
        """ãƒ¢ãƒ‡ãƒ«ãŒthinking_levelã‚’ã‚µãƒãƒ¼ãƒˆã™ã‚‹ã‹ãƒã‚§ãƒƒã‚¯"""
        return model.startswith("gemini-3")


# ===================================================================
# RAG Agentè¨­å®š
# ===================================================================

class AgentConfig:
    """ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆè¨­å®š"""

    # RAGæ¤œç´¢è¨­å®š
    RAG_DEFAULT_COLLECTION: str = "qa_a02_qa_pairs_wikipedia_ja" # Default collection to search
    RAG_AVAILABLE_COLLECTIONS: List[str] = [ # List of available collections
        "qa_a02_qa_pairs_wikipedia_ja",
        "qa_a02_qa_pairs_livedoor",
        # Add other relevant collection names here
    ]
    RAG_SEARCH_LIMIT: int = 3
    RAG_SCORE_THRESHOLD: float = 0.50  # æ¤œç´¢çµæœã¨ã—ã¦æ¡ç”¨ã™ã‚‹æœ€å°ã‚¹ã‚³ã‚¢ (0.7 -> 0.5ã«ç·©å’Œ)

    # ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãƒ¢ãƒ‡ãƒ«è¨­å®š
    MODEL_NAME: str = GeminiConfig.DEFAULT_MODEL

    # ãƒ­ã‚®ãƒ³ã‚°è¨­å®š
    CHAT_LOG_FILE_NAME: str = "agent_chat.log"
    CHAT_LOG_LEVEL: str = "INFO"


# ===================================================================
# LLMãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼è¨­å®š
# ===================================================================

class LLMProviderConfig:
    """LLMãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼è¨­å®š"""

    # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼
    DEFAULT_LLM_PROVIDER: str = "gemini"  # "openai" or "gemini"
    DEFAULT_EMBEDDING_PROVIDER: str = "gemini"  # "openai" or "gemini"

    @classmethod
    def get_embedding_dims(cls, provider: Optional[str] = None) -> int:
        """ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã«å¿œã˜ãŸEmbeddingæ¬¡å…ƒæ•°ã‚’å–å¾—"""
        provider = provider or cls.DEFAULT_EMBEDDING_PROVIDER
        if provider.lower() == "gemini":
            return GeminiConfig.EMBEDDING_DIMS
        else:
            return QdrantConfig.DEFAULT_VECTOR_SIZE


# ===================================================================
# å¾Œæ–¹äº’æ›æ€§ã®ãŸã‚ã®ã‚¨ã‚¤ãƒªã‚¢ã‚¹
# ===================================================================

# helper_rag.py ã® AppConfig äº’æ›
class AppConfig(ModelConfig):
    """AppConfigäº’æ›ã‚¯ãƒ©ã‚¹ (å¾Œæ–¹äº’æ›æ€§ç”¨)"""
    pass


# DATASET_CONFIGS è¾æ›¸å½¢å¼ï¼ˆå¾Œæ–¹äº’æ›æ€§ç”¨ï¼‰
def get_dataset_configs() -> Dict[str, Dict[str, Any]]:
    """DATASET_CONFIGSè¾æ›¸ã‚’å–å¾—ï¼ˆå¾Œæ–¹äº’æ›æ€§ç”¨ï¼‰"""
    return {
        name: DatasetConfig.get_dataset_dict(name)
        for name in DatasetConfig.get_all_dataset_names()
    }


# ã‚°ãƒ­ãƒ¼ãƒãƒ«å¤‰æ•°ã¨ã—ã¦å…¬é–‹ï¼ˆå¾Œæ–¹äº’æ›æ€§ç”¨ï¼‰
DATASET_CONFIGS = get_dataset_configs()
NO_TEMPERATURE_MODELS = ModelConfig.NO_TEMPERATURE_MODELS


def supports_temperature(model: str) -> bool:
    """temperatureã‚µãƒãƒ¼ãƒˆãƒã‚§ãƒƒã‚¯ï¼ˆå¾Œæ–¹äº’æ›æ€§ç”¨ï¼‰"""
    return ModelConfig.supports_temperature(model)